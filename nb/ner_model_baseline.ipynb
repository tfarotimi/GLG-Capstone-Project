{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMu6Qp9jQrhc"
      },
      "source": [
        "# 0. Verify GPU/Install Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwKJKyCtEvMp",
        "outputId": "9bfb51b3-c8ff-456e-d647-d2d0bed5f65c"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6dK_j14KguL",
        "outputId": "ef5f1007-c1aa-45dc-8222-1a79eff4e291"
      },
      "source": [
        "!pip install transformers seqeval[gpu]\n",
        "!pip install torch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgHcYaYBI9BM"
      },
      "source": [
        "# 1. Import Libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLPTWF5UKOUi",
        "outputId": "3a9a4728-d6ce-44cf-d905-518d832cfa39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOYOkfuJJD5F"
      },
      "source": [
        "# 2. Import Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u86D-NQLXzR",
        "outputId": "52290512-8c13-47c4-911c-d94fae1bc0e2"
      },
      "source": [
        "#Mount Google Drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izxCE6oziBeI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ShkOYSADLZPf"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_559/2884283367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#read in annotated data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mner_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nb/ner_model_baseline.ipynb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode-escape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nb/ner_model_baseline.ipynb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode-escape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "#read in annotated data\n",
        "ner_dataset = pd.read_csv(\"nb/ner_model_baseline.ipynb\",encoding='unicode-escape',on_bad_lines='skip')\n",
        "ner = pd.read_csv(\"nb/ner_model_baseline.ipynb\",encoding='unicode-escape',on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMChVHopJwRz"
      },
      "source": [
        "# 3. Pre-process Data \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9weeF6hMd8t",
        "outputId": "0bb865e5-3f70-493b-f772-3569730b9c1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sentence #      47959\n",
              "Word          1048575\n",
              "POS           1048575\n",
              "Tag           1048575\n",
              "dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner_dataset.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lliAuewVMwCp",
        "outputId": "57342394-b4e6-4a09-e153-088dd53daea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "O        887908\n",
              "B-geo     37644\n",
              "B-tim     20333\n",
              "B-org     20143\n",
              "I-per     17251\n",
              "B-per     16990\n",
              "I-org     16784\n",
              "B-gpe     15870\n",
              "I-geo      7414\n",
              "I-tim      6528\n",
              "B-art       402\n",
              "B-eve       308\n",
              "I-art       297\n",
              "I-eve       253\n",
              "B-nat       201\n",
              "I-gpe       198\n",
              "I-nat        51\n",
              "Name: Tag, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "frequencies = ner_dataset['Tag'].value_counts()\n",
        "frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a5jqpNzN48j",
        "outputId": "9f0dd427-fee0-41d4-841e-fc1c0277ae39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'geo': 45058,\n",
              " 'tim': 26861,\n",
              " 'org': 36927,\n",
              " 'per': 34241,\n",
              " 'gpe': 16068,\n",
              " 'art': 699,\n",
              " 'eve': 561,\n",
              " 'nat': 252}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#count entities \n",
        "tags = {}\n",
        "for tag, count in zip(frequencies.index, frequencies):\n",
        "  if tag != 'O':\n",
        "    if tag[2:5] not in tags.keys():\n",
        "      tags[tag[2:5]] = count\n",
        "    else:\n",
        "      tags[tag[2:5]] += count\n",
        "\n",
        "tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W_qWL5tRRRQ",
        "outputId": "d0e5c067-7d75-4f54-e9d5-d76430dd62a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "O        887908\n",
              "B-geo     37644\n",
              "B-tim     20333\n",
              "B-org     20143\n",
              "I-per     17251\n",
              "B-per     16990\n",
              "I-org     16784\n",
              "B-gpe     15870\n",
              "I-geo      7414\n",
              "I-tim      6528\n",
              "I-gpe       198\n",
              "Name: Tag, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#remove nat, eve, art\n",
        "\n",
        "ner_dataset = data = ner_dataset[~ner_dataset.Tag.isin(['B-nat','B-eve', 'B-art','I-nat','I-eve', 'I-art'])]\n",
        "ner_dataset['Tag'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr1arztsxjj0",
        "outputId": "79035ed7-85b8-4af7-ab81-f08caa1d353b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'O': 0,\n",
              " 'B-geo': 1,\n",
              " 'B-gpe': 2,\n",
              " 'B-per': 3,\n",
              " 'I-geo': 4,\n",
              " 'B-org': 5,\n",
              " 'I-org': 6,\n",
              " 'B-tim': 7,\n",
              " 'I-per': 8,\n",
              " 'I-gpe': 9,\n",
              " 'I-tim': 10}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#create dictionary for tags \n",
        "labels_to_ids = {k:v for v, k in enumerate(ner_dataset.Tag.unique())}\n",
        "id_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n",
        "\n",
        "labels_to_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YBUZAkfRUMNU",
        "outputId": "461cbd28-aa17-405f-bda3-e9a52a4b66ad"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2447dd8f-de96-43d2-86dc-e0865ad843f8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2447dd8f-de96-43d2-86dc-e0865ad843f8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2447dd8f-de96-43d2-86dc-e0865ad843f8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2447dd8f-de96-43d2-86dc-e0865ad843f8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1  Sentence: 1             of   IN   O\n",
              "2  Sentence: 1  demonstrators  NNS   O\n",
              "3  Sentence: 1           have  VBP   O\n",
              "4  Sentence: 1        marched  VBN   O"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#replace NaNs\n",
        "ner_dataset = ner_dataset.fillna(method='ffill')\n",
        "ner_dataset.head(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "SRpWPk0bVwg4",
        "outputId": "864885f9-6a46-428f-dcfb-d13d730c72fa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-113ca9e7-29eb-4e7d-9e0e-120a8a52b328\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentence_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1048565</th>\n",
              "      <td>Sentence: 47958</td>\n",
              "      <td>impact</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "      <td>They say not all of the rockets exploded upon ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048566</th>\n",
              "      <td>Sentence: 47958</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>They say not all of the rockets exploded upon ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048567</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>Indian</td>\n",
              "      <td>JJ</td>\n",
              "      <td>B-gpe</td>\n",
              "      <td>Indian forces said they responded to the attack</td>\n",
              "      <td>B-gpe,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048568</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>forces</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "      <td>Indian forces said they responded to the attack</td>\n",
              "      <td>B-gpe,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048569</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>said</td>\n",
              "      <td>VBD</td>\n",
              "      <td>O</td>\n",
              "      <td>Indian forces said they responded to the attack</td>\n",
              "      <td>B-gpe,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048570</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>they</td>\n",
              "      <td>PRP</td>\n",
              "      <td>O</td>\n",
              "      <td>Indian forces said they responded to the attack</td>\n",
              "      <td>B-gpe,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048571</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>responded</td>\n",
              "      <td>VBD</td>\n",
              "      <td>O</td>\n",
              "      <td>Indian forces said they responded to the attack</td>\n",
              "      <td>B-gpe,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048572</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "      <td>O</td>\n",
              "      <td>Indian forces said they responded to the attack</td>\n",
              "      <td>B-gpe,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048573</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "      <td>Indian forces said they responded to the attack</td>\n",
              "      <td>B-gpe,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048574</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>attack</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "      <td>Indian forces said they responded to the attack</td>\n",
              "      <td>B-gpe,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-113ca9e7-29eb-4e7d-9e0e-120a8a52b328')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-113ca9e7-29eb-4e7d-9e0e-120a8a52b328 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-113ca9e7-29eb-4e7d-9e0e-120a8a52b328');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              Sentence #       Word  POS    Tag  \\\n",
              "1048565  Sentence: 47958     impact   NN      O   \n",
              "1048566  Sentence: 47958          .    .      O   \n",
              "1048567  Sentence: 47959     Indian   JJ  B-gpe   \n",
              "1048568  Sentence: 47959     forces  NNS      O   \n",
              "1048569  Sentence: 47959       said  VBD      O   \n",
              "1048570  Sentence: 47959       they  PRP      O   \n",
              "1048571  Sentence: 47959  responded  VBD      O   \n",
              "1048572  Sentence: 47959         to   TO      O   \n",
              "1048573  Sentence: 47959        the   DT      O   \n",
              "1048574  Sentence: 47959     attack   NN      O   \n",
              "\n",
              "                                                  sentence  \\\n",
              "1048565  They say not all of the rockets exploded upon ...   \n",
              "1048566  They say not all of the rockets exploded upon ...   \n",
              "1048567    Indian forces said they responded to the attack   \n",
              "1048568    Indian forces said they responded to the attack   \n",
              "1048569    Indian forces said they responded to the attack   \n",
              "1048570    Indian forces said they responded to the attack   \n",
              "1048571    Indian forces said they responded to the attack   \n",
              "1048572    Indian forces said they responded to the attack   \n",
              "1048573    Indian forces said they responded to the attack   \n",
              "1048574    Indian forces said they responded to the attack   \n",
              "\n",
              "                 sentence_tags  \n",
              "1048565  O,O,O,O,O,O,O,O,O,O,O  \n",
              "1048566  O,O,O,O,O,O,O,O,O,O,O  \n",
              "1048567    B-gpe,O,O,O,O,O,O,O  \n",
              "1048568    B-gpe,O,O,O,O,O,O,O  \n",
              "1048569    B-gpe,O,O,O,O,O,O,O  \n",
              "1048570    B-gpe,O,O,O,O,O,O,O  \n",
              "1048571    B-gpe,O,O,O,O,O,O,O  \n",
              "1048572    B-gpe,O,O,O,O,O,O,O  \n",
              "1048573    B-gpe,O,O,O,O,O,O,O  \n",
              "1048574    B-gpe,O,O,O,O,O,O,O  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#group by sentence\n",
        "data = ner_dataset.groupby(ner_dataset['Sentence #'])\n",
        "\n",
        "#create sentence column\n",
        "ner_dataset['sentence'] =data['Word'].transform(lambda x:' '.join(x))\n",
        "\n",
        "#create sentence tags column\n",
        "ner_dataset['sentence_tags'] = data['Tag'].transform(lambda x:','.join(x))\n",
        "\n",
        "ner_dataset.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gMunD-Jn0ZAZ",
        "outputId": "fb489157-257b-44dc-9bf7-f56769f42408"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-090e683d-b949-4243-a67d-14d4c1182380\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentence_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Families of soldiers killed in the conflict jo...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They marched from the Houses of Parliament to ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,B-geo,I-geo,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The protest comes on the eve of the annual con...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,B-geo,O,O,B-org,I-org,O,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-090e683d-b949-4243-a67d-14d4c1182380')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-090e683d-b949-4243-a67d-14d4c1182380 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-090e683d-b949-4243-a67d-14d4c1182380');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            sentence  \\\n",
              "0  Thousands of demonstrators have marched throug...   \n",
              "1  Families of soldiers killed in the conflict jo...   \n",
              "2  They marched from the Houses of Parliament to ...   \n",
              "3  Police put the number of marchers at 10,000 wh...   \n",
              "4  The protest comes on the eve of the annual con...   \n",
              "\n",
              "                                       sentence_tags  \n",
              "0  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
              "1  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...  \n",
              "2                O,O,O,O,O,O,O,O,O,O,O,B-geo,I-geo,O  \n",
              "3                      O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
              "4  O,O,O,O,O,O,O,O,O,O,O,B-geo,O,O,B-org,I-org,O,...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner_dataset = ner_dataset[[\"sentence\", \"sentence_tags\"]].drop_duplicates().reset_index(drop=True)\n",
        "ner_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0K8MFKB0mXk",
        "outputId": "043af1c1-e059-4802-a0d7-a16943e039ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47571"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ner_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N4DKr4ZJwmA"
      },
      "source": [
        "# 4.Transform Data for Use in Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "72e7c41c42a54f8ca902021e883d8e0b",
            "478847241d02467badd38b2be2ffc36d",
            "9969ab2ce15b49e28e32bca46b9d0eeb",
            "5a08cf79c3684773997908dc608b72d8",
            "186616aa25144a8e956069276e574dfa",
            "af5a8ab788be45ccb06007b9647d3efa",
            "30f44504f0d24629897801a44caacf99",
            "fdf74450f9444628bc6b7527ba3d23dd",
            "09c406674a854d8a9d7a1a2f887fde6f",
            "96fa318db3394eebb43351af24c8e9b8",
            "365aea2ce9d94af58a4a55edf343ccd6",
            "21912a216a3b4cb1af7ef0a24e3b89db",
            "e364d07b38b74359acf255a5416a757c",
            "526d6ce9c0cd439b98624b8a4b1c28cd",
            "c5d110d610d6402792e7197e73c691f7",
            "3cedbe20d2804134a47e0c85a12b379e",
            "a0b922631d844583bac144d127264e34",
            "767fe7a0a45b4293ab07c800a23ff92d",
            "c21ef92cfd78456c83c838857dd845e2",
            "b47a72c47395411fbc267c8e56d761a1",
            "c8c8bc41098046babcf9f944bb3385fe",
            "ce068d8f5ed2431bad0017d00062d1a8",
            "f87b87c8a4cf4efb816bb20310f89fa1",
            "386fc05de954494aaeb770ea38f5b830",
            "44cc7cd1f25e44d59ba3dea998c66684",
            "28e7b8d5b8f6425bb892fdace08699e5",
            "1cf23fb16a5841898b7100ca06b013b7",
            "aac03bfbc7bc4d0dbcf611e01ac890d1",
            "938cada5b49b4b3e8a4476d56e103831",
            "574ce02c814547b98c485352eddad1e2",
            "810f47705a9c46528c1ccae3c3544232",
            "f3c1af83a6a248f88a727cd186554b12",
            "e489c299466d4bbf8a0ce7d7927c9b15",
            "564ea5ee81c049c2aaf519391db5b3b1",
            "ea7290e35bcc4f1ba681a2c056aaf687",
            "506c83cfbf3548e5a7105f217d7b1499",
            "8e6db33a39a5466db49a8e6e56b31046",
            "e2cf7febc6ab400ca3a33715809b37b6",
            "5b123f1fc2754a82963f338496fa974e",
            "9fb0b5d4e4af45999f3df633cf6b6d62",
            "d988d1d2b2604932a6ed0a6ffa5f2863",
            "5ebb64a055984eeb897c7ea8975b93b7",
            "ae8cdfd371ed4c0888d5562b0deb3af8",
            "2e5ef85d47d84141ab50612b6f930e6f"
          ]
        },
        "id": "9HnDuyB-8aFu",
        "outputId": "62db2587-271c-4651-9007-50882734cf01"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72e7c41c42a54f8ca902021e883d8e0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21912a216a3b4cb1af7ef0a24e3b89db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f87b87c8a4cf4efb816bb20310f89fa1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "564ea5ee81c049c2aaf519391db5b3b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mc1qKHO4VSi"
      },
      "outputs": [],
      "source": [
        "#create a dataset object for use in BERT NER model\n",
        "class dataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "      \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels \n",
        "        sentence = self.data.sentence.to_numpy()[index]  \n",
        "        sentence_tags = self.data.sentence_tags.iloc[index].split(\",\") \n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             return_offsets_mapping=True, \n",
        "                             padding='max_length', \n",
        "                             truncation=True, \n",
        "                             max_length=self.max_len)\n",
        "        \n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in sentence_tags] \n",
        "\n",
        "        #labels = labels.to_numpy()\n",
        "        \n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "        \n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "          if mapping[0] == 0 and mapping[1] != 0:\n",
        "            # overwrite label\n",
        "            encoded_labels[idx] = labels[i]\n",
        "            i += 1\n",
        "\n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.as_tensor(encoded_labels)\n",
        "        \n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaIBWvkI7WWH"
      },
      "outputs": [],
      "source": [
        "#split into training and testing \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "  \n",
        "train_set, test_set = train_test_split(ner_dataset, train_size = 0.8, random_state = 42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXfRbO0nwc4d"
      },
      "outputs": [],
      "source": [
        "training_set = dataset(train_set, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_set, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGvcTs_lHcUu"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ZP1QtRJmQZ"
      },
      "source": [
        "\n",
        "\n",
        "# 5. Train Model for 1 epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867,
          "referenced_widgets": [
            "6fc2dae41f0d49c78e447355d6b8caab",
            "b3fe36c9bc0240bf956998af6f3419c8",
            "39b0cf1c971e4fa68ec10958a761b88b",
            "758be8b7c78344f1b61669490cf21f54",
            "a4ea4171c78b46898f7b52f80026b64d",
            "4fd31b77d4db49a194e7babc01a929e7",
            "d9dbfe6326d94af581cd7de0c6c716d4",
            "f32044a1a8ce4a49a48f155632a30a31",
            "1ae0981bc9d043a18833cf4b28860702",
            "5ae2108ee6c74636bdeb19c5975eaccc",
            "a1b363c69dfb438a8510cd78ea11b8ee"
          ]
        },
        "id": "-IpfJhd3JC7k",
        "outputId": "9a21973d-9476-4f07-b4b1-48af6ad5d08e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fc2dae41f0d49c78e447355d6b8caab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#instantiate Bert for NER model and send to gpu \n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov8C-xkoYCTZ",
        "outputId": "09941bea-d00a-4f80-af4b-ff7a08b6ecf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9T4AxcAJSze"
      },
      "outputs": [],
      "source": [
        "#define the optimizer \n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGpq0sRqLH5k"
      },
      "outputs": [],
      "source": [
        "#training function \n",
        "def train(training_model, epochs):\n",
        "\n",
        "  nb_batches = 0\n",
        "  nb_tr_sentences = 0\n",
        "  avg_loss = 0\n",
        "  tr_loss, tr_accuracy = 0, 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  loss_per_batch = []\n",
        "  loss_per_epoch = []\n",
        "  acc_per_batch = []\n",
        "  acc_per_epoch = []\n",
        "\n",
        "  model = training_model\n",
        "\n",
        "  for idx, batch in enumerate(training_loader):\n",
        "    \n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    \n",
        "    ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "    mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "    labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "    loss = model(input_ids=ids, attention_mask=mask, labels=labels)[0]\n",
        "    tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels)[1]\n",
        "\n",
        "    #import pdb\n",
        "    #pdb.set_trace()\n",
        "    \n",
        "    tr_loss += loss.item()\n",
        "   \n",
        "\n",
        "    nb_tr_steps += 1\n",
        "    nb_tr_sentences += labels.size(0)\n",
        "\n",
        "    nb_batches += 1\n",
        "  \n",
        "    \n",
        "    if idx % 100==0:\n",
        "        loss_step = tr_loss/nb_tr_steps\n",
        "        print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "        \n",
        "    # compute training accuracy\n",
        "    flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "    active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "    \n",
        "    # only compute accuracy at active labels\n",
        "    active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "    #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "    \n",
        "    labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "    predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "    \n",
        "    tr_labels.extend(labels)\n",
        "    tr_preds.extend(predictions)\n",
        "\n",
        "    tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "    tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "    # gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(\n",
        "        parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "    )\n",
        "    \n",
        "    # backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    loss_per_batch.append(epoch_loss)\n",
        "\n",
        "    batch_accuracy = tr_accuracy / nb_tr_steps\n",
        "    acc_per_batch.append(batch_accuracy)\n",
        "\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
        "    print(f\"Number of batches: {nb_batches}\")  \n",
        "    print(f\"Number of Sentences Trained on: {nb_tr_sentences}\")  \n",
        "\n",
        "  loss_per_epoch.append(epoch_loss)\n",
        "  acc_per_epoch.append(batch_accuracy)\n",
        "\n",
        "  return (loss_per_batch, loss_per_epoch, acc_per_batch, acc_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsfOFzOxUepJ",
        "outputId": "0b56c394-8194-4c5b-dffe-3d0cc6d22b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training loss epoch: 0.3137761241027268\n",
            "Training accuracy epoch: 1035.3125\n",
            "Number of batches: 1133\n",
            "Number of Sentences Trained on: 18128\n",
            "Training loss epoch: 0.31388356216823454\n",
            "Training accuracy epoch: 1036.1875\n",
            "Number of batches: 1134\n",
            "Number of Sentences Trained on: 18144\n",
            "Training loss epoch: 0.3138197646086568\n",
            "Training accuracy epoch: 1037.125\n",
            "Number of batches: 1135\n",
            "Number of Sentences Trained on: 18160\n",
            "Training loss epoch: 0.31367090219323535\n",
            "Training accuracy epoch: 1038.0625\n",
            "Number of batches: 1136\n",
            "Number of Sentences Trained on: 18176\n",
            "Training loss epoch: 0.31371692804129847\n",
            "Training accuracy epoch: 1038.9375\n",
            "Number of batches: 1137\n",
            "Number of Sentences Trained on: 18192\n",
            "Training loss epoch: 0.3136298682471911\n",
            "Training accuracy epoch: 1039.875\n",
            "Number of batches: 1138\n",
            "Number of Sentences Trained on: 18208\n",
            "Training loss epoch: 0.31364883584010433\n",
            "Training accuracy epoch: 1040.8125\n",
            "Number of batches: 1139\n",
            "Number of Sentences Trained on: 18224\n",
            "Training loss epoch: 0.31345993970079644\n",
            "Training accuracy epoch: 1041.8125\n",
            "Number of batches: 1140\n",
            "Number of Sentences Trained on: 18240\n",
            "Training loss epoch: 0.31333415862744207\n",
            "Training accuracy epoch: 1042.8125\n",
            "Number of batches: 1141\n",
            "Number of Sentences Trained on: 18256\n",
            "Training loss epoch: 0.3131001958388659\n",
            "Training accuracy epoch: 1043.75\n",
            "Number of batches: 1142\n",
            "Number of Sentences Trained on: 18272\n",
            "Training loss epoch: 0.31286650818665873\n",
            "Training accuracy epoch: 1044.75\n",
            "Number of batches: 1143\n",
            "Number of Sentences Trained on: 18288\n",
            "Training loss epoch: 0.3126837430551153\n",
            "Training accuracy epoch: 1045.6875\n",
            "Number of batches: 1144\n",
            "Number of Sentences Trained on: 18304\n",
            "Training loss epoch: 0.31254304209032696\n",
            "Training accuracy epoch: 1046.6875\n",
            "Number of batches: 1145\n",
            "Number of Sentences Trained on: 18320\n",
            "Training loss epoch: 0.31231591541216064\n",
            "Training accuracy epoch: 1047.6875\n",
            "Number of batches: 1146\n",
            "Number of Sentences Trained on: 18336\n",
            "Training loss epoch: 0.3123174723523564\n",
            "Training accuracy epoch: 1048.5625\n",
            "Number of batches: 1147\n",
            "Number of Sentences Trained on: 18352\n",
            "Training loss epoch: 0.3123989782932146\n",
            "Training accuracy epoch: 1049.3125\n",
            "Number of batches: 1148\n",
            "Number of Sentences Trained on: 18368\n",
            "Training loss epoch: 0.31235597190083214\n",
            "Training accuracy epoch: 1050.1875\n",
            "Number of batches: 1149\n",
            "Number of Sentences Trained on: 18384\n",
            "Training loss epoch: 0.31216330363860595\n",
            "Training accuracy epoch: 1051.1875\n",
            "Number of batches: 1150\n",
            "Number of Sentences Trained on: 18400\n",
            "Training loss epoch: 0.3122393350245562\n",
            "Training accuracy epoch: 1052.0\n",
            "Number of batches: 1151\n",
            "Number of Sentences Trained on: 18416\n",
            "Training loss epoch: 0.31233473713412724\n",
            "Training accuracy epoch: 1052.875\n",
            "Number of batches: 1152\n",
            "Number of Sentences Trained on: 18432\n",
            "Training loss epoch: 0.3124166595130021\n",
            "Training accuracy epoch: 1053.8125\n",
            "Number of batches: 1153\n",
            "Number of Sentences Trained on: 18448\n",
            "Training loss epoch: 0.312478210436474\n",
            "Training accuracy epoch: 1054.6875\n",
            "Number of batches: 1154\n",
            "Number of Sentences Trained on: 18464\n",
            "Training loss epoch: 0.31226576178524146\n",
            "Training accuracy epoch: 1055.6875\n",
            "Number of batches: 1155\n",
            "Number of Sentences Trained on: 18480\n",
            "Training loss epoch: 0.3121400142046639\n",
            "Training accuracy epoch: 1056.625\n",
            "Number of batches: 1156\n",
            "Number of Sentences Trained on: 18496\n",
            "Training loss epoch: 0.3119423421326777\n",
            "Training accuracy epoch: 1057.625\n",
            "Number of batches: 1157\n",
            "Number of Sentences Trained on: 18512\n",
            "Training loss epoch: 0.3117126027180046\n",
            "Training accuracy epoch: 1058.625\n",
            "Number of batches: 1158\n",
            "Number of Sentences Trained on: 18528\n",
            "Training loss epoch: 0.31161864382970994\n",
            "Training accuracy epoch: 1059.625\n",
            "Number of batches: 1159\n",
            "Number of Sentences Trained on: 18544\n",
            "Training loss epoch: 0.31149640948640117\n",
            "Training accuracy epoch: 1060.5625\n",
            "Number of batches: 1160\n",
            "Number of Sentences Trained on: 18560\n",
            "Training loss epoch: 0.3112756169219016\n",
            "Training accuracy epoch: 1061.5625\n",
            "Number of batches: 1161\n",
            "Number of Sentences Trained on: 18576\n",
            "Training loss epoch: 0.31119199945285225\n",
            "Training accuracy epoch: 1062.5\n",
            "Number of batches: 1162\n",
            "Number of Sentences Trained on: 18592\n",
            "Training loss epoch: 0.31114597834313573\n",
            "Training accuracy epoch: 1063.4375\n",
            "Number of batches: 1163\n",
            "Number of Sentences Trained on: 18608\n",
            "Training loss epoch: 0.311000043793527\n",
            "Training accuracy epoch: 1064.4375\n",
            "Number of batches: 1164\n",
            "Number of Sentences Trained on: 18624\n",
            "Training loss epoch: 0.3110952775925398\n",
            "Training accuracy epoch: 1065.3125\n",
            "Number of batches: 1165\n",
            "Number of Sentences Trained on: 18640\n",
            "Training loss epoch: 0.3108983880682093\n",
            "Training accuracy epoch: 1066.3125\n",
            "Number of batches: 1166\n",
            "Number of Sentences Trained on: 18656\n",
            "Training loss epoch: 0.31090200899109793\n",
            "Training accuracy epoch: 1067.1875\n",
            "Number of batches: 1167\n",
            "Number of Sentences Trained on: 18672\n",
            "Training loss epoch: 0.31067854876164347\n",
            "Training accuracy epoch: 1068.1875\n",
            "Number of batches: 1168\n",
            "Number of Sentences Trained on: 18688\n",
            "Training loss epoch: 0.31078423676459427\n",
            "Training accuracy epoch: 1069.0625\n",
            "Number of batches: 1169\n",
            "Number of Sentences Trained on: 18704\n",
            "Training loss epoch: 0.31059908610410414\n",
            "Training accuracy epoch: 1070.0625\n",
            "Number of batches: 1170\n",
            "Number of Sentences Trained on: 18720\n",
            "Training loss epoch: 0.3104002549369477\n",
            "Training accuracy epoch: 1071.0625\n",
            "Number of batches: 1171\n",
            "Number of Sentences Trained on: 18736\n",
            "Training loss epoch: 0.31033066329117165\n",
            "Training accuracy epoch: 1072.0\n",
            "Number of batches: 1172\n",
            "Number of Sentences Trained on: 18752\n",
            "Training loss epoch: 0.31063266256816197\n",
            "Training accuracy epoch: 1072.8125\n",
            "Number of batches: 1173\n",
            "Number of Sentences Trained on: 18768\n",
            "Training loss epoch: 0.3104244567647424\n",
            "Training accuracy epoch: 1073.8125\n",
            "Number of batches: 1174\n",
            "Number of Sentences Trained on: 18784\n",
            "Training loss epoch: 0.31029233882560375\n",
            "Training accuracy epoch: 1074.75\n",
            "Number of batches: 1175\n",
            "Number of Sentences Trained on: 18800\n",
            "Training loss epoch: 0.3101847291730826\n",
            "Training accuracy epoch: 1075.6875\n",
            "Number of batches: 1176\n",
            "Number of Sentences Trained on: 18816\n",
            "Training loss epoch: 0.30996750733049977\n",
            "Training accuracy epoch: 1076.6875\n",
            "Number of batches: 1177\n",
            "Number of Sentences Trained on: 18832\n",
            "Training loss epoch: 0.309726660740712\n",
            "Training accuracy epoch: 1077.6875\n",
            "Number of batches: 1178\n",
            "Number of Sentences Trained on: 18848\n",
            "Training loss epoch: 0.30957574365886464\n",
            "Training accuracy epoch: 1078.625\n",
            "Number of batches: 1179\n",
            "Number of Sentences Trained on: 18864\n",
            "Training loss epoch: 0.30953690089695785\n",
            "Training accuracy epoch: 1079.5\n",
            "Number of batches: 1180\n",
            "Number of Sentences Trained on: 18880\n",
            "Training loss epoch: 0.30930680376426284\n",
            "Training accuracy epoch: 1080.5\n",
            "Number of batches: 1181\n",
            "Number of Sentences Trained on: 18896\n",
            "Training loss epoch: 0.30928761038500596\n",
            "Training accuracy epoch: 1081.375\n",
            "Number of batches: 1182\n",
            "Number of Sentences Trained on: 18912\n",
            "Training loss epoch: 0.30917011388276\n",
            "Training accuracy epoch: 1082.375\n",
            "Number of batches: 1183\n",
            "Number of Sentences Trained on: 18928\n",
            "Training loss epoch: 0.30910446468860264\n",
            "Training accuracy epoch: 1083.25\n",
            "Number of batches: 1184\n",
            "Number of Sentences Trained on: 18944\n",
            "Training loss epoch: 0.3093717587569469\n",
            "Training accuracy epoch: 1084.0625\n",
            "Number of batches: 1185\n",
            "Number of Sentences Trained on: 18960\n",
            "Training loss epoch: 0.30914732098460046\n",
            "Training accuracy epoch: 1085.0625\n",
            "Number of batches: 1186\n",
            "Number of Sentences Trained on: 18976\n",
            "Training loss epoch: 0.3089124175712855\n",
            "Training accuracy epoch: 1086.0625\n",
            "Number of batches: 1187\n",
            "Number of Sentences Trained on: 18992\n",
            "Training loss epoch: 0.30866588226296887\n",
            "Training accuracy epoch: 1087.0625\n",
            "Number of batches: 1188\n",
            "Number of Sentences Trained on: 19008\n",
            "Training loss epoch: 0.30852897528518236\n",
            "Training accuracy epoch: 1088.0\n",
            "Number of batches: 1189\n",
            "Number of Sentences Trained on: 19024\n",
            "Training loss epoch: 0.30846753480429406\n",
            "Training accuracy epoch: 1088.9375\n",
            "Number of batches: 1190\n",
            "Number of Sentences Trained on: 19040\n",
            "Training loss epoch: 0.3082586092597592\n",
            "Training accuracy epoch: 1089.9375\n",
            "Number of batches: 1191\n",
            "Number of Sentences Trained on: 19056\n",
            "Training loss epoch: 0.30809481908886027\n",
            "Training accuracy epoch: 1090.875\n",
            "Number of batches: 1192\n",
            "Number of Sentences Trained on: 19072\n",
            "Training loss epoch: 0.30811548382293646\n",
            "Training accuracy epoch: 1091.8125\n",
            "Number of batches: 1193\n",
            "Number of Sentences Trained on: 19088\n",
            "Training loss epoch: 0.30796962420470153\n",
            "Training accuracy epoch: 1092.8125\n",
            "Number of batches: 1194\n",
            "Number of Sentences Trained on: 19104\n",
            "Training loss epoch: 0.30775319824470637\n",
            "Training accuracy epoch: 1093.8125\n",
            "Number of batches: 1195\n",
            "Number of Sentences Trained on: 19120\n",
            "Training loss epoch: 0.3077642642316661\n",
            "Training accuracy epoch: 1094.6875\n",
            "Number of batches: 1196\n",
            "Number of Sentences Trained on: 19136\n",
            "Training loss epoch: 0.3075269934244348\n",
            "Training accuracy epoch: 1095.6875\n",
            "Number of batches: 1197\n",
            "Number of Sentences Trained on: 19152\n",
            "Training loss epoch: 0.307297048612944\n",
            "Training accuracy epoch: 1096.6875\n",
            "Number of batches: 1198\n",
            "Number of Sentences Trained on: 19168\n",
            "Training loss epoch: 0.30745869899654854\n",
            "Training accuracy epoch: 1097.5625\n",
            "Number of batches: 1199\n",
            "Number of Sentences Trained on: 19184\n",
            "Training loss epoch: 0.30725589661393315\n",
            "Training accuracy epoch: 1098.5625\n",
            "Number of batches: 1200\n",
            "Number of Sentences Trained on: 19200\n",
            "Training loss per 100 training steps: 0.30722189886180634\n",
            "Training loss epoch: 0.30722189886180634\n",
            "Training accuracy epoch: 1099.4375\n",
            "Number of batches: 1201\n",
            "Number of Sentences Trained on: 19216\n",
            "Training loss epoch: 0.3071693688144477\n",
            "Training accuracy epoch: 1100.3125\n",
            "Number of batches: 1202\n",
            "Number of Sentences Trained on: 19232\n",
            "Training loss epoch: 0.3069339897019087\n",
            "Training accuracy epoch: 1101.3125\n",
            "Number of batches: 1203\n",
            "Number of Sentences Trained on: 19248\n",
            "Training loss epoch: 0.30673331299957596\n",
            "Training accuracy epoch: 1102.3125\n",
            "Number of batches: 1204\n",
            "Number of Sentences Trained on: 19264\n",
            "Training loss epoch: 0.3065850303798172\n",
            "Training accuracy epoch: 1103.25\n",
            "Number of batches: 1205\n",
            "Number of Sentences Trained on: 19280\n",
            "Training loss epoch: 0.3066898755540772\n",
            "Training accuracy epoch: 1104.1875\n",
            "Number of batches: 1206\n",
            "Number of Sentences Trained on: 19296\n",
            "Training loss epoch: 0.30675414633453385\n",
            "Training accuracy epoch: 1105.0625\n",
            "Number of batches: 1207\n",
            "Number of Sentences Trained on: 19312\n",
            "Training loss epoch: 0.30680819565510464\n",
            "Training accuracy epoch: 1105.9375\n",
            "Number of batches: 1208\n",
            "Number of Sentences Trained on: 19328\n",
            "Training loss epoch: 0.30662791501049713\n",
            "Training accuracy epoch: 1106.875\n",
            "Number of batches: 1209\n",
            "Number of Sentences Trained on: 19344\n",
            "Training loss epoch: 0.3066556833484326\n",
            "Training accuracy epoch: 1107.75\n",
            "Number of batches: 1210\n",
            "Number of Sentences Trained on: 19360\n",
            "Training loss epoch: 0.30697366750155425\n",
            "Training accuracy epoch: 1108.5\n",
            "Number of batches: 1211\n",
            "Number of Sentences Trained on: 19376\n",
            "Training loss epoch: 0.30688320274738345\n",
            "Training accuracy epoch: 1109.4375\n",
            "Number of batches: 1212\n",
            "Number of Sentences Trained on: 19392\n",
            "Training loss epoch: 0.3068546773658517\n",
            "Training accuracy epoch: 1110.375\n",
            "Number of batches: 1213\n",
            "Number of Sentences Trained on: 19408\n",
            "Training loss epoch: 0.3071225437717712\n",
            "Training accuracy epoch: 1111.1875\n",
            "Number of batches: 1214\n",
            "Number of Sentences Trained on: 19424\n",
            "Training loss epoch: 0.3070304091559884\n",
            "Training accuracy epoch: 1112.125\n",
            "Number of batches: 1215\n",
            "Number of Sentences Trained on: 19440\n",
            "Training loss epoch: 0.30680097371039566\n",
            "Training accuracy epoch: 1113.125\n",
            "Number of batches: 1216\n",
            "Number of Sentences Trained on: 19456\n",
            "Training loss epoch: 0.3066186234706993\n",
            "Training accuracy epoch: 1114.0625\n",
            "Number of batches: 1217\n",
            "Number of Sentences Trained on: 19472\n",
            "Training loss epoch: 0.3064246691613533\n",
            "Training accuracy epoch: 1115.0625\n",
            "Number of batches: 1218\n",
            "Number of Sentences Trained on: 19488\n",
            "Training loss epoch: 0.3062506130804064\n",
            "Training accuracy epoch: 1116.0625\n",
            "Number of batches: 1219\n",
            "Number of Sentences Trained on: 19504\n",
            "Training loss epoch: 0.3060325026619019\n",
            "Training accuracy epoch: 1117.0625\n",
            "Number of batches: 1220\n",
            "Number of Sentences Trained on: 19520\n",
            "Training loss epoch: 0.3060440808812265\n",
            "Training accuracy epoch: 1118.0\n",
            "Number of batches: 1221\n",
            "Number of Sentences Trained on: 19536\n",
            "Training loss epoch: 0.3058658624433725\n",
            "Training accuracy epoch: 1118.9375\n",
            "Number of batches: 1222\n",
            "Number of Sentences Trained on: 19552\n",
            "Training loss epoch: 0.30579738717797295\n",
            "Training accuracy epoch: 1119.875\n",
            "Number of batches: 1223\n",
            "Number of Sentences Trained on: 19568\n",
            "Training loss epoch: 0.3058154630989104\n",
            "Training accuracy epoch: 1120.75\n",
            "Number of batches: 1224\n",
            "Number of Sentences Trained on: 19584\n",
            "Training loss epoch: 0.30571622622712535\n",
            "Training accuracy epoch: 1121.6875\n",
            "Number of batches: 1225\n",
            "Number of Sentences Trained on: 19600\n",
            "Training loss epoch: 0.3055764305987205\n",
            "Training accuracy epoch: 1122.6875\n",
            "Number of batches: 1226\n",
            "Number of Sentences Trained on: 19616\n",
            "Training loss epoch: 0.305351118803838\n",
            "Training accuracy epoch: 1123.6875\n",
            "Number of batches: 1227\n",
            "Number of Sentences Trained on: 19632\n",
            "Training loss epoch: 0.3051330637572036\n",
            "Training accuracy epoch: 1124.6875\n",
            "Number of batches: 1228\n",
            "Number of Sentences Trained on: 19648\n",
            "Training loss epoch: 0.30494308272078086\n",
            "Training accuracy epoch: 1125.6875\n",
            "Number of batches: 1229\n",
            "Number of Sentences Trained on: 19664\n",
            "Training loss epoch: 0.3047129806555141\n",
            "Training accuracy epoch: 1126.6875\n",
            "Number of batches: 1230\n",
            "Number of Sentences Trained on: 19680\n",
            "Training loss epoch: 0.30458541007169276\n",
            "Training accuracy epoch: 1127.6875\n",
            "Number of batches: 1231\n",
            "Number of Sentences Trained on: 19696\n",
            "Training loss epoch: 0.30451511793358665\n",
            "Training accuracy epoch: 1128.625\n",
            "Number of batches: 1232\n",
            "Number of Sentences Trained on: 19712\n",
            "Training loss epoch: 0.30436941201512346\n",
            "Training accuracy epoch: 1129.5625\n",
            "Number of batches: 1233\n",
            "Number of Sentences Trained on: 19728\n",
            "Training loss epoch: 0.30419407038371316\n",
            "Training accuracy epoch: 1130.5625\n",
            "Number of batches: 1234\n",
            "Number of Sentences Trained on: 19744\n",
            "Training loss epoch: 0.30421268510130733\n",
            "Training accuracy epoch: 1131.4375\n",
            "Number of batches: 1235\n",
            "Number of Sentences Trained on: 19760\n",
            "Training loss epoch: 0.30407528437878995\n",
            "Training accuracy epoch: 1132.375\n",
            "Number of batches: 1236\n",
            "Number of Sentences Trained on: 19776\n",
            "Training loss epoch: 0.30386375316336645\n",
            "Training accuracy epoch: 1133.375\n",
            "Number of batches: 1237\n",
            "Number of Sentences Trained on: 19792\n",
            "Training loss epoch: 0.30387952967406956\n",
            "Training accuracy epoch: 1134.25\n",
            "Number of batches: 1238\n",
            "Number of Sentences Trained on: 19808\n",
            "Training loss epoch: 0.3039666720492355\n",
            "Training accuracy epoch: 1135.125\n",
            "Number of batches: 1239\n",
            "Number of Sentences Trained on: 19824\n",
            "Training loss epoch: 0.30390955175182993\n",
            "Training accuracy epoch: 1136.0625\n",
            "Number of batches: 1240\n",
            "Number of Sentences Trained on: 19840\n",
            "Training loss epoch: 0.3038867212183575\n",
            "Training accuracy epoch: 1136.875\n",
            "Number of batches: 1241\n",
            "Number of Sentences Trained on: 19856\n",
            "Training loss epoch: 0.303752401255779\n",
            "Training accuracy epoch: 1137.8125\n",
            "Number of batches: 1242\n",
            "Number of Sentences Trained on: 19872\n",
            "Training loss epoch: 0.30353406758878565\n",
            "Training accuracy epoch: 1138.8125\n",
            "Number of batches: 1243\n",
            "Number of Sentences Trained on: 19888\n",
            "Training loss epoch: 0.30339634265463167\n",
            "Training accuracy epoch: 1139.75\n",
            "Number of batches: 1244\n",
            "Number of Sentences Trained on: 19904\n",
            "Training loss epoch: 0.3032954236021243\n",
            "Training accuracy epoch: 1140.6875\n",
            "Number of batches: 1245\n",
            "Number of Sentences Trained on: 19920\n",
            "Training loss epoch: 0.30322871846632726\n",
            "Training accuracy epoch: 1141.625\n",
            "Number of batches: 1246\n",
            "Number of Sentences Trained on: 19936\n",
            "Training loss epoch: 0.3032639498442244\n",
            "Training accuracy epoch: 1142.5\n",
            "Number of batches: 1247\n",
            "Number of Sentences Trained on: 19952\n",
            "Training loss epoch: 0.3030652006288083\n",
            "Training accuracy epoch: 1143.5\n",
            "Number of batches: 1248\n",
            "Number of Sentences Trained on: 19968\n",
            "Training loss epoch: 0.30312558314841304\n",
            "Training accuracy epoch: 1144.4375\n",
            "Number of batches: 1249\n",
            "Number of Sentences Trained on: 19984\n",
            "Training loss epoch: 0.3029819308042526\n",
            "Training accuracy epoch: 1145.4375\n",
            "Number of batches: 1250\n",
            "Number of Sentences Trained on: 20000\n",
            "Training loss epoch: 0.30311178355861146\n",
            "Training accuracy epoch: 1146.375\n",
            "Number of batches: 1251\n",
            "Number of Sentences Trained on: 20016\n",
            "Training loss epoch: 0.30307292257444546\n",
            "Training accuracy epoch: 1147.3125\n",
            "Number of batches: 1252\n",
            "Number of Sentences Trained on: 20032\n",
            "Training loss epoch: 0.3030029564786699\n",
            "Training accuracy epoch: 1148.25\n",
            "Number of batches: 1253\n",
            "Number of Sentences Trained on: 20048\n",
            "Training loss epoch: 0.3028234816386559\n",
            "Training accuracy epoch: 1149.25\n",
            "Number of batches: 1254\n",
            "Number of Sentences Trained on: 20064\n",
            "Training loss epoch: 0.30275166535757453\n",
            "Training accuracy epoch: 1150.1875\n",
            "Number of batches: 1255\n",
            "Number of Sentences Trained on: 20080\n",
            "Training loss epoch: 0.3026114467080611\n",
            "Training accuracy epoch: 1151.125\n",
            "Number of batches: 1256\n",
            "Number of Sentences Trained on: 20096\n",
            "Training loss epoch: 0.3025266230224899\n",
            "Training accuracy epoch: 1152.0625\n",
            "Number of batches: 1257\n",
            "Number of Sentences Trained on: 20112\n",
            "Training loss epoch: 0.30238076138335307\n",
            "Training accuracy epoch: 1153.0\n",
            "Number of batches: 1258\n",
            "Number of Sentences Trained on: 20128\n",
            "Training loss epoch: 0.302206193938011\n",
            "Training accuracy epoch: 1154.0\n",
            "Number of batches: 1259\n",
            "Number of Sentences Trained on: 20144\n",
            "Training loss epoch: 0.3019769807915307\n",
            "Training accuracy epoch: 1155.0\n",
            "Number of batches: 1260\n",
            "Number of Sentences Trained on: 20160\n",
            "Training loss epoch: 0.301796832717187\n",
            "Training accuracy epoch: 1155.9375\n",
            "Number of batches: 1261\n",
            "Number of Sentences Trained on: 20176\n",
            "Training loss epoch: 0.3015736234330066\n",
            "Training accuracy epoch: 1156.9375\n",
            "Number of batches: 1262\n",
            "Number of Sentences Trained on: 20192\n",
            "Training loss epoch: 0.30135225842877195\n",
            "Training accuracy epoch: 1157.9375\n",
            "Number of batches: 1263\n",
            "Number of Sentences Trained on: 20208\n",
            "Training loss epoch: 0.3014259970167805\n",
            "Training accuracy epoch: 1158.8125\n",
            "Number of batches: 1264\n",
            "Number of Sentences Trained on: 20224\n",
            "Training loss epoch: 0.3014394312176721\n",
            "Training accuracy epoch: 1159.6875\n",
            "Number of batches: 1265\n",
            "Number of Sentences Trained on: 20240\n",
            "Training loss epoch: 0.3013345766687504\n",
            "Training accuracy epoch: 1160.625\n",
            "Number of batches: 1266\n",
            "Number of Sentences Trained on: 20256\n",
            "Training loss epoch: 0.3011635474018288\n",
            "Training accuracy epoch: 1161.5625\n",
            "Number of batches: 1267\n",
            "Number of Sentences Trained on: 20272\n",
            "Training loss epoch: 0.30100054567318396\n",
            "Training accuracy epoch: 1162.5625\n",
            "Number of batches: 1268\n",
            "Number of Sentences Trained on: 20288\n",
            "Training loss epoch: 0.3012055663386077\n",
            "Training accuracy epoch: 1163.4375\n",
            "Number of batches: 1269\n",
            "Number of Sentences Trained on: 20304\n",
            "Training loss epoch: 0.3010291114823497\n",
            "Training accuracy epoch: 1164.4375\n",
            "Number of batches: 1270\n",
            "Number of Sentences Trained on: 20320\n",
            "Training loss epoch: 0.301066125595076\n",
            "Training accuracy epoch: 1165.375\n",
            "Number of batches: 1271\n",
            "Number of Sentences Trained on: 20336\n",
            "Training loss epoch: 0.30136371882114\n",
            "Training accuracy epoch: 1166.1875\n",
            "Number of batches: 1272\n",
            "Number of Sentences Trained on: 20352\n",
            "Training loss epoch: 0.3012102430083519\n",
            "Training accuracy epoch: 1167.125\n",
            "Number of batches: 1273\n",
            "Number of Sentences Trained on: 20368\n",
            "Training loss epoch: 0.3011247207969135\n",
            "Training accuracy epoch: 1168.0625\n",
            "Number of batches: 1274\n",
            "Number of Sentences Trained on: 20384\n",
            "Training loss epoch: 0.30105125075680955\n",
            "Training accuracy epoch: 1169.0\n",
            "Number of batches: 1275\n",
            "Number of Sentences Trained on: 20400\n",
            "Training loss epoch: 0.30082630015162276\n",
            "Training accuracy epoch: 1170.0\n",
            "Number of batches: 1276\n",
            "Number of Sentences Trained on: 20416\n",
            "Training loss epoch: 0.3008023177389531\n",
            "Training accuracy epoch: 1170.9375\n",
            "Number of batches: 1277\n",
            "Number of Sentences Trained on: 20432\n",
            "Training loss epoch: 0.30059697349294906\n",
            "Training accuracy epoch: 1171.9375\n",
            "Number of batches: 1278\n",
            "Number of Sentences Trained on: 20448\n",
            "Training loss epoch: 0.3003945162955255\n",
            "Training accuracy epoch: 1172.9375\n",
            "Number of batches: 1279\n",
            "Number of Sentences Trained on: 20464\n",
            "Training loss epoch: 0.30044695375254377\n",
            "Training accuracy epoch: 1173.8125\n",
            "Number of batches: 1280\n",
            "Number of Sentences Trained on: 20480\n",
            "Training loss epoch: 0.3002661439902051\n",
            "Training accuracy epoch: 1174.8125\n",
            "Number of batches: 1281\n",
            "Number of Sentences Trained on: 20496\n",
            "Training loss epoch: 0.3001441568798747\n",
            "Training accuracy epoch: 1175.75\n",
            "Number of batches: 1282\n",
            "Number of Sentences Trained on: 20512\n",
            "Training loss epoch: 0.2999467857658213\n",
            "Training accuracy epoch: 1176.75\n",
            "Number of batches: 1283\n",
            "Number of Sentences Trained on: 20528\n",
            "Training loss epoch: 0.2999514251790732\n",
            "Training accuracy epoch: 1177.6875\n",
            "Number of batches: 1284\n",
            "Number of Sentences Trained on: 20544\n",
            "Training loss epoch: 0.2997312882168632\n",
            "Training accuracy epoch: 1178.6875\n",
            "Number of batches: 1285\n",
            "Number of Sentences Trained on: 20560\n",
            "Training loss epoch: 0.2995205488139952\n",
            "Training accuracy epoch: 1179.6875\n",
            "Number of batches: 1286\n",
            "Number of Sentences Trained on: 20576\n",
            "Training loss epoch: 0.2994502556047865\n",
            "Training accuracy epoch: 1180.5625\n",
            "Number of batches: 1287\n",
            "Number of Sentences Trained on: 20592\n",
            "Training loss epoch: 0.2994377327466858\n",
            "Training accuracy epoch: 1181.5\n",
            "Number of batches: 1288\n",
            "Number of Sentences Trained on: 20608\n",
            "Training loss epoch: 0.2992688553021965\n",
            "Training accuracy epoch: 1182.5\n",
            "Number of batches: 1289\n",
            "Number of Sentences Trained on: 20624\n",
            "Training loss epoch: 0.2992868762939822\n",
            "Training accuracy epoch: 1183.4375\n",
            "Number of batches: 1290\n",
            "Number of Sentences Trained on: 20640\n",
            "Training loss epoch: 0.29920793086215336\n",
            "Training accuracy epoch: 1184.375\n",
            "Number of batches: 1291\n",
            "Number of Sentences Trained on: 20656\n",
            "Training loss epoch: 0.2990300379529157\n",
            "Training accuracy epoch: 1185.375\n",
            "Number of batches: 1292\n",
            "Number of Sentences Trained on: 20672\n",
            "Training loss epoch: 0.298946358711507\n",
            "Training accuracy epoch: 1186.3125\n",
            "Number of batches: 1293\n",
            "Number of Sentences Trained on: 20688\n",
            "Training loss epoch: 0.29887045797110295\n",
            "Training accuracy epoch: 1187.25\n",
            "Number of batches: 1294\n",
            "Number of Sentences Trained on: 20704\n",
            "Training loss epoch: 0.2986938382480941\n",
            "Training accuracy epoch: 1188.25\n",
            "Number of batches: 1295\n",
            "Number of Sentences Trained on: 20720\n",
            "Training loss epoch: 0.298587207652249\n",
            "Training accuracy epoch: 1189.25\n",
            "Number of batches: 1296\n",
            "Number of Sentences Trained on: 20736\n",
            "Training loss epoch: 0.29855045127490665\n",
            "Training accuracy epoch: 1190.125\n",
            "Number of batches: 1297\n",
            "Number of Sentences Trained on: 20752\n",
            "Training loss epoch: 0.2984063197458263\n",
            "Training accuracy epoch: 1191.0625\n",
            "Number of batches: 1298\n",
            "Number of Sentences Trained on: 20768\n",
            "Training loss epoch: 0.2984041175500327\n",
            "Training accuracy epoch: 1192.0\n",
            "Number of batches: 1299\n",
            "Number of Sentences Trained on: 20784\n",
            "Training loss epoch: 0.29834137480849254\n",
            "Training accuracy epoch: 1192.9375\n",
            "Number of batches: 1300\n",
            "Number of Sentences Trained on: 20800\n",
            "Training loss per 100 training steps: 0.29834863731206657\n",
            "Training loss epoch: 0.29834863731206657\n",
            "Training accuracy epoch: 1193.875\n",
            "Number of batches: 1301\n",
            "Number of Sentences Trained on: 20816\n",
            "Training loss epoch: 0.2984006611784826\n",
            "Training accuracy epoch: 1194.6875\n",
            "Number of batches: 1302\n",
            "Number of Sentences Trained on: 20832\n",
            "Training loss epoch: 0.2984180869670415\n",
            "Training accuracy epoch: 1195.5625\n",
            "Number of batches: 1303\n",
            "Number of Sentences Trained on: 20848\n",
            "Training loss epoch: 0.29821438031939596\n",
            "Training accuracy epoch: 1196.5625\n",
            "Number of batches: 1304\n",
            "Number of Sentences Trained on: 20864\n",
            "Training loss epoch: 0.2980025397306772\n",
            "Training accuracy epoch: 1197.5625\n",
            "Number of batches: 1305\n",
            "Number of Sentences Trained on: 20880\n",
            "Training loss epoch: 0.29778811816628137\n",
            "Training accuracy epoch: 1198.5625\n",
            "Number of batches: 1306\n",
            "Number of Sentences Trained on: 20896\n",
            "Training loss epoch: 0.297699532891298\n",
            "Training accuracy epoch: 1199.5\n",
            "Number of batches: 1307\n",
            "Number of Sentences Trained on: 20912\n",
            "Training loss epoch: 0.2979558024068423\n",
            "Training accuracy epoch: 1200.3125\n",
            "Number of batches: 1308\n",
            "Number of Sentences Trained on: 20928\n",
            "Training loss epoch: 0.2979797824229718\n",
            "Training accuracy epoch: 1201.1875\n",
            "Number of batches: 1309\n",
            "Number of Sentences Trained on: 20944\n",
            "Training loss epoch: 0.29780169947371454\n",
            "Training accuracy epoch: 1202.1875\n",
            "Number of batches: 1310\n",
            "Number of Sentences Trained on: 20960\n",
            "Training loss epoch: 0.297740323480409\n",
            "Training accuracy epoch: 1203.125\n",
            "Number of batches: 1311\n",
            "Number of Sentences Trained on: 20976\n",
            "Training loss epoch: 0.29803894698852673\n",
            "Training accuracy epoch: 1203.9375\n",
            "Number of batches: 1312\n",
            "Number of Sentences Trained on: 20992\n",
            "Training loss epoch: 0.29795259429228416\n",
            "Training accuracy epoch: 1204.875\n",
            "Number of batches: 1313\n",
            "Number of Sentences Trained on: 21008\n",
            "Training loss epoch: 0.2980137372253352\n",
            "Training accuracy epoch: 1205.75\n",
            "Number of batches: 1314\n",
            "Number of Sentences Trained on: 21024\n",
            "Training loss epoch: 0.29779987584077133\n",
            "Training accuracy epoch: 1206.75\n",
            "Number of batches: 1315\n",
            "Number of Sentences Trained on: 21040\n",
            "Training loss epoch: 0.29761666233161616\n",
            "Training accuracy epoch: 1207.75\n",
            "Number of batches: 1316\n",
            "Number of Sentences Trained on: 21056\n",
            "Training loss epoch: 0.29741651175194334\n",
            "Training accuracy epoch: 1208.75\n",
            "Number of batches: 1317\n",
            "Number of Sentences Trained on: 21072\n",
            "Training loss epoch: 0.29740778306055365\n",
            "Training accuracy epoch: 1209.6875\n",
            "Number of batches: 1318\n",
            "Number of Sentences Trained on: 21088\n",
            "Training loss epoch: 0.29719676754216384\n",
            "Training accuracy epoch: 1210.6875\n",
            "Number of batches: 1319\n",
            "Number of Sentences Trained on: 21104\n",
            "Training loss epoch: 0.297270653488566\n",
            "Training accuracy epoch: 1211.5625\n",
            "Number of batches: 1320\n",
            "Number of Sentences Trained on: 21120\n",
            "Training loss epoch: 0.2970970126953227\n",
            "Training accuracy epoch: 1212.5625\n",
            "Number of batches: 1321\n",
            "Number of Sentences Trained on: 21136\n",
            "Training loss epoch: 0.2969172324283604\n",
            "Training accuracy epoch: 1213.5\n",
            "Number of batches: 1322\n",
            "Number of Sentences Trained on: 21152\n",
            "Training loss epoch: 0.29681999045575513\n",
            "Training accuracy epoch: 1214.4375\n",
            "Number of batches: 1323\n",
            "Number of Sentences Trained on: 21168\n",
            "Training loss epoch: 0.2968017469509153\n",
            "Training accuracy epoch: 1215.4375\n",
            "Number of batches: 1324\n",
            "Number of Sentences Trained on: 21184\n",
            "Training loss epoch: 0.296856172783476\n",
            "Training accuracy epoch: 1216.375\n",
            "Number of batches: 1325\n",
            "Number of Sentences Trained on: 21200\n",
            "Training loss epoch: 0.296744941509024\n",
            "Training accuracy epoch: 1217.3125\n",
            "Number of batches: 1326\n",
            "Number of Sentences Trained on: 21216\n",
            "Training loss epoch: 0.29657409448645444\n",
            "Training accuracy epoch: 1218.3125\n",
            "Number of batches: 1327\n",
            "Number of Sentences Trained on: 21232\n",
            "Training loss epoch: 0.29657001515250114\n",
            "Training accuracy epoch: 1219.25\n",
            "Number of batches: 1328\n",
            "Number of Sentences Trained on: 21248\n",
            "Training loss epoch: 0.2963617124585638\n",
            "Training accuracy epoch: 1220.25\n",
            "Number of batches: 1329\n",
            "Number of Sentences Trained on: 21264\n",
            "Training loss epoch: 0.2962177266513831\n",
            "Training accuracy epoch: 1221.25\n",
            "Number of batches: 1330\n",
            "Number of Sentences Trained on: 21280\n",
            "Training loss epoch: 0.29612598838431553\n",
            "Training accuracy epoch: 1222.1875\n",
            "Number of batches: 1331\n",
            "Number of Sentences Trained on: 21296\n",
            "Training loss epoch: 0.2963009077157859\n",
            "Training accuracy epoch: 1223.0625\n",
            "Number of batches: 1332\n",
            "Number of Sentences Trained on: 21312\n",
            "Training loss epoch: 0.2960834489116656\n",
            "Training accuracy epoch: 1224.0625\n",
            "Number of batches: 1333\n",
            "Number of Sentences Trained on: 21328\n",
            "Training loss epoch: 0.29603205963332524\n",
            "Training accuracy epoch: 1224.9375\n",
            "Number of batches: 1334\n",
            "Number of Sentences Trained on: 21344\n",
            "Training loss epoch: 0.29583152023733555\n",
            "Training accuracy epoch: 1225.9375\n",
            "Number of batches: 1335\n",
            "Number of Sentences Trained on: 21360\n",
            "Training loss epoch: 0.2957206298527447\n",
            "Training accuracy epoch: 1226.875\n",
            "Number of batches: 1336\n",
            "Number of Sentences Trained on: 21376\n",
            "Training loss epoch: 0.29552078033500007\n",
            "Training accuracy epoch: 1227.875\n",
            "Number of batches: 1337\n",
            "Number of Sentences Trained on: 21392\n",
            "Training loss epoch: 0.29553320607858163\n",
            "Training accuracy epoch: 1228.75\n",
            "Number of batches: 1338\n",
            "Number of Sentences Trained on: 21408\n",
            "Training loss epoch: 0.2955055239974139\n",
            "Training accuracy epoch: 1229.6875\n",
            "Number of batches: 1339\n",
            "Number of Sentences Trained on: 21424\n",
            "Training loss epoch: 0.2953752694387378\n",
            "Training accuracy epoch: 1230.6875\n",
            "Number of batches: 1340\n",
            "Number of Sentences Trained on: 21440\n",
            "Training loss epoch: 0.29549691493162505\n",
            "Training accuracy epoch: 1231.5625\n",
            "Number of batches: 1341\n",
            "Number of Sentences Trained on: 21456\n",
            "Training loss epoch: 0.2952970881028942\n",
            "Training accuracy epoch: 1232.5625\n",
            "Number of batches: 1342\n",
            "Number of Sentences Trained on: 21472\n",
            "Training loss epoch: 0.2950957111656466\n",
            "Training accuracy epoch: 1233.5625\n",
            "Number of batches: 1343\n",
            "Number of Sentences Trained on: 21488\n",
            "Training loss epoch: 0.2949298445337696\n",
            "Training accuracy epoch: 1234.5625\n",
            "Number of batches: 1344\n",
            "Number of Sentences Trained on: 21504\n",
            "Training loss epoch: 0.29476760430988547\n",
            "Training accuracy epoch: 1235.5\n",
            "Number of batches: 1345\n",
            "Number of Sentences Trained on: 21520\n",
            "Training loss epoch: 0.29480538046870375\n",
            "Training accuracy epoch: 1236.375\n",
            "Number of batches: 1346\n",
            "Number of Sentences Trained on: 21536\n",
            "Training loss epoch: 0.29467869315603396\n",
            "Training accuracy epoch: 1237.3125\n",
            "Number of batches: 1347\n",
            "Number of Sentences Trained on: 21552\n",
            "Training loss epoch: 0.2944700439261599\n",
            "Training accuracy epoch: 1238.3125\n",
            "Number of batches: 1348\n",
            "Number of Sentences Trained on: 21568\n",
            "Training loss epoch: 0.2945699862669118\n",
            "Training accuracy epoch: 1239.125\n",
            "Number of batches: 1349\n",
            "Number of Sentences Trained on: 21584\n",
            "Training loss epoch: 0.2943691619802956\n",
            "Training accuracy epoch: 1240.125\n",
            "Number of batches: 1350\n",
            "Number of Sentences Trained on: 21600\n",
            "Training loss epoch: 0.2944177787883316\n",
            "Training accuracy epoch: 1241.0\n",
            "Number of batches: 1351\n",
            "Number of Sentences Trained on: 21616\n",
            "Training loss epoch: 0.29429443444114334\n",
            "Training accuracy epoch: 1241.9375\n",
            "Number of batches: 1352\n",
            "Number of Sentences Trained on: 21632\n",
            "Training loss epoch: 0.2941792305443279\n",
            "Training accuracy epoch: 1242.875\n",
            "Number of batches: 1353\n",
            "Number of Sentences Trained on: 21648\n",
            "Training loss epoch: 0.2940694886208835\n",
            "Training accuracy epoch: 1243.8125\n",
            "Number of batches: 1354\n",
            "Number of Sentences Trained on: 21664\n",
            "Training loss epoch: 0.293894382188186\n",
            "Training accuracy epoch: 1244.8125\n",
            "Number of batches: 1355\n",
            "Number of Sentences Trained on: 21680\n",
            "Training loss epoch: 0.2937280084803195\n",
            "Training accuracy epoch: 1245.8125\n",
            "Number of batches: 1356\n",
            "Number of Sentences Trained on: 21696\n",
            "Training loss epoch: 0.2935399507860775\n",
            "Training accuracy epoch: 1246.8125\n",
            "Number of batches: 1357\n",
            "Number of Sentences Trained on: 21712\n",
            "Training loss epoch: 0.2934190558258647\n",
            "Training accuracy epoch: 1247.75\n",
            "Number of batches: 1358\n",
            "Number of Sentences Trained on: 21728\n",
            "Training loss epoch: 0.29340127152010437\n",
            "Training accuracy epoch: 1248.6875\n",
            "Number of batches: 1359\n",
            "Number of Sentences Trained on: 21744\n",
            "Training loss epoch: 0.29321594738867135\n",
            "Training accuracy epoch: 1249.6875\n",
            "Number of batches: 1360\n",
            "Number of Sentences Trained on: 21760\n",
            "Training loss epoch: 0.2931462561234033\n",
            "Training accuracy epoch: 1250.625\n",
            "Number of batches: 1361\n",
            "Number of Sentences Trained on: 21776\n",
            "Training loss epoch: 0.2930426666945241\n",
            "Training accuracy epoch: 1251.5625\n",
            "Number of batches: 1362\n",
            "Number of Sentences Trained on: 21792\n",
            "Training loss epoch: 0.292977077812667\n",
            "Training accuracy epoch: 1252.5\n",
            "Number of batches: 1363\n",
            "Number of Sentences Trained on: 21808\n",
            "Training loss epoch: 0.29276921139767836\n",
            "Training accuracy epoch: 1253.5\n",
            "Number of batches: 1364\n",
            "Number of Sentences Trained on: 21824\n",
            "Training loss epoch: 0.29273510050888246\n",
            "Training accuracy epoch: 1254.375\n",
            "Number of batches: 1365\n",
            "Number of Sentences Trained on: 21840\n",
            "Training loss epoch: 0.29264298694052815\n",
            "Training accuracy epoch: 1255.25\n",
            "Number of batches: 1366\n",
            "Number of Sentences Trained on: 21856\n",
            "Training loss epoch: 0.29263662966683607\n",
            "Training accuracy epoch: 1256.1875\n",
            "Number of batches: 1367\n",
            "Number of Sentences Trained on: 21872\n",
            "Training loss epoch: 0.29242850923420566\n",
            "Training accuracy epoch: 1257.1875\n",
            "Number of batches: 1368\n",
            "Number of Sentences Trained on: 21888\n",
            "Training loss epoch: 0.2922425580866107\n",
            "Training accuracy epoch: 1258.1875\n",
            "Number of batches: 1369\n",
            "Number of Sentences Trained on: 21904\n",
            "Training loss epoch: 0.2921365281118311\n",
            "Training accuracy epoch: 1259.125\n",
            "Number of batches: 1370\n",
            "Number of Sentences Trained on: 21920\n",
            "Training loss epoch: 0.2919582160614991\n",
            "Training accuracy epoch: 1260.125\n",
            "Number of batches: 1371\n",
            "Number of Sentences Trained on: 21936\n",
            "Training loss epoch: 0.29193305292875443\n",
            "Training accuracy epoch: 1261.0\n",
            "Number of batches: 1372\n",
            "Number of Sentences Trained on: 21952\n",
            "Training loss epoch: 0.29176845837544546\n",
            "Training accuracy epoch: 1262.0\n",
            "Number of batches: 1373\n",
            "Number of Sentences Trained on: 21968\n",
            "Training loss epoch: 0.2915655817427121\n",
            "Training accuracy epoch: 1263.0\n",
            "Number of batches: 1374\n",
            "Number of Sentences Trained on: 21984\n",
            "Training loss epoch: 0.29136413055522875\n",
            "Training accuracy epoch: 1264.0\n",
            "Number of batches: 1375\n",
            "Number of Sentences Trained on: 22000\n",
            "Training loss epoch: 0.29126508643736976\n",
            "Training accuracy epoch: 1265.0\n",
            "Number of batches: 1376\n",
            "Number of Sentences Trained on: 22016\n",
            "Training loss epoch: 0.29107620960578984\n",
            "Training accuracy epoch: 1266.0\n",
            "Number of batches: 1377\n",
            "Number of Sentences Trained on: 22032\n",
            "Training loss epoch: 0.290975221108337\n",
            "Training accuracy epoch: 1266.9375\n",
            "Number of batches: 1378\n",
            "Number of Sentences Trained on: 22048\n",
            "Training loss epoch: 0.2907760470451826\n",
            "Training accuracy epoch: 1267.9375\n",
            "Number of batches: 1379\n",
            "Number of Sentences Trained on: 22064\n",
            "Training loss epoch: 0.2906104567798152\n",
            "Training accuracy epoch: 1268.875\n",
            "Number of batches: 1380\n",
            "Number of Sentences Trained on: 22080\n",
            "Training loss epoch: 0.290749989984089\n",
            "Training accuracy epoch: 1269.6875\n",
            "Number of batches: 1381\n",
            "Number of Sentences Trained on: 22096\n",
            "Training loss epoch: 0.2905783329466317\n",
            "Training accuracy epoch: 1270.6875\n",
            "Number of batches: 1382\n",
            "Number of Sentences Trained on: 22112\n",
            "Training loss epoch: 0.2906148249764246\n",
            "Training accuracy epoch: 1271.625\n",
            "Number of batches: 1383\n",
            "Number of Sentences Trained on: 22128\n",
            "Training loss epoch: 0.2904717722595427\n",
            "Training accuracy epoch: 1272.625\n",
            "Number of batches: 1384\n",
            "Number of Sentences Trained on: 22144\n",
            "Training loss epoch: 0.2904517014749644\n",
            "Training accuracy epoch: 1273.5625\n",
            "Number of batches: 1385\n",
            "Number of Sentences Trained on: 22160\n",
            "Training loss epoch: 0.2902702947038335\n",
            "Training accuracy epoch: 1274.5625\n",
            "Number of batches: 1386\n",
            "Number of Sentences Trained on: 22176\n",
            "Training loss epoch: 0.29017093649272363\n",
            "Training accuracy epoch: 1275.5625\n",
            "Number of batches: 1387\n",
            "Number of Sentences Trained on: 22192\n",
            "Training loss epoch: 0.2902694855808611\n",
            "Training accuracy epoch: 1276.4375\n",
            "Number of batches: 1388\n",
            "Number of Sentences Trained on: 22208\n",
            "Training loss epoch: 0.2902316195049502\n",
            "Training accuracy epoch: 1277.375\n",
            "Number of batches: 1389\n",
            "Number of Sentences Trained on: 22224\n",
            "Training loss epoch: 0.29015241218663806\n",
            "Training accuracy epoch: 1278.25\n",
            "Number of batches: 1390\n",
            "Number of Sentences Trained on: 22240\n",
            "Training loss epoch: 0.29014422626674474\n",
            "Training accuracy epoch: 1279.125\n",
            "Number of batches: 1391\n",
            "Number of Sentences Trained on: 22256\n",
            "Training loss epoch: 0.2900028503874595\n",
            "Training accuracy epoch: 1280.0625\n",
            "Number of batches: 1392\n",
            "Number of Sentences Trained on: 22272\n",
            "Training loss epoch: 0.2899016016092732\n",
            "Training accuracy epoch: 1281.0\n",
            "Number of batches: 1393\n",
            "Number of Sentences Trained on: 22288\n",
            "Training loss epoch: 0.28974285442782954\n",
            "Training accuracy epoch: 1282.0\n",
            "Number of batches: 1394\n",
            "Number of Sentences Trained on: 22304\n",
            "Training loss epoch: 0.2895386721291262\n",
            "Training accuracy epoch: 1283.0\n",
            "Number of batches: 1395\n",
            "Number of Sentences Trained on: 22320\n",
            "Training loss epoch: 0.2893432413239865\n",
            "Training accuracy epoch: 1284.0\n",
            "Number of batches: 1396\n",
            "Number of Sentences Trained on: 22336\n",
            "Training loss epoch: 0.28924077631853673\n",
            "Training accuracy epoch: 1285.0\n",
            "Number of batches: 1397\n",
            "Number of Sentences Trained on: 22352\n",
            "Training loss epoch: 0.28907494053167315\n",
            "Training accuracy epoch: 1286.0\n",
            "Number of batches: 1398\n",
            "Number of Sentences Trained on: 22368\n",
            "Training loss epoch: 0.2889599415937637\n",
            "Training accuracy epoch: 1287.0\n",
            "Number of batches: 1399\n",
            "Number of Sentences Trained on: 22384\n",
            "Training loss epoch: 0.28876689684377715\n",
            "Training accuracy epoch: 1288.0\n",
            "Number of batches: 1400\n",
            "Number of Sentences Trained on: 22400\n",
            "Training loss per 100 training steps: 0.2885800929165723\n",
            "Training loss epoch: 0.2885800929165723\n",
            "Training accuracy epoch: 1289.0\n",
            "Number of batches: 1401\n",
            "Number of Sentences Trained on: 22416\n",
            "Training loss epoch: 0.2887238411215558\n",
            "Training accuracy epoch: 1289.8125\n",
            "Number of batches: 1402\n",
            "Number of Sentences Trained on: 22432\n",
            "Training loss epoch: 0.28864338363475\n",
            "Training accuracy epoch: 1290.75\n",
            "Number of batches: 1403\n",
            "Number of Sentences Trained on: 22448\n",
            "Training loss epoch: 0.2884595001207935\n",
            "Training accuracy epoch: 1291.75\n",
            "Number of batches: 1404\n",
            "Number of Sentences Trained on: 22464\n",
            "Training loss epoch: 0.28865294415981\n",
            "Training accuracy epoch: 1292.5625\n",
            "Number of batches: 1405\n",
            "Number of Sentences Trained on: 22480\n",
            "Training loss epoch: 0.2886906715771011\n",
            "Training accuracy epoch: 1293.4375\n",
            "Number of batches: 1406\n",
            "Number of Sentences Trained on: 22496\n",
            "Training loss epoch: 0.2887044274033059\n",
            "Training accuracy epoch: 1294.3125\n",
            "Number of batches: 1407\n",
            "Number of Sentences Trained on: 22512\n",
            "Training loss epoch: 0.28852351109923224\n",
            "Training accuracy epoch: 1295.3125\n",
            "Number of batches: 1408\n",
            "Number of Sentences Trained on: 22528\n",
            "Training loss epoch: 0.2885694507016378\n",
            "Training accuracy epoch: 1296.1875\n",
            "Number of batches: 1409\n",
            "Number of Sentences Trained on: 22544\n",
            "Training loss epoch: 0.28842300481579086\n",
            "Training accuracy epoch: 1297.1875\n",
            "Number of batches: 1410\n",
            "Number of Sentences Trained on: 22560\n",
            "Training loss epoch: 0.28827330879378643\n",
            "Training accuracy epoch: 1298.1875\n",
            "Number of batches: 1411\n",
            "Number of Sentences Trained on: 22576\n",
            "Training loss epoch: 0.2882562518791261\n",
            "Training accuracy epoch: 1299.0625\n",
            "Number of batches: 1412\n",
            "Number of Sentences Trained on: 22592\n",
            "Training loss epoch: 0.2880777617618547\n",
            "Training accuracy epoch: 1300.0625\n",
            "Number of batches: 1413\n",
            "Number of Sentences Trained on: 22608\n",
            "Training loss epoch: 0.2878954759258856\n",
            "Training accuracy epoch: 1301.0625\n",
            "Number of batches: 1414\n",
            "Number of Sentences Trained on: 22624\n",
            "Training loss epoch: 0.28771002761585235\n",
            "Training accuracy epoch: 1302.0625\n",
            "Number of batches: 1415\n",
            "Number of Sentences Trained on: 22640\n",
            "Training loss epoch: 0.28753440559639115\n",
            "Training accuracy epoch: 1303.0625\n",
            "Number of batches: 1416\n",
            "Number of Sentences Trained on: 22656\n",
            "Training loss epoch: 0.2873793535529989\n",
            "Training accuracy epoch: 1304.0625\n",
            "Number of batches: 1417\n",
            "Number of Sentences Trained on: 22672\n",
            "Training loss epoch: 0.2875876388951285\n",
            "Training accuracy epoch: 1304.9375\n",
            "Number of batches: 1418\n",
            "Number of Sentences Trained on: 22688\n",
            "Training loss epoch: 0.28745845918772694\n",
            "Training accuracy epoch: 1305.875\n",
            "Number of batches: 1419\n",
            "Number of Sentences Trained on: 22704\n",
            "Training loss epoch: 0.2873011800585131\n",
            "Training accuracy epoch: 1306.875\n",
            "Number of batches: 1420\n",
            "Number of Sentences Trained on: 22720\n",
            "Training loss epoch: 0.2871034309418084\n",
            "Training accuracy epoch: 1307.875\n",
            "Number of batches: 1421\n",
            "Number of Sentences Trained on: 22736\n",
            "Training loss epoch: 0.2870728177311221\n",
            "Training accuracy epoch: 1308.8125\n",
            "Number of batches: 1422\n",
            "Number of Sentences Trained on: 22752\n",
            "Training loss epoch: 0.286890682164522\n",
            "Training accuracy epoch: 1309.8125\n",
            "Number of batches: 1423\n",
            "Number of Sentences Trained on: 22768\n",
            "Training loss epoch: 0.2867035027125751\n",
            "Training accuracy epoch: 1310.8125\n",
            "Number of batches: 1424\n",
            "Number of Sentences Trained on: 22784\n",
            "Training loss epoch: 0.2865760626032818\n",
            "Training accuracy epoch: 1311.75\n",
            "Number of batches: 1425\n",
            "Number of Sentences Trained on: 22800\n",
            "Training loss epoch: 0.2864025082980605\n",
            "Training accuracy epoch: 1312.75\n",
            "Number of batches: 1426\n",
            "Number of Sentences Trained on: 22816\n",
            "Training loss epoch: 0.28650408560872215\n",
            "Training accuracy epoch: 1313.625\n",
            "Number of batches: 1427\n",
            "Number of Sentences Trained on: 22832\n",
            "Training loss epoch: 0.2863215634078231\n",
            "Training accuracy epoch: 1314.625\n",
            "Number of batches: 1428\n",
            "Number of Sentences Trained on: 22848\n",
            "Training loss epoch: 0.28614114521610945\n",
            "Training accuracy epoch: 1315.625\n",
            "Number of batches: 1429\n",
            "Number of Sentences Trained on: 22864\n",
            "Training loss epoch: 0.28597245211452504\n",
            "Training accuracy epoch: 1316.5625\n",
            "Number of batches: 1430\n",
            "Number of Sentences Trained on: 22880\n",
            "Training loss epoch: 0.2858636078852991\n",
            "Training accuracy epoch: 1317.5\n",
            "Number of batches: 1431\n",
            "Number of Sentences Trained on: 22896\n",
            "Training loss epoch: 0.2856999445376871\n",
            "Training accuracy epoch: 1318.5\n",
            "Number of batches: 1432\n",
            "Number of Sentences Trained on: 22912\n",
            "Training loss epoch: 0.28552864566321867\n",
            "Training accuracy epoch: 1319.5\n",
            "Number of batches: 1433\n",
            "Number of Sentences Trained on: 22928\n",
            "Training loss epoch: 0.2853461463797339\n",
            "Training accuracy epoch: 1320.5\n",
            "Number of batches: 1434\n",
            "Number of Sentences Trained on: 22944\n",
            "Training loss epoch: 0.2855619421952407\n",
            "Training accuracy epoch: 1321.3125\n",
            "Number of batches: 1435\n",
            "Number of Sentences Trained on: 22960\n",
            "Training loss epoch: 0.28538495319873397\n",
            "Training accuracy epoch: 1322.3125\n",
            "Number of batches: 1436\n",
            "Number of Sentences Trained on: 22976\n",
            "Training loss epoch: 0.2853429936026155\n",
            "Training accuracy epoch: 1323.25\n",
            "Number of batches: 1437\n",
            "Number of Sentences Trained on: 22992\n",
            "Training loss epoch: 0.28548878133951094\n",
            "Training accuracy epoch: 1324.125\n",
            "Number of batches: 1438\n",
            "Number of Sentences Trained on: 23008\n",
            "Training loss epoch: 0.2854296328235208\n",
            "Training accuracy epoch: 1325.0625\n",
            "Number of batches: 1439\n",
            "Number of Sentences Trained on: 23024\n",
            "Training loss epoch: 0.2856665539351525\n",
            "Training accuracy epoch: 1325.9375\n",
            "Number of batches: 1440\n",
            "Number of Sentences Trained on: 23040\n",
            "Training loss epoch: 0.2856246259462251\n",
            "Training accuracy epoch: 1326.875\n",
            "Number of batches: 1441\n",
            "Number of Sentences Trained on: 23056\n",
            "Training loss epoch: 0.28564004339671717\n",
            "Training accuracy epoch: 1327.8125\n",
            "Number of batches: 1442\n",
            "Number of Sentences Trained on: 23072\n",
            "Training loss epoch: 0.2856188065382734\n",
            "Training accuracy epoch: 1328.6875\n",
            "Number of batches: 1443\n",
            "Number of Sentences Trained on: 23088\n",
            "Training loss epoch: 0.28553890509155533\n",
            "Training accuracy epoch: 1329.625\n",
            "Number of batches: 1444\n",
            "Number of Sentences Trained on: 23104\n",
            "Training loss epoch: 0.28552194600731495\n",
            "Training accuracy epoch: 1330.5\n",
            "Number of batches: 1445\n",
            "Number of Sentences Trained on: 23120\n",
            "Training loss epoch: 0.2853517822234055\n",
            "Training accuracy epoch: 1331.4375\n",
            "Number of batches: 1446\n",
            "Number of Sentences Trained on: 23136\n",
            "Training loss epoch: 0.28519781382880594\n",
            "Training accuracy epoch: 1332.4375\n",
            "Number of batches: 1447\n",
            "Number of Sentences Trained on: 23152\n",
            "Training loss epoch: 0.2850470060475546\n",
            "Training accuracy epoch: 1333.4375\n",
            "Number of batches: 1448\n",
            "Number of Sentences Trained on: 23168\n",
            "Training loss epoch: 0.2849751420481692\n",
            "Training accuracy epoch: 1334.3125\n",
            "Number of batches: 1449\n",
            "Number of Sentences Trained on: 23184\n",
            "Training loss epoch: 0.28485611031827485\n",
            "Training accuracy epoch: 1335.3125\n",
            "Number of batches: 1450\n",
            "Number of Sentences Trained on: 23200\n",
            "Training loss epoch: 0.28471616982706377\n",
            "Training accuracy epoch: 1336.3125\n",
            "Number of batches: 1451\n",
            "Number of Sentences Trained on: 23216\n",
            "Training loss epoch: 0.28466305312201556\n",
            "Training accuracy epoch: 1337.25\n",
            "Number of batches: 1452\n",
            "Number of Sentences Trained on: 23232\n",
            "Training loss epoch: 0.28463000077619655\n",
            "Training accuracy epoch: 1338.1875\n",
            "Number of batches: 1453\n",
            "Number of Sentences Trained on: 23248\n",
            "Training loss epoch: 0.2844645243648573\n",
            "Training accuracy epoch: 1339.1875\n",
            "Number of batches: 1454\n",
            "Number of Sentences Trained on: 23264\n",
            "Training loss epoch: 0.28429421290172796\n",
            "Training accuracy epoch: 1340.1875\n",
            "Number of batches: 1455\n",
            "Number of Sentences Trained on: 23280\n",
            "Training loss epoch: 0.28442029003341746\n",
            "Training accuracy epoch: 1341.0625\n",
            "Number of batches: 1456\n",
            "Number of Sentences Trained on: 23296\n",
            "Training loss epoch: 0.2842648425356817\n",
            "Training accuracy epoch: 1342.0\n",
            "Number of batches: 1457\n",
            "Number of Sentences Trained on: 23312\n",
            "Training loss epoch: 0.284459440390527\n",
            "Training accuracy epoch: 1342.8125\n",
            "Number of batches: 1458\n",
            "Number of Sentences Trained on: 23328\n",
            "Training loss epoch: 0.2845075594378103\n",
            "Training accuracy epoch: 1343.75\n",
            "Number of batches: 1459\n",
            "Number of Sentences Trained on: 23344\n",
            "Training loss epoch: 0.28441204188091757\n",
            "Training accuracy epoch: 1344.6875\n",
            "Number of batches: 1460\n",
            "Number of Sentences Trained on: 23360\n",
            "Training loss epoch: 0.28429912994306566\n",
            "Training accuracy epoch: 1345.625\n",
            "Number of batches: 1461\n",
            "Number of Sentences Trained on: 23376\n",
            "Training loss epoch: 0.284226305311719\n",
            "Training accuracy epoch: 1346.5625\n",
            "Number of batches: 1462\n",
            "Number of Sentences Trained on: 23392\n",
            "Training loss epoch: 0.28409556231492655\n",
            "Training accuracy epoch: 1347.5\n",
            "Number of batches: 1463\n",
            "Number of Sentences Trained on: 23408\n",
            "Training loss epoch: 0.2839365056082937\n",
            "Training accuracy epoch: 1348.5\n",
            "Number of batches: 1464\n",
            "Number of Sentences Trained on: 23424\n",
            "Training loss epoch: 0.28391143297204435\n",
            "Training accuracy epoch: 1349.375\n",
            "Number of batches: 1465\n",
            "Number of Sentences Trained on: 23440\n",
            "Training loss epoch: 0.2838239201219399\n",
            "Training accuracy epoch: 1350.3125\n",
            "Number of batches: 1466\n",
            "Number of Sentences Trained on: 23456\n",
            "Training loss epoch: 0.283657195434119\n",
            "Training accuracy epoch: 1351.3125\n",
            "Number of batches: 1467\n",
            "Number of Sentences Trained on: 23472\n",
            "Training loss epoch: 0.2835695802488514\n",
            "Training accuracy epoch: 1352.25\n",
            "Number of batches: 1468\n",
            "Number of Sentences Trained on: 23488\n",
            "Training loss epoch: 0.28365721992819926\n",
            "Training accuracy epoch: 1353.125\n",
            "Number of batches: 1469\n",
            "Number of Sentences Trained on: 23504\n",
            "Training loss epoch: 0.28349534644948027\n",
            "Training accuracy epoch: 1354.125\n",
            "Number of batches: 1470\n",
            "Number of Sentences Trained on: 23520\n",
            "Training loss epoch: 0.283634344251658\n",
            "Training accuracy epoch: 1355.0\n",
            "Number of batches: 1471\n",
            "Number of Sentences Trained on: 23536\n",
            "Training loss epoch: 0.28348717514872196\n",
            "Training accuracy epoch: 1356.0\n",
            "Number of batches: 1472\n",
            "Number of Sentences Trained on: 23552\n",
            "Training loss epoch: 0.2833257175742371\n",
            "Training accuracy epoch: 1357.0\n",
            "Number of batches: 1473\n",
            "Number of Sentences Trained on: 23568\n",
            "Training loss epoch: 0.2831479489397045\n",
            "Training accuracy epoch: 1358.0\n",
            "Number of batches: 1474\n",
            "Number of Sentences Trained on: 23584\n",
            "Training loss epoch: 0.28321710332754557\n",
            "Training accuracy epoch: 1358.9375\n",
            "Number of batches: 1475\n",
            "Number of Sentences Trained on: 23600\n",
            "Training loss epoch: 0.28304794653625476\n",
            "Training accuracy epoch: 1359.9375\n",
            "Number of batches: 1476\n",
            "Number of Sentences Trained on: 23616\n",
            "Training loss epoch: 0.282985764130327\n",
            "Training accuracy epoch: 1360.875\n",
            "Number of batches: 1477\n",
            "Number of Sentences Trained on: 23632\n",
            "Training loss epoch: 0.2828360318142608\n",
            "Training accuracy epoch: 1361.875\n",
            "Number of batches: 1478\n",
            "Number of Sentences Trained on: 23648\n",
            "Training loss epoch: 0.28269596208495223\n",
            "Training accuracy epoch: 1362.875\n",
            "Number of batches: 1479\n",
            "Number of Sentences Trained on: 23664\n",
            "Training loss epoch: 0.28265295037779503\n",
            "Training accuracy epoch: 1363.8125\n",
            "Number of batches: 1480\n",
            "Number of Sentences Trained on: 23680\n",
            "Training loss epoch: 0.28255307473208324\n",
            "Training accuracy epoch: 1364.75\n",
            "Number of batches: 1481\n",
            "Number of Sentences Trained on: 23696\n",
            "Training loss epoch: 0.2825078183685162\n",
            "Training accuracy epoch: 1365.6875\n",
            "Number of batches: 1482\n",
            "Number of Sentences Trained on: 23712\n",
            "Training loss epoch: 0.2823271570800705\n",
            "Training accuracy epoch: 1366.6875\n",
            "Number of batches: 1483\n",
            "Number of Sentences Trained on: 23728\n",
            "Training loss epoch: 0.28222737871606224\n",
            "Training accuracy epoch: 1367.625\n",
            "Number of batches: 1484\n",
            "Number of Sentences Trained on: 23744\n",
            "Training loss epoch: 0.2820642537349578\n",
            "Training accuracy epoch: 1368.625\n",
            "Number of batches: 1485\n",
            "Number of Sentences Trained on: 23760\n",
            "Training loss epoch: 0.28201605607304236\n",
            "Training accuracy epoch: 1369.5625\n",
            "Number of batches: 1486\n",
            "Number of Sentences Trained on: 23776\n",
            "Training loss epoch: 0.28188099547382633\n",
            "Training accuracy epoch: 1370.5625\n",
            "Number of batches: 1487\n",
            "Number of Sentences Trained on: 23792\n",
            "Training loss epoch: 0.28172228513229186\n",
            "Training accuracy epoch: 1371.5625\n",
            "Number of batches: 1488\n",
            "Number of Sentences Trained on: 23808\n",
            "Training loss epoch: 0.28167567618630324\n",
            "Training accuracy epoch: 1372.5\n",
            "Number of batches: 1489\n",
            "Number of Sentences Trained on: 23824\n",
            "Training loss epoch: 0.28151736150045675\n",
            "Training accuracy epoch: 1373.5\n",
            "Number of batches: 1490\n",
            "Number of Sentences Trained on: 23840\n",
            "Training loss epoch: 0.2814038870609437\n",
            "Training accuracy epoch: 1374.4375\n",
            "Number of batches: 1491\n",
            "Number of Sentences Trained on: 23856\n",
            "Training loss epoch: 0.28125995659330566\n",
            "Training accuracy epoch: 1375.4375\n",
            "Number of batches: 1492\n",
            "Number of Sentences Trained on: 23872\n",
            "Training loss epoch: 0.2812169388754384\n",
            "Training accuracy epoch: 1376.375\n",
            "Number of batches: 1493\n",
            "Number of Sentences Trained on: 23888\n",
            "Training loss epoch: 0.28123166531896643\n",
            "Training accuracy epoch: 1377.25\n",
            "Number of batches: 1494\n",
            "Number of Sentences Trained on: 23904\n",
            "Training loss epoch: 0.2810733023803544\n",
            "Training accuracy epoch: 1378.25\n",
            "Number of batches: 1495\n",
            "Number of Sentences Trained on: 23920\n",
            "Training loss epoch: 0.28090732894124426\n",
            "Training accuracy epoch: 1379.25\n",
            "Number of batches: 1496\n",
            "Number of Sentences Trained on: 23936\n",
            "Training loss epoch: 0.2807529865236519\n",
            "Training accuracy epoch: 1380.25\n",
            "Number of batches: 1497\n",
            "Number of Sentences Trained on: 23952\n",
            "Training loss epoch: 0.28080966638292976\n",
            "Training accuracy epoch: 1381.125\n",
            "Number of batches: 1498\n",
            "Number of Sentences Trained on: 23968\n",
            "Training loss epoch: 0.28082457752749224\n",
            "Training accuracy epoch: 1381.9375\n",
            "Number of batches: 1499\n",
            "Number of Sentences Trained on: 23984\n",
            "Training loss epoch: 0.28072345311784497\n",
            "Training accuracy epoch: 1382.875\n",
            "Number of batches: 1500\n",
            "Number of Sentences Trained on: 24000\n",
            "Training loss per 100 training steps: 0.28054682663465036\n",
            "Training loss epoch: 0.28054682663465036\n",
            "Training accuracy epoch: 1383.875\n",
            "Number of batches: 1501\n",
            "Number of Sentences Trained on: 24016\n",
            "Training loss epoch: 0.2804817353780152\n",
            "Training accuracy epoch: 1384.875\n",
            "Number of batches: 1502\n",
            "Number of Sentences Trained on: 24032\n",
            "Training loss epoch: 0.280437343662477\n",
            "Training accuracy epoch: 1385.8125\n",
            "Number of batches: 1503\n",
            "Number of Sentences Trained on: 24048\n",
            "Training loss epoch: 0.28040661480732293\n",
            "Training accuracy epoch: 1386.75\n",
            "Number of batches: 1504\n",
            "Number of Sentences Trained on: 24064\n",
            "Training loss epoch: 0.2805181953223713\n",
            "Training accuracy epoch: 1387.625\n",
            "Number of batches: 1505\n",
            "Number of Sentences Trained on: 24080\n",
            "Training loss epoch: 0.28034453990542707\n",
            "Training accuracy epoch: 1388.625\n",
            "Number of batches: 1506\n",
            "Number of Sentences Trained on: 24096\n",
            "Training loss epoch: 0.2802907778151017\n",
            "Training accuracy epoch: 1389.5625\n",
            "Number of batches: 1507\n",
            "Number of Sentences Trained on: 24112\n",
            "Training loss epoch: 0.2801359015494086\n",
            "Training accuracy epoch: 1390.5625\n",
            "Number of batches: 1508\n",
            "Number of Sentences Trained on: 24128\n",
            "Training loss epoch: 0.28001612640461826\n",
            "Training accuracy epoch: 1391.5\n",
            "Number of batches: 1509\n",
            "Number of Sentences Trained on: 24144\n",
            "Training loss epoch: 0.27990873043844783\n",
            "Training accuracy epoch: 1392.5\n",
            "Number of batches: 1510\n",
            "Number of Sentences Trained on: 24160\n",
            "Training loss epoch: 0.2797641995699471\n",
            "Training accuracy epoch: 1393.4375\n",
            "Number of batches: 1511\n",
            "Number of Sentences Trained on: 24176\n",
            "Training loss epoch: 0.2796752308307477\n",
            "Training accuracy epoch: 1394.375\n",
            "Number of batches: 1512\n",
            "Number of Sentences Trained on: 24192\n",
            "Training loss epoch: 0.2796424285684739\n",
            "Training accuracy epoch: 1395.3125\n",
            "Number of batches: 1513\n",
            "Number of Sentences Trained on: 24208\n",
            "Training loss epoch: 0.2796950974243926\n",
            "Training accuracy epoch: 1396.125\n",
            "Number of batches: 1514\n",
            "Number of Sentences Trained on: 24224\n",
            "Training loss epoch: 0.2795716683939714\n",
            "Training accuracy epoch: 1397.125\n",
            "Number of batches: 1515\n",
            "Number of Sentences Trained on: 24240\n",
            "Training loss epoch: 0.27949624291239034\n",
            "Training accuracy epoch: 1398.0\n",
            "Number of batches: 1516\n",
            "Number of Sentences Trained on: 24256\n",
            "Training loss epoch: 0.2794168534806525\n",
            "Training accuracy epoch: 1398.9375\n",
            "Number of batches: 1517\n",
            "Number of Sentences Trained on: 24272\n",
            "Training loss epoch: 0.2793600409815017\n",
            "Training accuracy epoch: 1399.875\n",
            "Number of batches: 1518\n",
            "Number of Sentences Trained on: 24288\n",
            "Training loss epoch: 0.27919963262012626\n",
            "Training accuracy epoch: 1400.875\n",
            "Number of batches: 1519\n",
            "Number of Sentences Trained on: 24304\n",
            "Training loss epoch: 0.2792110190270346\n",
            "Training accuracy epoch: 1401.8125\n",
            "Number of batches: 1520\n",
            "Number of Sentences Trained on: 24320\n",
            "Training loss epoch: 0.27906820783330366\n",
            "Training accuracy epoch: 1402.8125\n",
            "Number of batches: 1521\n",
            "Number of Sentences Trained on: 24336\n",
            "Training loss epoch: 0.278904163781321\n",
            "Training accuracy epoch: 1403.8125\n",
            "Number of batches: 1522\n",
            "Number of Sentences Trained on: 24352\n",
            "Training loss epoch: 0.27873745010994516\n",
            "Training accuracy epoch: 1404.8125\n",
            "Number of batches: 1523\n",
            "Number of Sentences Trained on: 24368\n",
            "Training loss epoch: 0.2785624440223482\n",
            "Training accuracy epoch: 1405.8125\n",
            "Number of batches: 1524\n",
            "Number of Sentences Trained on: 24384\n",
            "Training loss epoch: 0.27855883437315704\n",
            "Training accuracy epoch: 1406.75\n",
            "Number of batches: 1525\n",
            "Number of Sentences Trained on: 24400\n",
            "Training loss epoch: 0.2784849011591302\n",
            "Training accuracy epoch: 1407.75\n",
            "Number of batches: 1526\n",
            "Number of Sentences Trained on: 24416\n",
            "Training loss epoch: 0.2783872079718358\n",
            "Training accuracy epoch: 1408.6875\n",
            "Number of batches: 1527\n",
            "Number of Sentences Trained on: 24432\n",
            "Training loss epoch: 0.27834527755457256\n",
            "Training accuracy epoch: 1409.5625\n",
            "Number of batches: 1528\n",
            "Number of Sentences Trained on: 24448\n",
            "Training loss epoch: 0.2785031858187819\n",
            "Training accuracy epoch: 1410.3125\n",
            "Number of batches: 1529\n",
            "Number of Sentences Trained on: 24464\n",
            "Training loss epoch: 0.278551808537404\n",
            "Training accuracy epoch: 1411.25\n",
            "Number of batches: 1530\n",
            "Number of Sentences Trained on: 24480\n",
            "Training loss epoch: 0.2786066974324657\n",
            "Training accuracy epoch: 1412.1875\n",
            "Number of batches: 1531\n",
            "Number of Sentences Trained on: 24496\n",
            "Training loss epoch: 0.278564953347048\n",
            "Training accuracy epoch: 1413.125\n",
            "Number of batches: 1532\n",
            "Number of Sentences Trained on: 24512\n",
            "Training loss epoch: 0.2784932997908178\n",
            "Training accuracy epoch: 1414.0625\n",
            "Number of batches: 1533\n",
            "Number of Sentences Trained on: 24528\n",
            "Training loss epoch: 0.27841280609814906\n",
            "Training accuracy epoch: 1414.9375\n",
            "Number of batches: 1534\n",
            "Number of Sentences Trained on: 24544\n",
            "Training loss epoch: 0.2783587967458178\n",
            "Training accuracy epoch: 1415.8125\n",
            "Number of batches: 1535\n",
            "Number of Sentences Trained on: 24560\n",
            "Training loss epoch: 0.27869224795934616\n",
            "Training accuracy epoch: 1416.4375\n",
            "Number of batches: 1536\n",
            "Number of Sentences Trained on: 24576\n",
            "Training loss epoch: 0.27858523771175286\n",
            "Training accuracy epoch: 1417.375\n",
            "Number of batches: 1537\n",
            "Number of Sentences Trained on: 24592\n",
            "Training loss epoch: 0.27842284269692674\n",
            "Training accuracy epoch: 1418.375\n",
            "Number of batches: 1538\n",
            "Number of Sentences Trained on: 24608\n",
            "Training loss epoch: 0.2782766562426624\n",
            "Training accuracy epoch: 1419.375\n",
            "Number of batches: 1539\n",
            "Number of Sentences Trained on: 24624\n",
            "Training loss epoch: 0.27818192543625203\n",
            "Training accuracy epoch: 1420.25\n",
            "Number of batches: 1540\n",
            "Number of Sentences Trained on: 24640\n",
            "Training loss epoch: 0.27812548258385966\n",
            "Training accuracy epoch: 1421.1875\n",
            "Number of batches: 1541\n",
            "Number of Sentences Trained on: 24656\n",
            "Training loss epoch: 0.2780170906513287\n",
            "Training accuracy epoch: 1422.125\n",
            "Number of batches: 1542\n",
            "Number of Sentences Trained on: 24672\n",
            "Training loss epoch: 0.27785271465414996\n",
            "Training accuracy epoch: 1423.125\n",
            "Number of batches: 1543\n",
            "Number of Sentences Trained on: 24688\n",
            "Training loss epoch: 0.2778157644257929\n",
            "Training accuracy epoch: 1424.0625\n",
            "Number of batches: 1544\n",
            "Number of Sentences Trained on: 24704\n",
            "Training loss epoch: 0.27771090502823587\n",
            "Training accuracy epoch: 1425.0\n",
            "Number of batches: 1545\n",
            "Number of Sentences Trained on: 24720\n",
            "Training loss epoch: 0.2775916604726981\n",
            "Training accuracy epoch: 1425.9375\n",
            "Number of batches: 1546\n",
            "Number of Sentences Trained on: 24736\n",
            "Training loss epoch: 0.2778107229829119\n",
            "Training accuracy epoch: 1426.75\n",
            "Number of batches: 1547\n",
            "Number of Sentences Trained on: 24752\n",
            "Training loss epoch: 0.2776635102936709\n",
            "Training accuracy epoch: 1427.75\n",
            "Number of batches: 1548\n",
            "Number of Sentences Trained on: 24768\n",
            "Training loss epoch: 0.2775196658854425\n",
            "Training accuracy epoch: 1428.75\n",
            "Number of batches: 1549\n",
            "Number of Sentences Trained on: 24784\n",
            "Training loss epoch: 0.2773993968876499\n",
            "Training accuracy epoch: 1429.75\n",
            "Number of batches: 1550\n",
            "Number of Sentences Trained on: 24800\n",
            "Training loss epoch: 0.2774470335423231\n",
            "Training accuracy epoch: 1430.625\n",
            "Number of batches: 1551\n",
            "Number of Sentences Trained on: 24816\n",
            "Training loss epoch: 0.2772932436239795\n",
            "Training accuracy epoch: 1431.625\n",
            "Number of batches: 1552\n",
            "Number of Sentences Trained on: 24832\n",
            "Training loss epoch: 0.2773425682275139\n",
            "Training accuracy epoch: 1432.5\n",
            "Number of batches: 1553\n",
            "Number of Sentences Trained on: 24848\n",
            "Training loss epoch: 0.27732814057907534\n",
            "Training accuracy epoch: 1433.4375\n",
            "Number of batches: 1554\n",
            "Number of Sentences Trained on: 24864\n",
            "Training loss epoch: 0.27722968940751874\n",
            "Training accuracy epoch: 1434.4375\n",
            "Number of batches: 1555\n",
            "Number of Sentences Trained on: 24880\n",
            "Training loss epoch: 0.27716881858997744\n",
            "Training accuracy epoch: 1435.375\n",
            "Number of batches: 1556\n",
            "Number of Sentences Trained on: 24896\n",
            "Training loss epoch: 0.27703054414944833\n",
            "Training accuracy epoch: 1436.375\n",
            "Number of batches: 1557\n",
            "Number of Sentences Trained on: 24912\n",
            "Training loss epoch: 0.27693502656698216\n",
            "Training accuracy epoch: 1437.375\n",
            "Number of batches: 1558\n",
            "Number of Sentences Trained on: 24928\n",
            "Training loss epoch: 0.27684302787751086\n",
            "Training accuracy epoch: 1438.3125\n",
            "Number of batches: 1559\n",
            "Number of Sentences Trained on: 24944\n",
            "Training loss epoch: 0.2770008309729373\n",
            "Training accuracy epoch: 1439.0625\n",
            "Number of batches: 1560\n",
            "Number of Sentences Trained on: 24960\n",
            "Training loss epoch: 0.27684743392497807\n",
            "Training accuracy epoch: 1440.0625\n",
            "Number of batches: 1561\n",
            "Number of Sentences Trained on: 24976\n",
            "Training loss epoch: 0.2769808815431479\n",
            "Training accuracy epoch: 1440.9375\n",
            "Number of batches: 1562\n",
            "Number of Sentences Trained on: 24992\n",
            "Training loss epoch: 0.276991805759743\n",
            "Training accuracy epoch: 1441.875\n",
            "Number of batches: 1563\n",
            "Number of Sentences Trained on: 25008\n",
            "Training loss epoch: 0.2769671165324209\n",
            "Training accuracy epoch: 1442.8125\n",
            "Number of batches: 1564\n",
            "Number of Sentences Trained on: 25024\n",
            "Training loss epoch: 0.2771070289473159\n",
            "Training accuracy epoch: 1443.625\n",
            "Number of batches: 1565\n",
            "Number of Sentences Trained on: 25040\n",
            "Training loss epoch: 0.27694497189881806\n",
            "Training accuracy epoch: 1444.625\n",
            "Number of batches: 1566\n",
            "Number of Sentences Trained on: 25056\n",
            "Training loss epoch: 0.2768110645731083\n",
            "Training accuracy epoch: 1445.625\n",
            "Number of batches: 1567\n",
            "Number of Sentences Trained on: 25072\n",
            "Training loss epoch: 0.27668431882564765\n",
            "Training accuracy epoch: 1446.625\n",
            "Number of batches: 1568\n",
            "Number of Sentences Trained on: 25088\n",
            "Training loss epoch: 0.2765439958365039\n",
            "Training accuracy epoch: 1447.625\n",
            "Number of batches: 1569\n",
            "Number of Sentences Trained on: 25104\n",
            "Training loss epoch: 0.2764305462689987\n",
            "Training accuracy epoch: 1448.5625\n",
            "Number of batches: 1570\n",
            "Number of Sentences Trained on: 25120\n",
            "Training loss epoch: 0.27658502792614675\n",
            "Training accuracy epoch: 1449.4375\n",
            "Number of batches: 1571\n",
            "Number of Sentences Trained on: 25136\n",
            "Training loss epoch: 0.2765401701793032\n",
            "Training accuracy epoch: 1450.3125\n",
            "Number of batches: 1572\n",
            "Number of Sentences Trained on: 25152\n",
            "Training loss epoch: 0.27638135865201624\n",
            "Training accuracy epoch: 1451.3125\n",
            "Number of batches: 1573\n",
            "Number of Sentences Trained on: 25168\n",
            "Training loss epoch: 0.27624524288735053\n",
            "Training accuracy epoch: 1452.3125\n",
            "Number of batches: 1574\n",
            "Number of Sentences Trained on: 25184\n",
            "Training loss epoch: 0.2761916492533471\n",
            "Training accuracy epoch: 1453.3125\n",
            "Number of batches: 1575\n",
            "Number of Sentences Trained on: 25200\n",
            "Training loss epoch: 0.27604020447771627\n",
            "Training accuracy epoch: 1454.3125\n",
            "Number of batches: 1576\n",
            "Number of Sentences Trained on: 25216\n",
            "Training loss epoch: 0.2759621687828148\n",
            "Training accuracy epoch: 1455.25\n",
            "Number of batches: 1577\n",
            "Number of Sentences Trained on: 25232\n",
            "Training loss epoch: 0.2758273612527845\n",
            "Training accuracy epoch: 1456.25\n",
            "Number of batches: 1578\n",
            "Number of Sentences Trained on: 25248\n",
            "Training loss epoch: 0.27570954072622417\n",
            "Training accuracy epoch: 1457.25\n",
            "Number of batches: 1579\n",
            "Number of Sentences Trained on: 25264\n",
            "Training loss epoch: 0.27566282698100647\n",
            "Training accuracy epoch: 1458.1875\n",
            "Number of batches: 1580\n",
            "Number of Sentences Trained on: 25280\n",
            "Training loss epoch: 0.27552164518312655\n",
            "Training accuracy epoch: 1459.1875\n",
            "Number of batches: 1581\n",
            "Number of Sentences Trained on: 25296\n",
            "Training loss epoch: 0.2755487495041352\n",
            "Training accuracy epoch: 1460.125\n",
            "Number of batches: 1582\n",
            "Number of Sentences Trained on: 25312\n",
            "Training loss epoch: 0.2755020886258971\n",
            "Training accuracy epoch: 1461.0625\n",
            "Number of batches: 1583\n",
            "Number of Sentences Trained on: 25328\n",
            "Training loss epoch: 0.27552841314334525\n",
            "Training accuracy epoch: 1462.0\n",
            "Number of batches: 1584\n",
            "Number of Sentences Trained on: 25344\n",
            "Training loss epoch: 0.2754808730727843\n",
            "Training accuracy epoch: 1462.9375\n",
            "Number of batches: 1585\n",
            "Number of Sentences Trained on: 25360\n",
            "Training loss epoch: 0.27539836721815414\n",
            "Training accuracy epoch: 1463.875\n",
            "Number of batches: 1586\n",
            "Number of Sentences Trained on: 25376\n",
            "Training loss epoch: 0.275371333360895\n",
            "Training accuracy epoch: 1464.75\n",
            "Number of batches: 1587\n",
            "Number of Sentences Trained on: 25392\n",
            "Training loss epoch: 0.27541783703837114\n",
            "Training accuracy epoch: 1465.6875\n",
            "Number of batches: 1588\n",
            "Number of Sentences Trained on: 25408\n",
            "Training loss epoch: 0.27560039070306513\n",
            "Training accuracy epoch: 1466.4375\n",
            "Number of batches: 1589\n",
            "Number of Sentences Trained on: 25424\n",
            "Training loss epoch: 0.27544361846433635\n",
            "Training accuracy epoch: 1467.4375\n",
            "Number of batches: 1590\n",
            "Number of Sentences Trained on: 25440\n",
            "Training loss epoch: 0.27538065499544734\n",
            "Training accuracy epoch: 1468.375\n",
            "Number of batches: 1591\n",
            "Number of Sentences Trained on: 25456\n",
            "Training loss epoch: 0.2755059117329045\n",
            "Training accuracy epoch: 1469.25\n",
            "Number of batches: 1592\n",
            "Number of Sentences Trained on: 25472\n",
            "Training loss epoch: 0.27534936525936693\n",
            "Training accuracy epoch: 1470.25\n",
            "Number of batches: 1593\n",
            "Number of Sentences Trained on: 25488\n",
            "Training loss epoch: 0.27543803755342355\n",
            "Training accuracy epoch: 1471.125\n",
            "Number of batches: 1594\n",
            "Number of Sentences Trained on: 25504\n",
            "Training loss epoch: 0.2754305989703391\n",
            "Training accuracy epoch: 1472.0625\n",
            "Number of batches: 1595\n",
            "Number of Sentences Trained on: 25520\n",
            "Training loss epoch: 0.27553470293964427\n",
            "Training accuracy epoch: 1472.9375\n",
            "Number of batches: 1596\n",
            "Number of Sentences Trained on: 25536\n",
            "Training loss epoch: 0.2755147596817107\n",
            "Training accuracy epoch: 1473.8125\n",
            "Number of batches: 1597\n",
            "Number of Sentences Trained on: 25552\n",
            "Training loss epoch: 0.2756927790378427\n",
            "Training accuracy epoch: 1474.625\n",
            "Number of batches: 1598\n",
            "Number of Sentences Trained on: 25568\n",
            "Training loss epoch: 0.27553763935980785\n",
            "Training accuracy epoch: 1475.625\n",
            "Number of batches: 1599\n",
            "Number of Sentences Trained on: 25584\n",
            "Training loss epoch: 0.2753949615565944\n",
            "Training accuracy epoch: 1476.5625\n",
            "Number of batches: 1600\n",
            "Number of Sentences Trained on: 25600\n",
            "Training loss per 100 training steps: 0.2752434161955899\n",
            "Training loss epoch: 0.2752434161955899\n",
            "Training accuracy epoch: 1477.5625\n",
            "Number of batches: 1601\n",
            "Number of Sentences Trained on: 25616\n",
            "Training loss epoch: 0.2751019455968622\n",
            "Training accuracy epoch: 1478.5625\n",
            "Number of batches: 1602\n",
            "Number of Sentences Trained on: 25632\n",
            "Training loss epoch: 0.27507059449209126\n",
            "Training accuracy epoch: 1479.5\n",
            "Number of batches: 1603\n",
            "Number of Sentences Trained on: 25648\n",
            "Training loss epoch: 0.27504943535306686\n",
            "Training accuracy epoch: 1480.4375\n",
            "Number of batches: 1604\n",
            "Number of Sentences Trained on: 25664\n",
            "Training loss epoch: 0.2749740901600419\n",
            "Training accuracy epoch: 1481.375\n",
            "Number of batches: 1605\n",
            "Number of Sentences Trained on: 25680\n",
            "Training loss epoch: 0.27485207202647155\n",
            "Training accuracy epoch: 1482.375\n",
            "Number of batches: 1606\n",
            "Number of Sentences Trained on: 25696\n",
            "Training loss epoch: 0.27470397040932937\n",
            "Training accuracy epoch: 1483.375\n",
            "Number of batches: 1607\n",
            "Number of Sentences Trained on: 25712\n",
            "Training loss epoch: 0.2748457485510486\n",
            "Training accuracy epoch: 1484.1875\n",
            "Number of batches: 1608\n",
            "Number of Sentences Trained on: 25728\n",
            "Training loss epoch: 0.27477186666789993\n",
            "Training accuracy epoch: 1485.125\n",
            "Number of batches: 1609\n",
            "Number of Sentences Trained on: 25744\n",
            "Training loss epoch: 0.2746189945924055\n",
            "Training accuracy epoch: 1486.125\n",
            "Number of batches: 1610\n",
            "Number of Sentences Trained on: 25760\n",
            "Training loss epoch: 0.27458294213695705\n",
            "Training accuracy epoch: 1487.0625\n",
            "Number of batches: 1611\n",
            "Number of Sentences Trained on: 25776\n",
            "Training loss epoch: 0.27456534824423057\n",
            "Training accuracy epoch: 1487.9375\n",
            "Number of batches: 1612\n",
            "Number of Sentences Trained on: 25792\n",
            "Training loss epoch: 0.2744407916770092\n",
            "Training accuracy epoch: 1488.9375\n",
            "Number of batches: 1613\n",
            "Number of Sentences Trained on: 25808\n",
            "Training loss epoch: 0.2742755498279514\n",
            "Training accuracy epoch: 1489.9375\n",
            "Number of batches: 1614\n",
            "Number of Sentences Trained on: 25824\n",
            "Training loss epoch: 0.2741314550057261\n",
            "Training accuracy epoch: 1490.9375\n",
            "Number of batches: 1615\n",
            "Number of Sentences Trained on: 25840\n",
            "Training loss epoch: 0.27412534406780964\n",
            "Training accuracy epoch: 1491.8125\n",
            "Number of batches: 1616\n",
            "Number of Sentences Trained on: 25856\n",
            "Training loss epoch: 0.27396575566271786\n",
            "Training accuracy epoch: 1492.8125\n",
            "Number of batches: 1617\n",
            "Number of Sentences Trained on: 25872\n",
            "Training loss epoch: 0.27382493088607834\n",
            "Training accuracy epoch: 1493.8125\n",
            "Number of batches: 1618\n",
            "Number of Sentences Trained on: 25888\n",
            "Training loss epoch: 0.2736781354118633\n",
            "Training accuracy epoch: 1494.8125\n",
            "Number of batches: 1619\n",
            "Number of Sentences Trained on: 25904\n",
            "Training loss epoch: 0.2735292312812727\n",
            "Training accuracy epoch: 1495.8125\n",
            "Number of batches: 1620\n",
            "Number of Sentences Trained on: 25920\n",
            "Training loss epoch: 0.27337771414748674\n",
            "Training accuracy epoch: 1496.8125\n",
            "Number of batches: 1621\n",
            "Number of Sentences Trained on: 25936\n",
            "Training loss epoch: 0.2732197380427767\n",
            "Training accuracy epoch: 1497.8125\n",
            "Number of batches: 1622\n",
            "Number of Sentences Trained on: 25952\n",
            "Training loss epoch: 0.27313032906269513\n",
            "Training accuracy epoch: 1498.75\n",
            "Number of batches: 1623\n",
            "Number of Sentences Trained on: 25968\n",
            "Training loss epoch: 0.27338486168679227\n",
            "Training accuracy epoch: 1499.625\n",
            "Number of batches: 1624\n",
            "Number of Sentences Trained on: 25984\n",
            "Training loss epoch: 0.27361181185996303\n",
            "Training accuracy epoch: 1500.375\n",
            "Number of batches: 1625\n",
            "Number of Sentences Trained on: 26000\n",
            "Training loss epoch: 0.27348938268817835\n",
            "Training accuracy epoch: 1501.375\n",
            "Number of batches: 1626\n",
            "Number of Sentences Trained on: 26016\n",
            "Training loss epoch: 0.27338406434179624\n",
            "Training accuracy epoch: 1502.3125\n",
            "Number of batches: 1627\n",
            "Number of Sentences Trained on: 26032\n",
            "Training loss epoch: 0.2733332829510436\n",
            "Training accuracy epoch: 1503.1875\n",
            "Number of batches: 1628\n",
            "Number of Sentences Trained on: 26048\n",
            "Training loss epoch: 0.2731832090700634\n",
            "Training accuracy epoch: 1504.1875\n",
            "Number of batches: 1629\n",
            "Number of Sentences Trained on: 26064\n",
            "Training loss epoch: 0.2731770440956086\n",
            "Training accuracy epoch: 1505.125\n",
            "Number of batches: 1630\n",
            "Number of Sentences Trained on: 26080\n",
            "Training loss epoch: 0.2730508332419398\n",
            "Training accuracy epoch: 1506.125\n",
            "Number of batches: 1631\n",
            "Number of Sentences Trained on: 26096\n",
            "Training loss epoch: 0.2728967905240293\n",
            "Training accuracy epoch: 1507.125\n",
            "Number of batches: 1632\n",
            "Number of Sentences Trained on: 26112\n",
            "Training loss epoch: 0.27276861089420323\n",
            "Training accuracy epoch: 1508.125\n",
            "Number of batches: 1633\n",
            "Number of Sentences Trained on: 26128\n",
            "Training loss epoch: 0.27270591452648246\n",
            "Training accuracy epoch: 1509.0625\n",
            "Number of batches: 1634\n",
            "Number of Sentences Trained on: 26144\n",
            "Training loss epoch: 0.2726684937450112\n",
            "Training accuracy epoch: 1509.9375\n",
            "Number of batches: 1635\n",
            "Number of Sentences Trained on: 26160\n",
            "Training loss epoch: 0.27258275234633444\n",
            "Training accuracy epoch: 1510.875\n",
            "Number of batches: 1636\n",
            "Number of Sentences Trained on: 26176\n",
            "Training loss epoch: 0.27242935030586296\n",
            "Training accuracy epoch: 1511.875\n",
            "Number of batches: 1637\n",
            "Number of Sentences Trained on: 26192\n",
            "Training loss epoch: 0.27236077745254034\n",
            "Training accuracy epoch: 1512.8125\n",
            "Number of batches: 1638\n",
            "Number of Sentences Trained on: 26208\n",
            "Training loss epoch: 0.27236736063488043\n",
            "Training accuracy epoch: 1513.6875\n",
            "Number of batches: 1639\n",
            "Number of Sentences Trained on: 26224\n",
            "Training loss epoch: 0.2723426729346989\n",
            "Training accuracy epoch: 1514.5625\n",
            "Number of batches: 1640\n",
            "Number of Sentences Trained on: 26240\n",
            "Training loss epoch: 0.2721957281935054\n",
            "Training accuracy epoch: 1515.5625\n",
            "Number of batches: 1641\n",
            "Number of Sentences Trained on: 26256\n",
            "Training loss epoch: 0.2720734487329105\n",
            "Training accuracy epoch: 1516.5625\n",
            "Number of batches: 1642\n",
            "Number of Sentences Trained on: 26272\n",
            "Training loss epoch: 0.27193908455367277\n",
            "Training accuracy epoch: 1517.5625\n",
            "Number of batches: 1643\n",
            "Number of Sentences Trained on: 26288\n",
            "Training loss epoch: 0.27183718042909294\n",
            "Training accuracy epoch: 1518.5\n",
            "Number of batches: 1644\n",
            "Number of Sentences Trained on: 26304\n",
            "Training loss epoch: 0.2716875417726992\n",
            "Training accuracy epoch: 1519.5\n",
            "Number of batches: 1645\n",
            "Number of Sentences Trained on: 26320\n",
            "Training loss epoch: 0.2715385378180536\n",
            "Training accuracy epoch: 1520.5\n",
            "Number of batches: 1646\n",
            "Number of Sentences Trained on: 26336\n",
            "Training loss epoch: 0.2713905356722049\n",
            "Training accuracy epoch: 1521.5\n",
            "Number of batches: 1647\n",
            "Number of Sentences Trained on: 26352\n",
            "Training loss epoch: 0.2712991450914946\n",
            "Training accuracy epoch: 1522.4375\n",
            "Number of batches: 1648\n",
            "Number of Sentences Trained on: 26368\n",
            "Training loss epoch: 0.2711760423930103\n",
            "Training accuracy epoch: 1523.4375\n",
            "Number of batches: 1649\n",
            "Number of Sentences Trained on: 26384\n",
            "Training loss epoch: 0.27142314825743213\n",
            "Training accuracy epoch: 1524.1875\n",
            "Number of batches: 1650\n",
            "Number of Sentences Trained on: 26400\n",
            "Training loss epoch: 0.2714006561853064\n",
            "Training accuracy epoch: 1525.125\n",
            "Number of batches: 1651\n",
            "Number of Sentences Trained on: 26416\n",
            "Training loss epoch: 0.2714740819482079\n",
            "Training accuracy epoch: 1526.0\n",
            "Number of batches: 1652\n",
            "Number of Sentences Trained on: 26432\n",
            "Training loss epoch: 0.2713503391021064\n",
            "Training accuracy epoch: 1527.0\n",
            "Number of batches: 1653\n",
            "Number of Sentences Trained on: 26448\n",
            "Training loss epoch: 0.27148243525637566\n",
            "Training accuracy epoch: 1527.875\n",
            "Number of batches: 1654\n",
            "Number of Sentences Trained on: 26464\n",
            "Training loss epoch: 0.2713382506858434\n",
            "Training accuracy epoch: 1528.875\n",
            "Number of batches: 1655\n",
            "Number of Sentences Trained on: 26480\n",
            "Training loss epoch: 0.27136634263420334\n",
            "Training accuracy epoch: 1529.75\n",
            "Number of batches: 1656\n",
            "Number of Sentences Trained on: 26496\n",
            "Training loss epoch: 0.2712469640812923\n",
            "Training accuracy epoch: 1530.6875\n",
            "Number of batches: 1657\n",
            "Number of Sentences Trained on: 26512\n",
            "Training loss epoch: 0.2711615855808384\n",
            "Training accuracy epoch: 1531.625\n",
            "Number of batches: 1658\n",
            "Number of Sentences Trained on: 26528\n",
            "Training loss epoch: 0.27128263972244926\n",
            "Training accuracy epoch: 1532.4375\n",
            "Number of batches: 1659\n",
            "Number of Sentences Trained on: 26544\n",
            "Training loss epoch: 0.27112876029365346\n",
            "Training accuracy epoch: 1533.4375\n",
            "Number of batches: 1660\n",
            "Number of Sentences Trained on: 26560\n",
            "Training loss epoch: 0.2710087850153298\n",
            "Training accuracy epoch: 1534.375\n",
            "Number of batches: 1661\n",
            "Number of Sentences Trained on: 26576\n",
            "Training loss epoch: 0.27087137859093896\n",
            "Training accuracy epoch: 1535.375\n",
            "Number of batches: 1662\n",
            "Number of Sentences Trained on: 26592\n",
            "Training loss epoch: 0.27084040538935966\n",
            "Training accuracy epoch: 1536.3125\n",
            "Number of batches: 1663\n",
            "Number of Sentences Trained on: 26608\n",
            "Training loss epoch: 0.27072128539993384\n",
            "Training accuracy epoch: 1537.25\n",
            "Number of batches: 1664\n",
            "Number of Sentences Trained on: 26624\n",
            "Training loss epoch: 0.27081254249237274\n",
            "Training accuracy epoch: 1538.1875\n",
            "Number of batches: 1665\n",
            "Number of Sentences Trained on: 26640\n",
            "Training loss epoch: 0.27074729561071176\n",
            "Training accuracy epoch: 1539.1875\n",
            "Number of batches: 1666\n",
            "Number of Sentences Trained on: 26656\n",
            "Training loss epoch: 0.270607098349022\n",
            "Training accuracy epoch: 1540.1875\n",
            "Number of batches: 1667\n",
            "Number of Sentences Trained on: 26672\n",
            "Training loss epoch: 0.27047347547168166\n",
            "Training accuracy epoch: 1541.1875\n",
            "Number of batches: 1668\n",
            "Number of Sentences Trained on: 26688\n",
            "Training loss epoch: 0.2704897796126376\n",
            "Training accuracy epoch: 1542.125\n",
            "Number of batches: 1669\n",
            "Number of Sentences Trained on: 26704\n",
            "Training loss epoch: 0.27034898263313056\n",
            "Training accuracy epoch: 1543.125\n",
            "Number of batches: 1670\n",
            "Number of Sentences Trained on: 26720\n",
            "Training loss epoch: 0.27029304666041143\n",
            "Training accuracy epoch: 1544.0625\n",
            "Number of batches: 1671\n",
            "Number of Sentences Trained on: 26736\n",
            "Training loss epoch: 0.270256989100518\n",
            "Training accuracy epoch: 1544.9375\n",
            "Number of batches: 1672\n",
            "Number of Sentences Trained on: 26752\n",
            "Training loss epoch: 0.270296267283055\n",
            "Training accuracy epoch: 1545.8125\n",
            "Number of batches: 1673\n",
            "Number of Sentences Trained on: 26768\n",
            "Training loss epoch: 0.2701403735839209\n",
            "Training accuracy epoch: 1546.8125\n",
            "Number of batches: 1674\n",
            "Number of Sentences Trained on: 26784\n",
            "Training loss epoch: 0.2699866673108468\n",
            "Training accuracy epoch: 1547.8125\n",
            "Number of batches: 1675\n",
            "Number of Sentences Trained on: 26800\n",
            "Training loss epoch: 0.2698569841161126\n",
            "Training accuracy epoch: 1548.75\n",
            "Number of batches: 1676\n",
            "Number of Sentences Trained on: 26816\n",
            "Training loss epoch: 0.26969904104135506\n",
            "Training accuracy epoch: 1549.75\n",
            "Number of batches: 1677\n",
            "Number of Sentences Trained on: 26832\n",
            "Training loss epoch: 0.26960327313642357\n",
            "Training accuracy epoch: 1550.6875\n",
            "Number of batches: 1678\n",
            "Number of Sentences Trained on: 26848\n",
            "Training loss epoch: 0.2694688111004498\n",
            "Training accuracy epoch: 1551.6875\n",
            "Number of batches: 1679\n",
            "Number of Sentences Trained on: 26864\n",
            "Training loss epoch: 0.2693671508304154\n",
            "Training accuracy epoch: 1552.6875\n",
            "Number of batches: 1680\n",
            "Number of Sentences Trained on: 26880\n",
            "Training loss epoch: 0.2693837891293469\n",
            "Training accuracy epoch: 1553.625\n",
            "Number of batches: 1681\n",
            "Number of Sentences Trained on: 26896\n",
            "Training loss epoch: 0.26927440656586044\n",
            "Training accuracy epoch: 1554.5625\n",
            "Number of batches: 1682\n",
            "Number of Sentences Trained on: 26912\n",
            "Training loss epoch: 0.26922026381618164\n",
            "Training accuracy epoch: 1555.5\n",
            "Number of batches: 1683\n",
            "Number of Sentences Trained on: 26928\n",
            "Training loss epoch: 0.26910631013193526\n",
            "Training accuracy epoch: 1556.5\n",
            "Number of batches: 1684\n",
            "Number of Sentences Trained on: 26944\n",
            "Training loss epoch: 0.26896716782861896\n",
            "Training accuracy epoch: 1557.5\n",
            "Number of batches: 1685\n",
            "Number of Sentences Trained on: 26960\n",
            "Training loss epoch: 0.26895069244648034\n",
            "Training accuracy epoch: 1558.4375\n",
            "Number of batches: 1686\n",
            "Number of Sentences Trained on: 26976\n",
            "Training loss epoch: 0.26881470654555056\n",
            "Training accuracy epoch: 1559.4375\n",
            "Number of batches: 1687\n",
            "Number of Sentences Trained on: 26992\n",
            "Training loss epoch: 0.2688424771759765\n",
            "Training accuracy epoch: 1560.375\n",
            "Number of batches: 1688\n",
            "Number of Sentences Trained on: 27008\n",
            "Training loss epoch: 0.26869152104641963\n",
            "Training accuracy epoch: 1561.375\n",
            "Number of batches: 1689\n",
            "Number of Sentences Trained on: 27024\n",
            "Training loss epoch: 0.26853691431330023\n",
            "Training accuracy epoch: 1562.375\n",
            "Number of batches: 1690\n",
            "Number of Sentences Trained on: 27040\n",
            "Training loss epoch: 0.2684652615266415\n",
            "Training accuracy epoch: 1563.3125\n",
            "Number of batches: 1691\n",
            "Number of Sentences Trained on: 27056\n",
            "Training loss epoch: 0.26850384133113836\n",
            "Training accuracy epoch: 1564.25\n",
            "Number of batches: 1692\n",
            "Number of Sentences Trained on: 27072\n",
            "Training loss epoch: 0.26841997934947626\n",
            "Training accuracy epoch: 1565.1875\n",
            "Number of batches: 1693\n",
            "Number of Sentences Trained on: 27088\n",
            "Training loss epoch: 0.26847041995702214\n",
            "Training accuracy epoch: 1566.0625\n",
            "Number of batches: 1694\n",
            "Number of Sentences Trained on: 27104\n",
            "Training loss epoch: 0.26832343656102686\n",
            "Training accuracy epoch: 1567.0625\n",
            "Number of batches: 1695\n",
            "Number of Sentences Trained on: 27120\n",
            "Training loss epoch: 0.2682010582683911\n",
            "Training accuracy epoch: 1568.0\n",
            "Number of batches: 1696\n",
            "Number of Sentences Trained on: 27136\n",
            "Training loss epoch: 0.26817622064636404\n",
            "Training accuracy epoch: 1568.9375\n",
            "Number of batches: 1697\n",
            "Number of Sentences Trained on: 27152\n",
            "Training loss epoch: 0.2682130271505819\n",
            "Training accuracy epoch: 1569.8125\n",
            "Number of batches: 1698\n",
            "Number of Sentences Trained on: 27168\n",
            "Training loss epoch: 0.2682059457261946\n",
            "Training accuracy epoch: 1570.75\n",
            "Number of batches: 1699\n",
            "Number of Sentences Trained on: 27184\n",
            "Training loss epoch: 0.2680732548579245\n",
            "Training accuracy epoch: 1571.75\n",
            "Number of batches: 1700\n",
            "Number of Sentences Trained on: 27200\n",
            "Training loss per 100 training steps: 0.2679318020321201\n",
            "Training loss epoch: 0.2679318020321201\n",
            "Training accuracy epoch: 1572.75\n",
            "Number of batches: 1701\n",
            "Number of Sentences Trained on: 27216\n",
            "Training loss epoch: 0.26795024915431986\n",
            "Training accuracy epoch: 1573.625\n",
            "Number of batches: 1702\n",
            "Number of Sentences Trained on: 27232\n",
            "Training loss epoch: 0.2680478242955472\n",
            "Training accuracy epoch: 1574.4375\n",
            "Number of batches: 1703\n",
            "Number of Sentences Trained on: 27248\n",
            "Training loss epoch: 0.26792118104066515\n",
            "Training accuracy epoch: 1575.4375\n",
            "Number of batches: 1704\n",
            "Number of Sentences Trained on: 27264\n",
            "Training loss epoch: 0.2680662868345085\n",
            "Training accuracy epoch: 1576.25\n",
            "Number of batches: 1705\n",
            "Number of Sentences Trained on: 27280\n",
            "Training loss epoch: 0.26816460265229913\n",
            "Training accuracy epoch: 1577.125\n",
            "Number of batches: 1706\n",
            "Number of Sentences Trained on: 27296\n",
            "Training loss epoch: 0.2680113016053222\n",
            "Training accuracy epoch: 1578.125\n",
            "Number of batches: 1707\n",
            "Number of Sentences Trained on: 27312\n",
            "Training loss epoch: 0.26812983201969354\n",
            "Training accuracy epoch: 1578.9375\n",
            "Number of batches: 1708\n",
            "Number of Sentences Trained on: 27328\n",
            "Training loss epoch: 0.2679798587662974\n",
            "Training accuracy epoch: 1579.9375\n",
            "Number of batches: 1709\n",
            "Number of Sentences Trained on: 27344\n",
            "Training loss epoch: 0.2679013018060628\n",
            "Training accuracy epoch: 1580.9375\n",
            "Number of batches: 1710\n",
            "Number of Sentences Trained on: 27360\n",
            "Training loss epoch: 0.26788327539821927\n",
            "Training accuracy epoch: 1581.875\n",
            "Number of batches: 1711\n",
            "Number of Sentences Trained on: 27376\n",
            "Training loss epoch: 0.2677439088932082\n",
            "Training accuracy epoch: 1582.875\n",
            "Number of batches: 1712\n",
            "Number of Sentences Trained on: 27392\n",
            "Training loss epoch: 0.2677455986521038\n",
            "Training accuracy epoch: 1583.8125\n",
            "Number of batches: 1713\n",
            "Number of Sentences Trained on: 27408\n",
            "Training loss epoch: 0.2675945160241065\n",
            "Training accuracy epoch: 1584.8125\n",
            "Number of batches: 1714\n",
            "Number of Sentences Trained on: 27424\n",
            "Training loss epoch: 0.2674858065707604\n",
            "Training accuracy epoch: 1585.8125\n",
            "Number of batches: 1715\n",
            "Number of Sentences Trained on: 27440\n",
            "Training loss epoch: 0.2675475974953657\n",
            "Training accuracy epoch: 1586.75\n",
            "Number of batches: 1716\n",
            "Number of Sentences Trained on: 27456\n",
            "Training loss epoch: 0.2674103700347032\n",
            "Training accuracy epoch: 1587.75\n",
            "Number of batches: 1717\n",
            "Number of Sentences Trained on: 27472\n",
            "Training loss epoch: 0.2673135800699121\n",
            "Training accuracy epoch: 1588.6875\n",
            "Number of batches: 1718\n",
            "Number of Sentences Trained on: 27488\n",
            "Training loss epoch: 0.2672045552239489\n",
            "Training accuracy epoch: 1589.6875\n",
            "Number of batches: 1719\n",
            "Number of Sentences Trained on: 27504\n",
            "Training loss epoch: 0.2671369720716029\n",
            "Training accuracy epoch: 1590.625\n",
            "Number of batches: 1720\n",
            "Number of Sentences Trained on: 27520\n",
            "Training loss epoch: 0.2671163925140196\n",
            "Training accuracy epoch: 1591.5625\n",
            "Number of batches: 1721\n",
            "Number of Sentences Trained on: 27536\n",
            "Training loss epoch: 0.26697084541853333\n",
            "Training accuracy epoch: 1592.5625\n",
            "Number of batches: 1722\n",
            "Number of Sentences Trained on: 27552\n",
            "Training loss epoch: 0.26686180836250284\n",
            "Training accuracy epoch: 1593.5\n",
            "Number of batches: 1723\n",
            "Number of Sentences Trained on: 27568\n",
            "Training loss epoch: 0.2667472716215609\n",
            "Training accuracy epoch: 1594.5\n",
            "Number of batches: 1724\n",
            "Number of Sentences Trained on: 27584\n",
            "Training loss epoch: 0.26665924249891787\n",
            "Training accuracy epoch: 1595.4375\n",
            "Number of batches: 1725\n",
            "Number of Sentences Trained on: 27600\n",
            "Training loss epoch: 0.26672980788154127\n",
            "Training accuracy epoch: 1596.3125\n",
            "Number of batches: 1726\n",
            "Number of Sentences Trained on: 27616\n",
            "Training loss epoch: 0.2666540005889823\n",
            "Training accuracy epoch: 1597.25\n",
            "Number of batches: 1727\n",
            "Number of Sentences Trained on: 27632\n",
            "Training loss epoch: 0.2669612548833592\n",
            "Training accuracy epoch: 1598.0625\n",
            "Number of batches: 1728\n",
            "Number of Sentences Trained on: 27648\n",
            "Training loss epoch: 0.2668172538317115\n",
            "Training accuracy epoch: 1599.0625\n",
            "Number of batches: 1729\n",
            "Number of Sentences Trained on: 27664\n",
            "Training loss epoch: 0.2667794788040953\n",
            "Training accuracy epoch: 1600.0\n",
            "Number of batches: 1730\n",
            "Number of Sentences Trained on: 27680\n",
            "Training loss epoch: 0.2667119109154109\n",
            "Training accuracy epoch: 1601.0\n",
            "Number of batches: 1731\n",
            "Number of Sentences Trained on: 27696\n",
            "Training loss epoch: 0.2666370139078734\n",
            "Training accuracy epoch: 1601.9375\n",
            "Number of batches: 1732\n",
            "Number of Sentences Trained on: 27712\n",
            "Training loss epoch: 0.2664955908059644\n",
            "Training accuracy epoch: 1602.9375\n",
            "Number of batches: 1733\n",
            "Number of Sentences Trained on: 27728\n",
            "Training loss epoch: 0.26635310754734237\n",
            "Training accuracy epoch: 1603.9375\n",
            "Number of batches: 1734\n",
            "Number of Sentences Trained on: 27744\n",
            "Training loss epoch: 0.26635423961930216\n",
            "Training accuracy epoch: 1604.875\n",
            "Number of batches: 1735\n",
            "Number of Sentences Trained on: 27760\n",
            "Training loss epoch: 0.26625865679513233\n",
            "Training accuracy epoch: 1605.875\n",
            "Number of batches: 1736\n",
            "Number of Sentences Trained on: 27776\n",
            "Training loss epoch: 0.2663015553767422\n",
            "Training accuracy epoch: 1606.75\n",
            "Number of batches: 1737\n",
            "Number of Sentences Trained on: 27792\n",
            "Training loss epoch: 0.26619682759496677\n",
            "Training accuracy epoch: 1607.75\n",
            "Number of batches: 1738\n",
            "Number of Sentences Trained on: 27808\n",
            "Training loss epoch: 0.2660999864162026\n",
            "Training accuracy epoch: 1608.6875\n",
            "Number of batches: 1739\n",
            "Number of Sentences Trained on: 27824\n",
            "Training loss epoch: 0.26598455697062545\n",
            "Training accuracy epoch: 1609.6875\n",
            "Number of batches: 1740\n",
            "Number of Sentences Trained on: 27840\n",
            "Training loss epoch: 0.26595850047612074\n",
            "Training accuracy epoch: 1610.625\n",
            "Number of batches: 1741\n",
            "Number of Sentences Trained on: 27856\n",
            "Training loss epoch: 0.26584439583648806\n",
            "Training accuracy epoch: 1611.625\n",
            "Number of batches: 1742\n",
            "Number of Sentences Trained on: 27872\n",
            "Training loss epoch: 0.26570337952440043\n",
            "Training accuracy epoch: 1612.625\n",
            "Number of batches: 1743\n",
            "Number of Sentences Trained on: 27888\n",
            "Training loss epoch: 0.2657861791598106\n",
            "Training accuracy epoch: 1613.375\n",
            "Number of batches: 1744\n",
            "Number of Sentences Trained on: 27904\n",
            "Training loss epoch: 0.26566958074967634\n",
            "Training accuracy epoch: 1614.375\n",
            "Number of batches: 1745\n",
            "Number of Sentences Trained on: 27920\n",
            "Training loss epoch: 0.2657484418786166\n",
            "Training accuracy epoch: 1615.3125\n",
            "Number of batches: 1746\n",
            "Number of Sentences Trained on: 27936\n",
            "Training loss epoch: 0.2659255955485562\n",
            "Training accuracy epoch: 1616.1875\n",
            "Number of batches: 1747\n",
            "Number of Sentences Trained on: 27952\n",
            "Training loss epoch: 0.26578836471430567\n",
            "Training accuracy epoch: 1617.1875\n",
            "Number of batches: 1748\n",
            "Number of Sentences Trained on: 27968\n",
            "Training loss epoch: 0.26575091398779566\n",
            "Training accuracy epoch: 1618.125\n",
            "Number of batches: 1749\n",
            "Number of Sentences Trained on: 27984\n",
            "Training loss epoch: 0.265758211810674\n",
            "Training accuracy epoch: 1619.0625\n",
            "Number of batches: 1750\n",
            "Number of Sentences Trained on: 28000\n",
            "Training loss epoch: 0.2659348477782317\n",
            "Training accuracy epoch: 1619.9375\n",
            "Number of batches: 1751\n",
            "Number of Sentences Trained on: 28016\n",
            "Training loss epoch: 0.2658921202929631\n",
            "Training accuracy epoch: 1620.875\n",
            "Number of batches: 1752\n",
            "Number of Sentences Trained on: 28032\n",
            "Training loss epoch: 0.26574602025718824\n",
            "Training accuracy epoch: 1621.875\n",
            "Number of batches: 1753\n",
            "Number of Sentences Trained on: 28048\n",
            "Training loss epoch: 0.265597996148764\n",
            "Training accuracy epoch: 1622.875\n",
            "Number of batches: 1754\n",
            "Number of Sentences Trained on: 28064\n",
            "Training loss epoch: 0.26545599645184104\n",
            "Training accuracy epoch: 1623.875\n",
            "Number of batches: 1755\n",
            "Number of Sentences Trained on: 28080\n",
            "Training loss epoch: 0.26545136498795413\n",
            "Training accuracy epoch: 1624.8125\n",
            "Number of batches: 1756\n",
            "Number of Sentences Trained on: 28096\n",
            "Training loss epoch: 0.26552772042450196\n",
            "Training accuracy epoch: 1625.75\n",
            "Number of batches: 1757\n",
            "Number of Sentences Trained on: 28112\n",
            "Training loss epoch: 0.2654386030579467\n",
            "Training accuracy epoch: 1626.75\n",
            "Number of batches: 1758\n",
            "Number of Sentences Trained on: 28128\n",
            "Training loss epoch: 0.2654436228676652\n",
            "Training accuracy epoch: 1627.6875\n",
            "Number of batches: 1759\n",
            "Number of Sentences Trained on: 28144\n",
            "Training loss epoch: 0.2653964958881261\n",
            "Training accuracy epoch: 1628.625\n",
            "Number of batches: 1760\n",
            "Number of Sentences Trained on: 28160\n",
            "Training loss epoch: 0.26541249542526185\n",
            "Training accuracy epoch: 1629.4375\n",
            "Number of batches: 1761\n",
            "Number of Sentences Trained on: 28176\n",
            "Training loss epoch: 0.26531732158136007\n",
            "Training accuracy epoch: 1630.4375\n",
            "Number of batches: 1762\n",
            "Number of Sentences Trained on: 28192\n",
            "Training loss epoch: 0.2653585301632388\n",
            "Training accuracy epoch: 1631.25\n",
            "Number of batches: 1763\n",
            "Number of Sentences Trained on: 28208\n",
            "Training loss epoch: 0.26527190105033965\n",
            "Training accuracy epoch: 1632.25\n",
            "Number of batches: 1764\n",
            "Number of Sentences Trained on: 28224\n",
            "Training loss epoch: 0.265225956726967\n",
            "Training accuracy epoch: 1633.1875\n",
            "Number of batches: 1765\n",
            "Number of Sentences Trained on: 28240\n",
            "Training loss epoch: 0.2651109273088221\n",
            "Training accuracy epoch: 1634.1875\n",
            "Number of batches: 1766\n",
            "Number of Sentences Trained on: 28256\n",
            "Training loss epoch: 0.26499560485126467\n",
            "Training accuracy epoch: 1635.125\n",
            "Number of batches: 1767\n",
            "Number of Sentences Trained on: 28272\n",
            "Training loss epoch: 0.2649052972024058\n",
            "Training accuracy epoch: 1636.125\n",
            "Number of batches: 1768\n",
            "Number of Sentences Trained on: 28288\n",
            "Training loss epoch: 0.2647738321473714\n",
            "Training accuracy epoch: 1637.125\n",
            "Number of batches: 1769\n",
            "Number of Sentences Trained on: 28304\n",
            "Training loss epoch: 0.2648379609438638\n",
            "Training accuracy epoch: 1637.9375\n",
            "Number of batches: 1770\n",
            "Number of Sentences Trained on: 28320\n",
            "Training loss epoch: 0.264877241576813\n",
            "Training accuracy epoch: 1638.75\n",
            "Number of batches: 1771\n",
            "Number of Sentences Trained on: 28336\n",
            "Training loss epoch: 0.2647397291389847\n",
            "Training accuracy epoch: 1639.75\n",
            "Number of batches: 1772\n",
            "Number of Sentences Trained on: 28352\n",
            "Training loss epoch: 0.26468188250784064\n",
            "Training accuracy epoch: 1640.625\n",
            "Number of batches: 1773\n",
            "Number of Sentences Trained on: 28368\n",
            "Training loss epoch: 0.2645540549569441\n",
            "Training accuracy epoch: 1641.625\n",
            "Number of batches: 1774\n",
            "Number of Sentences Trained on: 28384\n",
            "Training loss epoch: 0.2645447125828917\n",
            "Training accuracy epoch: 1642.5\n",
            "Number of batches: 1775\n",
            "Number of Sentences Trained on: 28400\n",
            "Training loss epoch: 0.2645187721268362\n",
            "Training accuracy epoch: 1643.4375\n",
            "Number of batches: 1776\n",
            "Number of Sentences Trained on: 28416\n",
            "Training loss epoch: 0.2643846645508408\n",
            "Training accuracy epoch: 1644.4375\n",
            "Number of batches: 1777\n",
            "Number of Sentences Trained on: 28432\n",
            "Training loss epoch: 0.26433452786338235\n",
            "Training accuracy epoch: 1645.375\n",
            "Number of batches: 1778\n",
            "Number of Sentences Trained on: 28448\n",
            "Training loss epoch: 0.2643297702980245\n",
            "Training accuracy epoch: 1646.25\n",
            "Number of batches: 1779\n",
            "Number of Sentences Trained on: 28464\n",
            "Training loss epoch: 0.2642748428631595\n",
            "Training accuracy epoch: 1647.1875\n",
            "Number of batches: 1780\n",
            "Number of Sentences Trained on: 28480\n",
            "Training loss epoch: 0.2643271559749503\n",
            "Training accuracy epoch: 1648.0625\n",
            "Number of batches: 1781\n",
            "Number of Sentences Trained on: 28496\n",
            "Training loss epoch: 0.26421892653192225\n",
            "Training accuracy epoch: 1649.0625\n",
            "Number of batches: 1782\n",
            "Number of Sentences Trained on: 28512\n",
            "Training loss epoch: 0.26412594173990495\n",
            "Training accuracy epoch: 1650.0\n",
            "Number of batches: 1783\n",
            "Number of Sentences Trained on: 28528\n",
            "Training loss epoch: 0.2640378788641378\n",
            "Training accuracy epoch: 1651.0\n",
            "Number of batches: 1784\n",
            "Number of Sentences Trained on: 28544\n",
            "Training loss epoch: 0.26404076194161086\n",
            "Training accuracy epoch: 1651.875\n",
            "Number of batches: 1785\n",
            "Number of Sentences Trained on: 28560\n",
            "Training loss epoch: 0.26395189786910767\n",
            "Training accuracy epoch: 1652.875\n",
            "Number of batches: 1786\n",
            "Number of Sentences Trained on: 28576\n",
            "Training loss epoch: 0.2639219380128726\n",
            "Training accuracy epoch: 1653.75\n",
            "Number of batches: 1787\n",
            "Number of Sentences Trained on: 28592\n",
            "Training loss epoch: 0.2637966252148247\n",
            "Training accuracy epoch: 1654.75\n",
            "Number of batches: 1788\n",
            "Number of Sentences Trained on: 28608\n",
            "Training loss epoch: 0.26365930461150355\n",
            "Training accuracy epoch: 1655.75\n",
            "Number of batches: 1789\n",
            "Number of Sentences Trained on: 28624\n",
            "Training loss epoch: 0.26353655943111126\n",
            "Training accuracy epoch: 1656.75\n",
            "Number of batches: 1790\n",
            "Number of Sentences Trained on: 28640\n",
            "Training loss epoch: 0.26343028496827103\n",
            "Training accuracy epoch: 1657.75\n",
            "Number of batches: 1791\n",
            "Number of Sentences Trained on: 28656\n",
            "Training loss epoch: 0.2634704864454501\n",
            "Training accuracy epoch: 1658.625\n",
            "Number of batches: 1792\n",
            "Number of Sentences Trained on: 28672\n",
            "Training loss epoch: 0.26335586599223376\n",
            "Training accuracy epoch: 1659.625\n",
            "Number of batches: 1793\n",
            "Number of Sentences Trained on: 28688\n",
            "Training loss epoch: 0.26321447774827056\n",
            "Training accuracy epoch: 1660.625\n",
            "Number of batches: 1794\n",
            "Number of Sentences Trained on: 28704\n",
            "Training loss epoch: 0.2632297192321452\n",
            "Training accuracy epoch: 1661.5625\n",
            "Number of batches: 1795\n",
            "Number of Sentences Trained on: 28720\n",
            "Training loss epoch: 0.26308783970618693\n",
            "Training accuracy epoch: 1662.5625\n",
            "Number of batches: 1796\n",
            "Number of Sentences Trained on: 28736\n",
            "Training loss epoch: 0.26295572564572833\n",
            "Training accuracy epoch: 1663.5625\n",
            "Number of batches: 1797\n",
            "Number of Sentences Trained on: 28752\n",
            "Training loss epoch: 0.2630961050263381\n",
            "Training accuracy epoch: 1664.375\n",
            "Number of batches: 1798\n",
            "Number of Sentences Trained on: 28768\n",
            "Training loss epoch: 0.26300570684427677\n",
            "Training accuracy epoch: 1665.3125\n",
            "Number of batches: 1799\n",
            "Number of Sentences Trained on: 28784\n",
            "Training loss epoch: 0.26287287986914937\n",
            "Training accuracy epoch: 1666.3125\n",
            "Number of batches: 1800\n",
            "Number of Sentences Trained on: 28800\n",
            "Training loss per 100 training steps: 0.26281162909413586\n",
            "Training loss epoch: 0.26281162909413586\n",
            "Training accuracy epoch: 1667.25\n",
            "Number of batches: 1801\n",
            "Number of Sentences Trained on: 28816\n",
            "Training loss epoch: 0.26276523779460603\n",
            "Training accuracy epoch: 1668.1875\n",
            "Number of batches: 1802\n",
            "Number of Sentences Trained on: 28832\n",
            "Training loss epoch: 0.26269989418117945\n",
            "Training accuracy epoch: 1669.125\n",
            "Number of batches: 1803\n",
            "Number of Sentences Trained on: 28848\n",
            "Training loss epoch: 0.2625839058162661\n",
            "Training accuracy epoch: 1670.125\n",
            "Number of batches: 1804\n",
            "Number of Sentences Trained on: 28864\n",
            "Training loss epoch: 0.2625217725837557\n",
            "Training accuracy epoch: 1671.0625\n",
            "Number of batches: 1805\n",
            "Number of Sentences Trained on: 28880\n",
            "Training loss epoch: 0.2625236469127572\n",
            "Training accuracy epoch: 1671.9375\n",
            "Number of batches: 1806\n",
            "Number of Sentences Trained on: 28896\n",
            "Training loss epoch: 0.26269989989478376\n",
            "Training accuracy epoch: 1672.8125\n",
            "Number of batches: 1807\n",
            "Number of Sentences Trained on: 28912\n",
            "Training loss epoch: 0.26264676347604654\n",
            "Training accuracy epoch: 1673.6875\n",
            "Number of batches: 1808\n",
            "Number of Sentences Trained on: 28928\n",
            "Training loss epoch: 0.26275221555946304\n",
            "Training accuracy epoch: 1674.5625\n",
            "Number of batches: 1809\n",
            "Number of Sentences Trained on: 28944\n",
            "Training loss epoch: 0.26267065258660655\n",
            "Training accuracy epoch: 1675.5\n",
            "Number of batches: 1810\n",
            "Number of Sentences Trained on: 28960\n",
            "Training loss epoch: 0.26264029243664766\n",
            "Training accuracy epoch: 1676.375\n",
            "Number of batches: 1811\n",
            "Number of Sentences Trained on: 28976\n",
            "Training loss epoch: 0.2625213005879655\n",
            "Training accuracy epoch: 1677.375\n",
            "Number of batches: 1812\n",
            "Number of Sentences Trained on: 28992\n",
            "Training loss epoch: 0.262400884121416\n",
            "Training accuracy epoch: 1678.375\n",
            "Number of batches: 1813\n",
            "Number of Sentences Trained on: 29008\n",
            "Training loss epoch: 0.2623295132499476\n",
            "Training accuracy epoch: 1679.3125\n",
            "Number of batches: 1814\n",
            "Number of Sentences Trained on: 29024\n",
            "Training loss epoch: 0.262204080737727\n",
            "Training accuracy epoch: 1680.3125\n",
            "Number of batches: 1815\n",
            "Number of Sentences Trained on: 29040\n",
            "Training loss epoch: 0.26210309561759454\n",
            "Training accuracy epoch: 1681.3125\n",
            "Number of batches: 1816\n",
            "Number of Sentences Trained on: 29056\n",
            "Training loss epoch: 0.26226824692572964\n",
            "Training accuracy epoch: 1682.1875\n",
            "Number of batches: 1817\n",
            "Number of Sentences Trained on: 29072\n",
            "Training loss epoch: 0.2621370040224015\n",
            "Training accuracy epoch: 1683.1875\n",
            "Number of batches: 1818\n",
            "Number of Sentences Trained on: 29088\n",
            "Training loss epoch: 0.2620978322483747\n",
            "Training accuracy epoch: 1684.125\n",
            "Number of batches: 1819\n",
            "Number of Sentences Trained on: 29104\n",
            "Training loss epoch: 0.26196263542393694\n",
            "Training accuracy epoch: 1685.125\n",
            "Number of batches: 1820\n",
            "Number of Sentences Trained on: 29120\n",
            "Training loss epoch: 0.26183685406665064\n",
            "Training accuracy epoch: 1686.125\n",
            "Number of batches: 1821\n",
            "Number of Sentences Trained on: 29136\n",
            "Training loss epoch: 0.2618107346960463\n",
            "Training accuracy epoch: 1687.0625\n",
            "Number of batches: 1822\n",
            "Number of Sentences Trained on: 29152\n",
            "Training loss epoch: 0.26190452893952737\n",
            "Training accuracy epoch: 1687.9375\n",
            "Number of batches: 1823\n",
            "Number of Sentences Trained on: 29168\n",
            "Training loss epoch: 0.26191048735008254\n",
            "Training accuracy epoch: 1688.875\n",
            "Number of batches: 1824\n",
            "Number of Sentences Trained on: 29184\n",
            "Training loss epoch: 0.26181065069868753\n",
            "Training accuracy epoch: 1689.875\n",
            "Number of batches: 1825\n",
            "Number of Sentences Trained on: 29200\n",
            "Training loss epoch: 0.26182714609356816\n",
            "Training accuracy epoch: 1690.8125\n",
            "Number of batches: 1826\n",
            "Number of Sentences Trained on: 29216\n",
            "Training loss epoch: 0.2617640098626713\n",
            "Training accuracy epoch: 1691.8125\n",
            "Number of batches: 1827\n",
            "Number of Sentences Trained on: 29232\n",
            "Training loss epoch: 0.26168893877578864\n",
            "Training accuracy epoch: 1692.75\n",
            "Number of batches: 1828\n",
            "Number of Sentences Trained on: 29248\n",
            "Training loss epoch: 0.26172966592725694\n",
            "Training accuracy epoch: 1693.625\n",
            "Number of batches: 1829\n",
            "Number of Sentences Trained on: 29264\n",
            "Training loss epoch: 0.26158949345798116\n",
            "Training accuracy epoch: 1694.625\n",
            "Number of batches: 1830\n",
            "Number of Sentences Trained on: 29280\n",
            "Training loss epoch: 0.2614828664758601\n",
            "Training accuracy epoch: 1695.625\n",
            "Number of batches: 1831\n",
            "Number of Sentences Trained on: 29296\n",
            "Training loss epoch: 0.2615955285748312\n",
            "Training accuracy epoch: 1696.5625\n",
            "Number of batches: 1832\n",
            "Number of Sentences Trained on: 29312\n",
            "Training loss epoch: 0.26157897119184237\n",
            "Training accuracy epoch: 1697.5\n",
            "Number of batches: 1833\n",
            "Number of Sentences Trained on: 29328\n",
            "Training loss epoch: 0.2616427293800898\n",
            "Training accuracy epoch: 1698.375\n",
            "Number of batches: 1834\n",
            "Number of Sentences Trained on: 29344\n",
            "Training loss epoch: 0.2615879563954773\n",
            "Training accuracy epoch: 1699.375\n",
            "Number of batches: 1835\n",
            "Number of Sentences Trained on: 29360\n",
            "Training loss epoch: 0.261693072801533\n",
            "Training accuracy epoch: 1700.25\n",
            "Number of batches: 1836\n",
            "Number of Sentences Trained on: 29376\n",
            "Training loss epoch: 0.2615699463371323\n",
            "Training accuracy epoch: 1701.25\n",
            "Number of batches: 1837\n",
            "Number of Sentences Trained on: 29392\n",
            "Training loss epoch: 0.26149074646712195\n",
            "Training accuracy epoch: 1702.125\n",
            "Number of batches: 1838\n",
            "Number of Sentences Trained on: 29408\n",
            "Training loss epoch: 0.2613701085079423\n",
            "Training accuracy epoch: 1703.125\n",
            "Number of batches: 1839\n",
            "Number of Sentences Trained on: 29424\n",
            "Training loss epoch: 0.2612509138740437\n",
            "Training accuracy epoch: 1704.125\n",
            "Number of batches: 1840\n",
            "Number of Sentences Trained on: 29440\n",
            "Training loss epoch: 0.26122388077139985\n",
            "Training accuracy epoch: 1705.0625\n",
            "Number of batches: 1841\n",
            "Number of Sentences Trained on: 29456\n",
            "Training loss epoch: 0.26111003062227717\n",
            "Training accuracy epoch: 1706.0625\n",
            "Number of batches: 1842\n",
            "Number of Sentences Trained on: 29472\n",
            "Training loss epoch: 0.2610225783063708\n",
            "Training accuracy epoch: 1707.0\n",
            "Number of batches: 1843\n",
            "Number of Sentences Trained on: 29488\n",
            "Training loss epoch: 0.260902602797265\n",
            "Training accuracy epoch: 1708.0\n",
            "Number of batches: 1844\n",
            "Number of Sentences Trained on: 29504\n",
            "Training loss epoch: 0.26079508817090896\n",
            "Training accuracy epoch: 1709.0\n",
            "Number of batches: 1845\n",
            "Number of Sentences Trained on: 29520\n",
            "Training loss epoch: 0.260798447889519\n",
            "Training accuracy epoch: 1709.875\n",
            "Number of batches: 1846\n",
            "Number of Sentences Trained on: 29536\n",
            "Training loss epoch: 0.2606921934970954\n",
            "Training accuracy epoch: 1710.875\n",
            "Number of batches: 1847\n",
            "Number of Sentences Trained on: 29552\n",
            "Training loss epoch: 0.2605694174036803\n",
            "Training accuracy epoch: 1711.875\n",
            "Number of batches: 1848\n",
            "Number of Sentences Trained on: 29568\n",
            "Training loss epoch: 0.26052009262922265\n",
            "Training accuracy epoch: 1712.8125\n",
            "Number of batches: 1849\n",
            "Number of Sentences Trained on: 29584\n",
            "Training loss epoch: 0.26043364123919527\n",
            "Training accuracy epoch: 1713.8125\n",
            "Number of batches: 1850\n",
            "Number of Sentences Trained on: 29600\n",
            "Training loss epoch: 0.2604387337346936\n",
            "Training accuracy epoch: 1714.75\n",
            "Number of batches: 1851\n",
            "Number of Sentences Trained on: 29616\n",
            "Training loss epoch: 0.2604082024095029\n",
            "Training accuracy epoch: 1715.6875\n",
            "Number of batches: 1852\n",
            "Number of Sentences Trained on: 29632\n",
            "Training loss epoch: 0.2602988003767701\n",
            "Training accuracy epoch: 1716.6875\n",
            "Number of batches: 1853\n",
            "Number of Sentences Trained on: 29648\n",
            "Training loss epoch: 0.26023789714648116\n",
            "Training accuracy epoch: 1717.625\n",
            "Number of batches: 1854\n",
            "Number of Sentences Trained on: 29664\n",
            "Training loss epoch: 0.2601217842507716\n",
            "Training accuracy epoch: 1718.625\n",
            "Number of batches: 1855\n",
            "Number of Sentences Trained on: 29680\n",
            "Training loss epoch: 0.26040234850470684\n",
            "Training accuracy epoch: 1719.375\n",
            "Number of batches: 1856\n",
            "Number of Sentences Trained on: 29696\n",
            "Training loss epoch: 0.260499090519992\n",
            "Training accuracy epoch: 1720.1875\n",
            "Number of batches: 1857\n",
            "Number of Sentences Trained on: 29712\n",
            "Training loss epoch: 0.26054751900484857\n",
            "Training accuracy epoch: 1721.125\n",
            "Number of batches: 1858\n",
            "Number of Sentences Trained on: 29728\n",
            "Training loss epoch: 0.26065494956357493\n",
            "Training accuracy epoch: 1721.9375\n",
            "Number of batches: 1859\n",
            "Number of Sentences Trained on: 29744\n",
            "Training loss epoch: 0.2605187891712112\n",
            "Training accuracy epoch: 1722.9375\n",
            "Number of batches: 1860\n",
            "Number of Sentences Trained on: 29760\n",
            "Training loss epoch: 0.2605264465905594\n",
            "Training accuracy epoch: 1723.8125\n",
            "Number of batches: 1861\n",
            "Number of Sentences Trained on: 29776\n",
            "Training loss epoch: 0.2604776827656493\n",
            "Training accuracy epoch: 1724.6875\n",
            "Number of batches: 1862\n",
            "Number of Sentences Trained on: 29792\n",
            "Training loss epoch: 0.2605583793811112\n",
            "Training accuracy epoch: 1725.5625\n",
            "Number of batches: 1863\n",
            "Number of Sentences Trained on: 29808\n",
            "Training loss epoch: 0.26060089748752474\n",
            "Training accuracy epoch: 1726.5\n",
            "Number of batches: 1864\n",
            "Number of Sentences Trained on: 29824\n",
            "Training loss epoch: 0.2606081025849718\n",
            "Training accuracy epoch: 1727.375\n",
            "Number of batches: 1865\n",
            "Number of Sentences Trained on: 29840\n",
            "Training loss epoch: 0.26056705675579983\n",
            "Training accuracy epoch: 1728.3125\n",
            "Number of batches: 1866\n",
            "Number of Sentences Trained on: 29856\n",
            "Training loss epoch: 0.2604426196055516\n",
            "Training accuracy epoch: 1729.3125\n",
            "Number of batches: 1867\n",
            "Number of Sentences Trained on: 29872\n",
            "Training loss epoch: 0.26042090712725413\n",
            "Training accuracy epoch: 1730.25\n",
            "Number of batches: 1868\n",
            "Number of Sentences Trained on: 29888\n",
            "Training loss epoch: 0.2604874912938407\n",
            "Training accuracy epoch: 1731.1875\n",
            "Number of batches: 1869\n",
            "Number of Sentences Trained on: 29904\n",
            "Training loss epoch: 0.26038362233037937\n",
            "Training accuracy epoch: 1732.1875\n",
            "Number of batches: 1870\n",
            "Number of Sentences Trained on: 29920\n",
            "Training loss epoch: 0.26026463398101624\n",
            "Training accuracy epoch: 1733.1875\n",
            "Number of batches: 1871\n",
            "Number of Sentences Trained on: 29936\n",
            "Training loss epoch: 0.2601723243645591\n",
            "Training accuracy epoch: 1734.125\n",
            "Number of batches: 1872\n",
            "Number of Sentences Trained on: 29952\n",
            "Training loss epoch: 0.26007564609111533\n",
            "Training accuracy epoch: 1735.125\n",
            "Number of batches: 1873\n",
            "Number of Sentences Trained on: 29968\n",
            "Training loss epoch: 0.2600567560770182\n",
            "Training accuracy epoch: 1736.0625\n",
            "Number of batches: 1874\n",
            "Number of Sentences Trained on: 29984\n",
            "Training loss epoch: 0.26002475073138875\n",
            "Training accuracy epoch: 1737.0\n",
            "Number of batches: 1875\n",
            "Number of Sentences Trained on: 30000\n",
            "Training loss epoch: 0.2600176283128576\n",
            "Training accuracy epoch: 1737.8125\n",
            "Number of batches: 1876\n",
            "Number of Sentences Trained on: 30016\n",
            "Training loss epoch: 0.25998246824377763\n",
            "Training accuracy epoch: 1738.75\n",
            "Number of batches: 1877\n",
            "Number of Sentences Trained on: 30032\n",
            "Training loss epoch: 0.25997730092015575\n",
            "Training accuracy epoch: 1739.6875\n",
            "Number of batches: 1878\n",
            "Number of Sentences Trained on: 30048\n",
            "Training loss epoch: 0.2599473337862421\n",
            "Training accuracy epoch: 1740.625\n",
            "Number of batches: 1879\n",
            "Number of Sentences Trained on: 30064\n",
            "Training loss epoch: 0.2599181754138083\n",
            "Training accuracy epoch: 1741.625\n",
            "Number of batches: 1880\n",
            "Number of Sentences Trained on: 30080\n",
            "Training loss epoch: 0.2598012557598836\n",
            "Training accuracy epoch: 1742.625\n",
            "Number of batches: 1881\n",
            "Number of Sentences Trained on: 30096\n",
            "Training loss epoch: 0.2597469879308365\n",
            "Training accuracy epoch: 1743.5625\n",
            "Number of batches: 1882\n",
            "Number of Sentences Trained on: 30112\n",
            "Training loss epoch: 0.25969094100526463\n",
            "Training accuracy epoch: 1744.5\n",
            "Number of batches: 1883\n",
            "Number of Sentences Trained on: 30128\n",
            "Training loss epoch: 0.25977751387247616\n",
            "Training accuracy epoch: 1745.375\n",
            "Number of batches: 1884\n",
            "Number of Sentences Trained on: 30144\n",
            "Training loss epoch: 0.25987883060775324\n",
            "Training accuracy epoch: 1746.25\n",
            "Number of batches: 1885\n",
            "Number of Sentences Trained on: 30160\n",
            "Training loss epoch: 0.25978022246904997\n",
            "Training accuracy epoch: 1747.25\n",
            "Number of batches: 1886\n",
            "Number of Sentences Trained on: 30176\n",
            "Training loss epoch: 0.25967954354243233\n",
            "Training accuracy epoch: 1748.25\n",
            "Number of batches: 1887\n",
            "Number of Sentences Trained on: 30192\n",
            "Training loss epoch: 0.2597171007576635\n",
            "Training accuracy epoch: 1749.1875\n",
            "Number of batches: 1888\n",
            "Number of Sentences Trained on: 30208\n",
            "Training loss epoch: 0.2597946922510247\n",
            "Training accuracy epoch: 1750.0625\n",
            "Number of batches: 1889\n",
            "Number of Sentences Trained on: 30224\n",
            "Training loss epoch: 0.25966287082179434\n",
            "Training accuracy epoch: 1751.0625\n",
            "Number of batches: 1890\n",
            "Number of Sentences Trained on: 30240\n",
            "Training loss epoch: 0.2598136764966318\n",
            "Training accuracy epoch: 1751.875\n",
            "Number of batches: 1891\n",
            "Number of Sentences Trained on: 30256\n",
            "Training loss epoch: 0.25974830766464435\n",
            "Training accuracy epoch: 1752.8125\n",
            "Number of batches: 1892\n",
            "Number of Sentences Trained on: 30272\n",
            "Training loss epoch: 0.2596201690394044\n",
            "Training accuracy epoch: 1753.8125\n",
            "Number of batches: 1893\n",
            "Number of Sentences Trained on: 30288\n",
            "Training loss epoch: 0.2596559541180942\n",
            "Training accuracy epoch: 1754.6875\n",
            "Number of batches: 1894\n",
            "Number of Sentences Trained on: 30304\n",
            "Training loss epoch: 0.2595904367624495\n",
            "Training accuracy epoch: 1755.625\n",
            "Number of batches: 1895\n",
            "Number of Sentences Trained on: 30320\n",
            "Training loss epoch: 0.25947375731387484\n",
            "Training accuracy epoch: 1756.625\n",
            "Number of batches: 1896\n",
            "Number of Sentences Trained on: 30336\n",
            "Training loss epoch: 0.25949889221010186\n",
            "Training accuracy epoch: 1757.5\n",
            "Number of batches: 1897\n",
            "Number of Sentences Trained on: 30352\n",
            "Training loss epoch: 0.25950758677963226\n",
            "Training accuracy epoch: 1758.375\n",
            "Number of batches: 1898\n",
            "Number of Sentences Trained on: 30368\n",
            "Training loss epoch: 0.2596468434484186\n",
            "Training accuracy epoch: 1759.1875\n",
            "Number of batches: 1899\n",
            "Number of Sentences Trained on: 30384\n",
            "Training loss epoch: 0.2595261920630736\n",
            "Training accuracy epoch: 1760.1875\n",
            "Number of batches: 1900\n",
            "Number of Sentences Trained on: 30400\n",
            "Training loss per 100 training steps: 0.25941564475207235\n",
            "Training loss epoch: 0.25941564475207235\n",
            "Training accuracy epoch: 1761.1875\n",
            "Number of batches: 1901\n",
            "Number of Sentences Trained on: 30416\n",
            "Training loss epoch: 0.25944526900305215\n",
            "Training accuracy epoch: 1762.0625\n",
            "Number of batches: 1902\n",
            "Number of Sentences Trained on: 30432\n",
            "Training loss epoch: 0.2596027701794592\n",
            "Training accuracy epoch: 1762.8125\n",
            "Number of batches: 1903\n",
            "Number of Sentences Trained on: 30448\n",
            "Training loss epoch: 0.2595853414949022\n",
            "Training accuracy epoch: 1763.75\n",
            "Number of batches: 1904\n",
            "Number of Sentences Trained on: 30464\n",
            "Training loss epoch: 0.2595761645042638\n",
            "Training accuracy epoch: 1764.6875\n",
            "Number of batches: 1905\n",
            "Number of Sentences Trained on: 30480\n",
            "Training loss epoch: 0.25948489235483446\n",
            "Training accuracy epoch: 1765.6875\n",
            "Number of batches: 1906\n",
            "Number of Sentences Trained on: 30496\n",
            "Training loss epoch: 0.2594128602795307\n",
            "Training accuracy epoch: 1766.6875\n",
            "Number of batches: 1907\n",
            "Number of Sentences Trained on: 30512\n",
            "Training loss epoch: 0.25928212405549483\n",
            "Training accuracy epoch: 1767.6875\n",
            "Number of batches: 1908\n",
            "Number of Sentences Trained on: 30528\n",
            "Training loss epoch: 0.2591998211931166\n",
            "Training accuracy epoch: 1768.625\n",
            "Number of batches: 1909\n",
            "Number of Sentences Trained on: 30544\n",
            "Training loss epoch: 0.25925897824485344\n",
            "Training accuracy epoch: 1769.4375\n",
            "Number of batches: 1910\n",
            "Number of Sentences Trained on: 30560\n",
            "Training loss epoch: 0.2592157842893561\n",
            "Training accuracy epoch: 1770.4375\n",
            "Number of batches: 1911\n",
            "Number of Sentences Trained on: 30576\n",
            "Training loss epoch: 0.2590881664667349\n",
            "Training accuracy epoch: 1771.4375\n",
            "Number of batches: 1912\n",
            "Number of Sentences Trained on: 30592\n",
            "Training loss epoch: 0.25903241089761836\n",
            "Training accuracy epoch: 1772.3125\n",
            "Number of batches: 1913\n",
            "Number of Sentences Trained on: 30608\n",
            "Training loss epoch: 0.25900226060248516\n",
            "Training accuracy epoch: 1773.25\n",
            "Number of batches: 1914\n",
            "Number of Sentences Trained on: 30624\n",
            "Training loss epoch: 0.2588923419127849\n",
            "Training accuracy epoch: 1774.25\n",
            "Number of batches: 1915\n",
            "Number of Sentences Trained on: 30640\n",
            "Training loss epoch: 0.2588541176814648\n",
            "Training accuracy epoch: 1775.125\n",
            "Number of batches: 1916\n",
            "Number of Sentences Trained on: 30656\n",
            "Training loss epoch: 0.25876187362664915\n",
            "Training accuracy epoch: 1776.125\n",
            "Number of batches: 1917\n",
            "Number of Sentences Trained on: 30672\n",
            "Training loss epoch: 0.25865690858172286\n",
            "Training accuracy epoch: 1777.125\n",
            "Number of batches: 1918\n",
            "Number of Sentences Trained on: 30688\n",
            "Training loss epoch: 0.258619635541588\n",
            "Training accuracy epoch: 1778.0625\n",
            "Number of batches: 1919\n",
            "Number of Sentences Trained on: 30704\n",
            "Training loss epoch: 0.2584960838542126\n",
            "Training accuracy epoch: 1779.0625\n",
            "Number of batches: 1920\n",
            "Number of Sentences Trained on: 30720\n",
            "Training loss epoch: 0.2583769571335714\n",
            "Training accuracy epoch: 1780.0625\n",
            "Number of batches: 1921\n",
            "Number of Sentences Trained on: 30736\n",
            "Training loss epoch: 0.25844296437720155\n",
            "Training accuracy epoch: 1780.9375\n",
            "Number of batches: 1922\n",
            "Number of Sentences Trained on: 30752\n",
            "Training loss epoch: 0.2584620707983648\n",
            "Training accuracy epoch: 1781.8125\n",
            "Number of batches: 1923\n",
            "Number of Sentences Trained on: 30768\n",
            "Training loss epoch: 0.2584314665940093\n",
            "Training accuracy epoch: 1782.75\n",
            "Number of batches: 1924\n",
            "Number of Sentences Trained on: 30784\n",
            "Training loss epoch: 0.2583840723234144\n",
            "Training accuracy epoch: 1783.6875\n",
            "Number of batches: 1925\n",
            "Number of Sentences Trained on: 30800\n",
            "Training loss epoch: 0.25847296402011133\n",
            "Training accuracy epoch: 1784.5\n",
            "Number of batches: 1926\n",
            "Number of Sentences Trained on: 30816\n",
            "Training loss epoch: 0.25836735800997035\n",
            "Training accuracy epoch: 1785.5\n",
            "Number of batches: 1927\n",
            "Number of Sentences Trained on: 30832\n",
            "Training loss epoch: 0.2583268469936674\n",
            "Training accuracy epoch: 1786.4375\n",
            "Number of batches: 1928\n",
            "Number of Sentences Trained on: 30848\n",
            "Training loss epoch: 0.258227417359931\n",
            "Training accuracy epoch: 1787.4375\n",
            "Number of batches: 1929\n",
            "Number of Sentences Trained on: 30864\n",
            "Training loss epoch: 0.25815205252415696\n",
            "Training accuracy epoch: 1788.375\n",
            "Number of batches: 1930\n",
            "Number of Sentences Trained on: 30880\n",
            "Training loss epoch: 0.2583168028568794\n",
            "Training accuracy epoch: 1789.1875\n",
            "Number of batches: 1931\n",
            "Number of Sentences Trained on: 30896\n",
            "Training loss epoch: 0.2582019301315511\n",
            "Training accuracy epoch: 1790.1875\n",
            "Number of batches: 1932\n",
            "Number of Sentences Trained on: 30912\n",
            "Training loss epoch: 0.25809025242529615\n",
            "Training accuracy epoch: 1791.1875\n",
            "Number of batches: 1933\n",
            "Number of Sentences Trained on: 30928\n",
            "Training loss epoch: 0.2582369193029248\n",
            "Training accuracy epoch: 1792.0625\n",
            "Number of batches: 1934\n",
            "Number of Sentences Trained on: 30944\n",
            "Training loss epoch: 0.25812692284458905\n",
            "Training accuracy epoch: 1793.0625\n",
            "Number of batches: 1935\n",
            "Number of Sentences Trained on: 30960\n",
            "Training loss epoch: 0.2580167658329741\n",
            "Training accuracy epoch: 1794.0625\n",
            "Number of batches: 1936\n",
            "Number of Sentences Trained on: 30976\n",
            "Training loss epoch: 0.257905376075479\n",
            "Training accuracy epoch: 1795.0625\n",
            "Number of batches: 1937\n",
            "Number of Sentences Trained on: 30992\n",
            "Training loss epoch: 0.2578337503240566\n",
            "Training accuracy epoch: 1796.0\n",
            "Number of batches: 1938\n",
            "Number of Sentences Trained on: 31008\n",
            "Training loss epoch: 0.2577995342063675\n",
            "Training accuracy epoch: 1796.875\n",
            "Number of batches: 1939\n",
            "Number of Sentences Trained on: 31024\n",
            "Training loss epoch: 0.2578159980547912\n",
            "Training accuracy epoch: 1797.8125\n",
            "Number of batches: 1940\n",
            "Number of Sentences Trained on: 31040\n",
            "Training loss epoch: 0.25788644889666135\n",
            "Training accuracy epoch: 1798.625\n",
            "Number of batches: 1941\n",
            "Number of Sentences Trained on: 31056\n",
            "Training loss epoch: 0.25779429772469753\n",
            "Training accuracy epoch: 1799.625\n",
            "Number of batches: 1942\n",
            "Number of Sentences Trained on: 31072\n",
            "Training loss epoch: 0.2576825234207851\n",
            "Training accuracy epoch: 1800.625\n",
            "Number of batches: 1943\n",
            "Number of Sentences Trained on: 31088\n",
            "Training loss epoch: 0.25760239752444514\n",
            "Training accuracy epoch: 1801.625\n",
            "Number of batches: 1944\n",
            "Number of Sentences Trained on: 31104\n",
            "Training loss epoch: 0.25754185665571216\n",
            "Training accuracy epoch: 1802.5625\n",
            "Number of batches: 1945\n",
            "Number of Sentences Trained on: 31120\n",
            "Training loss epoch: 0.257419951476888\n",
            "Training accuracy epoch: 1803.5625\n",
            "Number of batches: 1946\n",
            "Number of Sentences Trained on: 31136\n",
            "Training loss epoch: 0.25737518204005866\n",
            "Training accuracy epoch: 1804.4375\n",
            "Number of batches: 1947\n",
            "Number of Sentences Trained on: 31152\n",
            "Training loss epoch: 0.25730451826987216\n",
            "Training accuracy epoch: 1805.4375\n",
            "Number of batches: 1948\n",
            "Number of Sentences Trained on: 31168\n",
            "Training loss epoch: 0.2572411921352202\n",
            "Training accuracy epoch: 1806.375\n",
            "Number of batches: 1949\n",
            "Number of Sentences Trained on: 31184\n",
            "Training loss epoch: 0.2572617292045974\n",
            "Training accuracy epoch: 1807.3125\n",
            "Number of batches: 1950\n",
            "Number of Sentences Trained on: 31200\n",
            "Training loss epoch: 0.25721860855582934\n",
            "Training accuracy epoch: 1808.25\n",
            "Number of batches: 1951\n",
            "Number of Sentences Trained on: 31216\n",
            "Training loss epoch: 0.2570906118303728\n",
            "Training accuracy epoch: 1809.25\n",
            "Number of batches: 1952\n",
            "Number of Sentences Trained on: 31232\n",
            "Training loss epoch: 0.25711527778502385\n",
            "Training accuracy epoch: 1810.1875\n",
            "Number of batches: 1953\n",
            "Number of Sentences Trained on: 31248\n",
            "Training loss epoch: 0.2571557110243171\n",
            "Training accuracy epoch: 1811.0625\n",
            "Number of batches: 1954\n",
            "Number of Sentences Trained on: 31264\n",
            "Training loss epoch: 0.25706853506862737\n",
            "Training accuracy epoch: 1812.0625\n",
            "Number of batches: 1955\n",
            "Number of Sentences Trained on: 31280\n",
            "Training loss epoch: 0.2569732741516556\n",
            "Training accuracy epoch: 1813.0625\n",
            "Number of batches: 1956\n",
            "Number of Sentences Trained on: 31296\n",
            "Training loss epoch: 0.2568564188857475\n",
            "Training accuracy epoch: 1814.0625\n",
            "Number of batches: 1957\n",
            "Number of Sentences Trained on: 31312\n",
            "Training loss epoch: 0.2567756397823734\n",
            "Training accuracy epoch: 1814.9375\n",
            "Number of batches: 1958\n",
            "Number of Sentences Trained on: 31328\n",
            "Training loss epoch: 0.25676756479452695\n",
            "Training accuracy epoch: 1815.875\n",
            "Number of batches: 1959\n",
            "Number of Sentences Trained on: 31344\n",
            "Training loss epoch: 0.2566824349959628\n",
            "Training accuracy epoch: 1816.875\n",
            "Number of batches: 1960\n",
            "Number of Sentences Trained on: 31360\n",
            "Training loss epoch: 0.2565758730888496\n",
            "Training accuracy epoch: 1817.875\n",
            "Number of batches: 1961\n",
            "Number of Sentences Trained on: 31376\n",
            "Training loss epoch: 0.25661898595038857\n",
            "Training accuracy epoch: 1818.8125\n",
            "Number of batches: 1962\n",
            "Number of Sentences Trained on: 31392\n",
            "Training loss epoch: 0.25650245915923764\n",
            "Training accuracy epoch: 1819.8125\n",
            "Number of batches: 1963\n",
            "Number of Sentences Trained on: 31408\n",
            "Training loss epoch: 0.2563837463260949\n",
            "Training accuracy epoch: 1820.8125\n",
            "Number of batches: 1964\n",
            "Number of Sentences Trained on: 31424\n",
            "Training loss epoch: 0.2564408067021396\n",
            "Training accuracy epoch: 1821.6875\n",
            "Number of batches: 1965\n",
            "Number of Sentences Trained on: 31440\n",
            "Training loss epoch: 0.25635461617850475\n",
            "Training accuracy epoch: 1822.625\n",
            "Number of batches: 1966\n",
            "Number of Sentences Trained on: 31456\n",
            "Training loss epoch: 0.2563661448359224\n",
            "Training accuracy epoch: 1823.5\n",
            "Number of batches: 1967\n",
            "Number of Sentences Trained on: 31472\n",
            "Training loss epoch: 0.25626132769814913\n",
            "Training accuracy epoch: 1824.5\n",
            "Number of batches: 1968\n",
            "Number of Sentences Trained on: 31488\n",
            "Training loss epoch: 0.25614115240268975\n",
            "Training accuracy epoch: 1825.5\n",
            "Number of batches: 1969\n",
            "Number of Sentences Trained on: 31504\n",
            "Training loss epoch: 0.256133814036449\n",
            "Training accuracy epoch: 1826.4375\n",
            "Number of batches: 1970\n",
            "Number of Sentences Trained on: 31520\n",
            "Training loss epoch: 0.2561034149547471\n",
            "Training accuracy epoch: 1827.3125\n",
            "Number of batches: 1971\n",
            "Number of Sentences Trained on: 31536\n",
            "Training loss epoch: 0.256136344153997\n",
            "Training accuracy epoch: 1828.1875\n",
            "Number of batches: 1972\n",
            "Number of Sentences Trained on: 31552\n",
            "Training loss epoch: 0.2560630464780976\n",
            "Training accuracy epoch: 1829.1875\n",
            "Number of batches: 1973\n",
            "Number of Sentences Trained on: 31568\n",
            "Training loss epoch: 0.2559943923319991\n",
            "Training accuracy epoch: 1830.125\n",
            "Number of batches: 1974\n",
            "Number of Sentences Trained on: 31584\n",
            "Training loss epoch: 0.2559889148321899\n",
            "Training accuracy epoch: 1831.0625\n",
            "Number of batches: 1975\n",
            "Number of Sentences Trained on: 31600\n",
            "Training loss epoch: 0.25595323943678167\n",
            "Training accuracy epoch: 1832.0\n",
            "Number of batches: 1976\n",
            "Number of Sentences Trained on: 31616\n",
            "Training loss epoch: 0.25598034969793393\n",
            "Training accuracy epoch: 1832.9375\n",
            "Number of batches: 1977\n",
            "Number of Sentences Trained on: 31632\n",
            "Training loss epoch: 0.25586105905580586\n",
            "Training accuracy epoch: 1833.9375\n",
            "Number of batches: 1978\n",
            "Number of Sentences Trained on: 31648\n",
            "Training loss epoch: 0.2558246119900363\n",
            "Training accuracy epoch: 1834.875\n",
            "Number of batches: 1979\n",
            "Number of Sentences Trained on: 31664\n",
            "Training loss epoch: 0.2557066881968028\n",
            "Training accuracy epoch: 1835.875\n",
            "Number of batches: 1980\n",
            "Number of Sentences Trained on: 31680\n",
            "Training loss epoch: 0.2556316810027743\n",
            "Training accuracy epoch: 1836.875\n",
            "Number of batches: 1981\n",
            "Number of Sentences Trained on: 31696\n",
            "Training loss epoch: 0.25563345284470923\n",
            "Training accuracy epoch: 1837.75\n",
            "Number of batches: 1982\n",
            "Number of Sentences Trained on: 31712\n",
            "Training loss epoch: 0.2555434750674825\n",
            "Training accuracy epoch: 1838.6875\n",
            "Number of batches: 1983\n",
            "Number of Sentences Trained on: 31728\n",
            "Training loss epoch: 0.2556332815696688\n",
            "Training accuracy epoch: 1839.625\n",
            "Number of batches: 1984\n",
            "Number of Sentences Trained on: 31744\n",
            "Training loss epoch: 0.255772243184448\n",
            "Training accuracy epoch: 1840.4375\n",
            "Number of batches: 1985\n",
            "Number of Sentences Trained on: 31760\n",
            "Training loss epoch: 0.25584042561076364\n",
            "Training accuracy epoch: 1841.3125\n",
            "Number of batches: 1986\n",
            "Number of Sentences Trained on: 31776\n",
            "Training loss epoch: 0.25574634642966904\n",
            "Training accuracy epoch: 1842.3125\n",
            "Number of batches: 1987\n",
            "Number of Sentences Trained on: 31792\n",
            "Training loss epoch: 0.25570504972195884\n",
            "Training accuracy epoch: 1843.25\n",
            "Number of batches: 1988\n",
            "Number of Sentences Trained on: 31808\n",
            "Training loss epoch: 0.25573504836354244\n",
            "Training accuracy epoch: 1844.125\n",
            "Number of batches: 1989\n",
            "Number of Sentences Trained on: 31824\n",
            "Training loss epoch: 0.25560932296173805\n",
            "Training accuracy epoch: 1845.125\n",
            "Number of batches: 1990\n",
            "Number of Sentences Trained on: 31840\n",
            "Training loss epoch: 0.2555670051436473\n",
            "Training accuracy epoch: 1846.125\n",
            "Number of batches: 1991\n",
            "Number of Sentences Trained on: 31856\n",
            "Training loss epoch: 0.25552289334993555\n",
            "Training accuracy epoch: 1847.0625\n",
            "Number of batches: 1992\n",
            "Number of Sentences Trained on: 31872\n",
            "Training loss epoch: 0.25550831027134574\n",
            "Training accuracy epoch: 1848.0\n",
            "Number of batches: 1993\n",
            "Number of Sentences Trained on: 31888\n",
            "Training loss epoch: 0.25540935505674767\n",
            "Training accuracy epoch: 1849.0\n",
            "Number of batches: 1994\n",
            "Number of Sentences Trained on: 31904\n",
            "Training loss epoch: 0.2553188612745295\n",
            "Training accuracy epoch: 1850.0\n",
            "Number of batches: 1995\n",
            "Number of Sentences Trained on: 31920\n",
            "Training loss epoch: 0.255195789577585\n",
            "Training accuracy epoch: 1851.0\n",
            "Number of batches: 1996\n",
            "Number of Sentences Trained on: 31936\n",
            "Training loss epoch: 0.25510350981998664\n",
            "Training accuracy epoch: 1852.0\n",
            "Number of batches: 1997\n",
            "Number of Sentences Trained on: 31952\n",
            "Training loss epoch: 0.2550073282968428\n",
            "Training accuracy epoch: 1853.0\n",
            "Number of batches: 1998\n",
            "Number of Sentences Trained on: 31968\n",
            "Training loss epoch: 0.25504707326351284\n",
            "Training accuracy epoch: 1853.875\n",
            "Number of batches: 1999\n",
            "Number of Sentences Trained on: 31984\n",
            "Training loss epoch: 0.25502028738055377\n",
            "Training accuracy epoch: 1854.8125\n",
            "Number of batches: 2000\n",
            "Number of Sentences Trained on: 32000\n",
            "Training loss per 100 training steps: 0.2551633684808898\n",
            "Training loss epoch: 0.2551633684808898\n",
            "Training accuracy epoch: 1855.625\n",
            "Number of batches: 2001\n",
            "Number of Sentences Trained on: 32016\n",
            "Training loss epoch: 0.2550721327326708\n",
            "Training accuracy epoch: 1856.625\n",
            "Number of batches: 2002\n",
            "Number of Sentences Trained on: 32032\n",
            "Training loss epoch: 0.2549677454316417\n",
            "Training accuracy epoch: 1857.625\n",
            "Number of batches: 2003\n",
            "Number of Sentences Trained on: 32048\n",
            "Training loss epoch: 0.2549252357517932\n",
            "Training accuracy epoch: 1858.5625\n",
            "Number of batches: 2004\n",
            "Number of Sentences Trained on: 32064\n",
            "Training loss epoch: 0.2548231485766589\n",
            "Training accuracy epoch: 1859.5625\n",
            "Number of batches: 2005\n",
            "Number of Sentences Trained on: 32080\n",
            "Training loss epoch: 0.2548093059994392\n",
            "Training accuracy epoch: 1860.5\n",
            "Number of batches: 2006\n",
            "Number of Sentences Trained on: 32096\n",
            "Training loss epoch: 0.2548076782976966\n",
            "Training accuracy epoch: 1861.375\n",
            "Number of batches: 2007\n",
            "Number of Sentences Trained on: 32112\n",
            "Training loss epoch: 0.2547122649764976\n",
            "Training accuracy epoch: 1862.375\n",
            "Number of batches: 2008\n",
            "Number of Sentences Trained on: 32128\n",
            "Training loss epoch: 0.25462905721689794\n",
            "Training accuracy epoch: 1863.3125\n",
            "Number of batches: 2009\n",
            "Number of Sentences Trained on: 32144\n",
            "Training loss epoch: 0.2545163776521659\n",
            "Training accuracy epoch: 1864.3125\n",
            "Number of batches: 2010\n",
            "Number of Sentences Trained on: 32160\n",
            "Training loss epoch: 0.2544080891578367\n",
            "Training accuracy epoch: 1865.3125\n",
            "Number of batches: 2011\n",
            "Number of Sentences Trained on: 32176\n",
            "Training loss epoch: 0.2543704027743273\n",
            "Training accuracy epoch: 1866.25\n",
            "Number of batches: 2012\n",
            "Number of Sentences Trained on: 32192\n",
            "Training loss epoch: 0.2542496910994207\n",
            "Training accuracy epoch: 1867.25\n",
            "Number of batches: 2013\n",
            "Number of Sentences Trained on: 32208\n",
            "Training loss epoch: 0.25413456109473564\n",
            "Training accuracy epoch: 1868.25\n",
            "Number of batches: 2014\n",
            "Number of Sentences Trained on: 32224\n",
            "Training loss epoch: 0.25412710254443416\n",
            "Training accuracy epoch: 1869.1875\n",
            "Number of batches: 2015\n",
            "Number of Sentences Trained on: 32240\n",
            "Training loss epoch: 0.2541125640582611\n",
            "Training accuracy epoch: 1870.125\n",
            "Number of batches: 2016\n",
            "Number of Sentences Trained on: 32256\n",
            "Training loss epoch: 0.25399792284728295\n",
            "Training accuracy epoch: 1871.125\n",
            "Number of batches: 2017\n",
            "Number of Sentences Trained on: 32272\n",
            "Training loss epoch: 0.2540257241455995\n",
            "Training accuracy epoch: 1872.0\n",
            "Number of batches: 2018\n",
            "Number of Sentences Trained on: 32288\n",
            "Training loss epoch: 0.2541197516405129\n",
            "Training accuracy epoch: 1872.875\n",
            "Number of batches: 2019\n",
            "Number of Sentences Trained on: 32304\n",
            "Training loss epoch: 0.2540148510611478\n",
            "Training accuracy epoch: 1873.875\n",
            "Number of batches: 2020\n",
            "Number of Sentences Trained on: 32320\n",
            "Training loss epoch: 0.2539164172006215\n",
            "Training accuracy epoch: 1874.875\n",
            "Number of batches: 2021\n",
            "Number of Sentences Trained on: 32336\n",
            "Training loss epoch: 0.2539086839378536\n",
            "Training accuracy epoch: 1875.8125\n",
            "Number of batches: 2022\n",
            "Number of Sentences Trained on: 32352\n",
            "Training loss epoch: 0.2539438830045923\n",
            "Training accuracy epoch: 1876.625\n",
            "Number of batches: 2023\n",
            "Number of Sentences Trained on: 32368\n",
            "Training loss epoch: 0.2539143992202327\n",
            "Training accuracy epoch: 1877.5625\n",
            "Number of batches: 2024\n",
            "Number of Sentences Trained on: 32384\n",
            "Training loss epoch: 0.25386594207980384\n",
            "Training accuracy epoch: 1878.5\n",
            "Number of batches: 2025\n",
            "Number of Sentences Trained on: 32400\n",
            "Training loss epoch: 0.2538323970874707\n",
            "Training accuracy epoch: 1879.4375\n",
            "Number of batches: 2026\n",
            "Number of Sentences Trained on: 32416\n",
            "Training loss epoch: 0.2537326863675126\n",
            "Training accuracy epoch: 1880.4375\n",
            "Number of batches: 2027\n",
            "Number of Sentences Trained on: 32432\n",
            "Training loss epoch: 0.25376878661761165\n",
            "Training accuracy epoch: 1881.3125\n",
            "Number of batches: 2028\n",
            "Number of Sentences Trained on: 32448\n",
            "Training loss epoch: 0.2536859867732629\n",
            "Training accuracy epoch: 1882.3125\n",
            "Number of batches: 2029\n",
            "Number of Sentences Trained on: 32464\n",
            "Training loss epoch: 0.25365846046462215\n",
            "Training accuracy epoch: 1883.1875\n",
            "Number of batches: 2030\n",
            "Number of Sentences Trained on: 32480\n",
            "Training loss epoch: 0.2536326961872475\n",
            "Training accuracy epoch: 1884.0625\n",
            "Number of batches: 2031\n",
            "Number of Sentences Trained on: 32496\n",
            "Training loss epoch: 0.253550281684468\n",
            "Training accuracy epoch: 1885.0\n",
            "Number of batches: 2032\n",
            "Number of Sentences Trained on: 32512\n",
            "Training loss epoch: 0.253475909971573\n",
            "Training accuracy epoch: 1885.9375\n",
            "Number of batches: 2033\n",
            "Number of Sentences Trained on: 32528\n",
            "Training loss epoch: 0.25341692345854233\n",
            "Training accuracy epoch: 1886.875\n",
            "Number of batches: 2034\n",
            "Number of Sentences Trained on: 32544\n",
            "Training loss epoch: 0.2533728864570251\n",
            "Training accuracy epoch: 1887.8125\n",
            "Number of batches: 2035\n",
            "Number of Sentences Trained on: 32560\n",
            "Training loss epoch: 0.2532842898350721\n",
            "Training accuracy epoch: 1888.8125\n",
            "Number of batches: 2036\n",
            "Number of Sentences Trained on: 32576\n",
            "Training loss epoch: 0.25323493039362577\n",
            "Training accuracy epoch: 1889.75\n",
            "Number of batches: 2037\n",
            "Number of Sentences Trained on: 32592\n",
            "Training loss epoch: 0.2531571212495194\n",
            "Training accuracy epoch: 1890.6875\n",
            "Number of batches: 2038\n",
            "Number of Sentences Trained on: 32608\n",
            "Training loss epoch: 0.25306492877503844\n",
            "Training accuracy epoch: 1891.6875\n",
            "Number of batches: 2039\n",
            "Number of Sentences Trained on: 32624\n",
            "Training loss epoch: 0.2530587699141025\n",
            "Training accuracy epoch: 1892.625\n",
            "Number of batches: 2040\n",
            "Number of Sentences Trained on: 32640\n",
            "Training loss epoch: 0.25302964883561246\n",
            "Training accuracy epoch: 1893.5625\n",
            "Number of batches: 2041\n",
            "Number of Sentences Trained on: 32656\n",
            "Training loss epoch: 0.2530824079344288\n",
            "Training accuracy epoch: 1894.5\n",
            "Number of batches: 2042\n",
            "Number of Sentences Trained on: 32672\n",
            "Training loss epoch: 0.25297086881870257\n",
            "Training accuracy epoch: 1895.5\n",
            "Number of batches: 2043\n",
            "Number of Sentences Trained on: 32688\n",
            "Training loss epoch: 0.25287063513087826\n",
            "Training accuracy epoch: 1896.5\n",
            "Number of batches: 2044\n",
            "Number of Sentences Trained on: 32704\n",
            "Training loss epoch: 0.2528091667620761\n",
            "Training accuracy epoch: 1897.4375\n",
            "Number of batches: 2045\n",
            "Number of Sentences Trained on: 32720\n",
            "Training loss epoch: 0.2527139752639871\n",
            "Training accuracy epoch: 1898.4375\n",
            "Number of batches: 2046\n",
            "Number of Sentences Trained on: 32736\n",
            "Training loss epoch: 0.2526297656697293\n",
            "Training accuracy epoch: 1899.4375\n",
            "Number of batches: 2047\n",
            "Number of Sentences Trained on: 32752\n",
            "Training loss epoch: 0.25253470430789093\n",
            "Training accuracy epoch: 1900.375\n",
            "Number of batches: 2048\n",
            "Number of Sentences Trained on: 32768\n",
            "Training loss epoch: 0.25247476068804864\n",
            "Training accuracy epoch: 1901.3125\n",
            "Number of batches: 2049\n",
            "Number of Sentences Trained on: 32784\n",
            "Training loss epoch: 0.2525188935052876\n",
            "Training accuracy epoch: 1902.1875\n",
            "Number of batches: 2050\n",
            "Number of Sentences Trained on: 32800\n",
            "Training loss epoch: 0.2524642972993065\n",
            "Training accuracy epoch: 1903.125\n",
            "Number of batches: 2051\n",
            "Number of Sentences Trained on: 32816\n",
            "Training loss epoch: 0.2525814264376428\n",
            "Training accuracy epoch: 1904.0\n",
            "Number of batches: 2052\n",
            "Number of Sentences Trained on: 32832\n",
            "Training loss epoch: 0.2524770346918827\n",
            "Training accuracy epoch: 1905.0\n",
            "Number of batches: 2053\n",
            "Number of Sentences Trained on: 32848\n",
            "Training loss epoch: 0.25238969597553773\n",
            "Training accuracy epoch: 1906.0\n",
            "Number of batches: 2054\n",
            "Number of Sentences Trained on: 32864\n",
            "Training loss epoch: 0.2522874534179948\n",
            "Training accuracy epoch: 1907.0\n",
            "Number of batches: 2055\n",
            "Number of Sentences Trained on: 32880\n",
            "Training loss epoch: 0.2522760897141914\n",
            "Training accuracy epoch: 1907.9375\n",
            "Number of batches: 2056\n",
            "Number of Sentences Trained on: 32896\n",
            "Training loss epoch: 0.25215771715946794\n",
            "Training accuracy epoch: 1908.9375\n",
            "Number of batches: 2057\n",
            "Number of Sentences Trained on: 32912\n",
            "Training loss epoch: 0.25211642861983535\n",
            "Training accuracy epoch: 1909.875\n",
            "Number of batches: 2058\n",
            "Number of Sentences Trained on: 32928\n",
            "Training loss epoch: 0.2520838463145221\n",
            "Training accuracy epoch: 1910.8125\n",
            "Number of batches: 2059\n",
            "Number of Sentences Trained on: 32944\n",
            "Training loss epoch: 0.25215232614473015\n",
            "Training accuracy epoch: 1911.6875\n",
            "Number of batches: 2060\n",
            "Number of Sentences Trained on: 32960\n",
            "Training loss epoch: 0.2520759227242337\n",
            "Training accuracy epoch: 1912.625\n",
            "Number of batches: 2061\n",
            "Number of Sentences Trained on: 32976\n",
            "Training loss epoch: 0.2519602004951724\n",
            "Training accuracy epoch: 1913.625\n",
            "Number of batches: 2062\n",
            "Number of Sentences Trained on: 32992\n",
            "Training loss epoch: 0.2518836487169498\n",
            "Training accuracy epoch: 1914.5625\n",
            "Number of batches: 2063\n",
            "Number of Sentences Trained on: 33008\n",
            "Training loss epoch: 0.25200172917476976\n",
            "Training accuracy epoch: 1915.4375\n",
            "Number of batches: 2064\n",
            "Number of Sentences Trained on: 33024\n",
            "Training loss epoch: 0.25203479781891186\n",
            "Training accuracy epoch: 1916.3125\n",
            "Number of batches: 2065\n",
            "Number of Sentences Trained on: 33040\n",
            "Training loss epoch: 0.2519499632581304\n",
            "Training accuracy epoch: 1917.25\n",
            "Number of batches: 2066\n",
            "Number of Sentences Trained on: 33056\n",
            "Training loss epoch: 0.25195246687078743\n",
            "Training accuracy epoch: 1918.1875\n",
            "Number of batches: 2067\n",
            "Number of Sentences Trained on: 33072\n",
            "Training loss epoch: 0.25188482822436026\n",
            "Training accuracy epoch: 1919.1875\n",
            "Number of batches: 2068\n",
            "Number of Sentences Trained on: 33088\n",
            "Training loss epoch: 0.25187315368901536\n",
            "Training accuracy epoch: 1920.125\n",
            "Number of batches: 2069\n",
            "Number of Sentences Trained on: 33104\n",
            "Training loss epoch: 0.25176764016494085\n",
            "Training accuracy epoch: 1921.125\n",
            "Number of batches: 2070\n",
            "Number of Sentences Trained on: 33120\n",
            "Training loss epoch: 0.2518336344483335\n",
            "Training accuracy epoch: 1922.0\n",
            "Number of batches: 2071\n",
            "Number of Sentences Trained on: 33136\n",
            "Training loss epoch: 0.2517827822571498\n",
            "Training accuracy epoch: 1922.9375\n",
            "Number of batches: 2072\n",
            "Number of Sentences Trained on: 33152\n",
            "Training loss epoch: 0.25174059850391534\n",
            "Training accuracy epoch: 1923.875\n",
            "Number of batches: 2073\n",
            "Number of Sentences Trained on: 33168\n",
            "Training loss epoch: 0.2518137573886918\n",
            "Training accuracy epoch: 1924.8125\n",
            "Number of batches: 2074\n",
            "Number of Sentences Trained on: 33184\n",
            "Training loss epoch: 0.2516992480204587\n",
            "Training accuracy epoch: 1925.8125\n",
            "Number of batches: 2075\n",
            "Number of Sentences Trained on: 33200\n",
            "Training loss epoch: 0.2516453977389825\n",
            "Training accuracy epoch: 1926.75\n",
            "Number of batches: 2076\n",
            "Number of Sentences Trained on: 33216\n",
            "Training loss epoch: 0.2515285838893976\n",
            "Training accuracy epoch: 1927.75\n",
            "Number of batches: 2077\n",
            "Number of Sentences Trained on: 33232\n",
            "Training loss epoch: 0.25145566842363676\n",
            "Training accuracy epoch: 1928.75\n",
            "Number of batches: 2078\n",
            "Number of Sentences Trained on: 33248\n",
            "Training loss epoch: 0.25138193949745247\n",
            "Training accuracy epoch: 1929.75\n",
            "Number of batches: 2079\n",
            "Number of Sentences Trained on: 33264\n",
            "Training loss epoch: 0.25142398797817384\n",
            "Training accuracy epoch: 1930.625\n",
            "Number of batches: 2080\n",
            "Number of Sentences Trained on: 33280\n",
            "Training loss epoch: 0.2513290405737085\n",
            "Training accuracy epoch: 1931.625\n",
            "Number of batches: 2081\n",
            "Number of Sentences Trained on: 33296\n",
            "Training loss epoch: 0.2512547440574604\n",
            "Training accuracy epoch: 1932.5625\n",
            "Number of batches: 2082\n",
            "Number of Sentences Trained on: 33312\n",
            "Training loss epoch: 0.25113698898824005\n",
            "Training accuracy epoch: 1933.5625\n",
            "Number of batches: 2083\n",
            "Number of Sentences Trained on: 33328\n",
            "Training loss epoch: 0.2510411029401362\n",
            "Training accuracy epoch: 1934.5625\n",
            "Number of batches: 2084\n",
            "Number of Sentences Trained on: 33344\n",
            "Training loss epoch: 0.25096642837057903\n",
            "Training accuracy epoch: 1935.5625\n",
            "Number of batches: 2085\n",
            "Number of Sentences Trained on: 33360\n",
            "Training loss epoch: 0.2510097838497577\n",
            "Training accuracy epoch: 1936.4375\n",
            "Number of batches: 2086\n",
            "Number of Sentences Trained on: 33376\n",
            "Training loss epoch: 0.2510055056996251\n",
            "Training accuracy epoch: 1937.375\n",
            "Number of batches: 2087\n",
            "Number of Sentences Trained on: 33392\n",
            "Training loss epoch: 0.251077713070382\n",
            "Training accuracy epoch: 1938.25\n",
            "Number of batches: 2088\n",
            "Number of Sentences Trained on: 33408\n",
            "Training loss epoch: 0.2510351547398486\n",
            "Training accuracy epoch: 1939.1875\n",
            "Number of batches: 2089\n",
            "Number of Sentences Trained on: 33424\n",
            "Training loss epoch: 0.2509509668576322\n",
            "Training accuracy epoch: 1940.1875\n",
            "Number of batches: 2090\n",
            "Number of Sentences Trained on: 33440\n",
            "Training loss epoch: 0.250933503172\n",
            "Training accuracy epoch: 1941.125\n",
            "Number of batches: 2091\n",
            "Number of Sentences Trained on: 33456\n",
            "Training loss epoch: 0.2509760462269846\n",
            "Training accuracy epoch: 1942.0625\n",
            "Number of batches: 2092\n",
            "Number of Sentences Trained on: 33472\n",
            "Training loss epoch: 0.25087061008070033\n",
            "Training accuracy epoch: 1943.0625\n",
            "Number of batches: 2093\n",
            "Number of Sentences Trained on: 33488\n",
            "Training loss epoch: 0.2507732703469235\n",
            "Training accuracy epoch: 1944.0625\n",
            "Number of batches: 2094\n",
            "Number of Sentences Trained on: 33504\n",
            "Training loss epoch: 0.2507931868101166\n",
            "Training accuracy epoch: 1944.9375\n",
            "Number of batches: 2095\n",
            "Number of Sentences Trained on: 33520\n",
            "Training loss epoch: 0.2507173574499818\n",
            "Training accuracy epoch: 1945.9375\n",
            "Number of batches: 2096\n",
            "Number of Sentences Trained on: 33536\n",
            "Training loss epoch: 0.2506238652122752\n",
            "Training accuracy epoch: 1946.9375\n",
            "Number of batches: 2097\n",
            "Number of Sentences Trained on: 33552\n",
            "Training loss epoch: 0.2505532598819449\n",
            "Training accuracy epoch: 1947.875\n",
            "Number of batches: 2098\n",
            "Number of Sentences Trained on: 33568\n",
            "Training loss epoch: 0.2504819023339482\n",
            "Training accuracy epoch: 1948.875\n",
            "Number of batches: 2099\n",
            "Number of Sentences Trained on: 33584\n",
            "Training loss epoch: 0.2504082078967864\n",
            "Training accuracy epoch: 1949.75\n",
            "Number of batches: 2100\n",
            "Number of Sentences Trained on: 33600\n",
            "Training loss per 100 training steps: 0.2502980926490767\n",
            "Training loss epoch: 0.2502980926490767\n",
            "Training accuracy epoch: 1950.75\n",
            "Number of batches: 2101\n",
            "Number of Sentences Trained on: 33616\n",
            "Training loss epoch: 0.25023042742489926\n",
            "Training accuracy epoch: 1951.6875\n",
            "Number of batches: 2102\n",
            "Number of Sentences Trained on: 33632\n",
            "Training loss epoch: 0.25013772011112334\n",
            "Training accuracy epoch: 1952.6875\n",
            "Number of batches: 2103\n",
            "Number of Sentences Trained on: 33648\n",
            "Training loss epoch: 0.2501657890386529\n",
            "Training accuracy epoch: 1953.625\n",
            "Number of batches: 2104\n",
            "Number of Sentences Trained on: 33664\n",
            "Training loss epoch: 0.25007691195172715\n",
            "Training accuracy epoch: 1954.5625\n",
            "Number of batches: 2105\n",
            "Number of Sentences Trained on: 33680\n",
            "Training loss epoch: 0.24997207649597847\n",
            "Training accuracy epoch: 1955.5625\n",
            "Number of batches: 2106\n",
            "Number of Sentences Trained on: 33696\n",
            "Training loss epoch: 0.24986921337653725\n",
            "Training accuracy epoch: 1956.5625\n",
            "Number of batches: 2107\n",
            "Number of Sentences Trained on: 33712\n",
            "Training loss epoch: 0.2499071810046826\n",
            "Training accuracy epoch: 1957.5\n",
            "Number of batches: 2108\n",
            "Number of Sentences Trained on: 33728\n",
            "Training loss epoch: 0.249863705746856\n",
            "Training accuracy epoch: 1958.4375\n",
            "Number of batches: 2109\n",
            "Number of Sentences Trained on: 33744\n",
            "Training loss epoch: 0.24978003695240836\n",
            "Training accuracy epoch: 1959.4375\n",
            "Number of batches: 2110\n",
            "Number of Sentences Trained on: 33760\n",
            "Training loss epoch: 0.24986805759254738\n",
            "Training accuracy epoch: 1960.3125\n",
            "Number of batches: 2111\n",
            "Number of Sentences Trained on: 33776\n",
            "Training loss epoch: 0.24982122331703344\n",
            "Training accuracy epoch: 1961.3125\n",
            "Number of batches: 2112\n",
            "Number of Sentences Trained on: 33792\n",
            "Training loss epoch: 0.2497127680872667\n",
            "Training accuracy epoch: 1962.3125\n",
            "Number of batches: 2113\n",
            "Number of Sentences Trained on: 33808\n",
            "Training loss epoch: 0.24968858285280177\n",
            "Training accuracy epoch: 1963.25\n",
            "Number of batches: 2114\n",
            "Number of Sentences Trained on: 33824\n",
            "Training loss epoch: 0.24965766316068405\n",
            "Training accuracy epoch: 1964.1875\n",
            "Number of batches: 2115\n",
            "Number of Sentences Trained on: 33840\n",
            "Training loss epoch: 0.2495545207600713\n",
            "Training accuracy epoch: 1965.1875\n",
            "Number of batches: 2116\n",
            "Number of Sentences Trained on: 33856\n",
            "Training loss epoch: 0.24946537776487226\n",
            "Training accuracy epoch: 1966.1875\n",
            "Number of batches: 2117\n",
            "Number of Sentences Trained on: 33872\n",
            "Training loss epoch: 0.2493727413325802\n",
            "Training accuracy epoch: 1967.1875\n",
            "Number of batches: 2118\n",
            "Number of Sentences Trained on: 33888\n",
            "Training loss epoch: 0.24932717097001947\n",
            "Training accuracy epoch: 1968.0625\n",
            "Number of batches: 2119\n",
            "Number of Sentences Trained on: 33904\n",
            "Training loss epoch: 0.24921201827786513\n",
            "Training accuracy epoch: 1969.0625\n",
            "Number of batches: 2120\n",
            "Number of Sentences Trained on: 33920\n",
            "Training loss epoch: 0.2491299816355011\n",
            "Training accuracy epoch: 1970.0625\n",
            "Number of batches: 2121\n",
            "Number of Sentences Trained on: 33936\n",
            "Training loss epoch: 0.24911339727470003\n",
            "Training accuracy epoch: 1970.9375\n",
            "Number of batches: 2122\n",
            "Number of Sentences Trained on: 33952\n",
            "Training loss epoch: 0.24907785272633712\n",
            "Training accuracy epoch: 1971.875\n",
            "Number of batches: 2123\n",
            "Number of Sentences Trained on: 33968\n",
            "Training loss epoch: 0.24908238763415838\n",
            "Training accuracy epoch: 1972.8125\n",
            "Number of batches: 2124\n",
            "Number of Sentences Trained on: 33984\n",
            "Training loss epoch: 0.24903225505045232\n",
            "Training accuracy epoch: 1973.75\n",
            "Number of batches: 2125\n",
            "Number of Sentences Trained on: 34000\n",
            "Training loss epoch: 0.2491271976116074\n",
            "Training accuracy epoch: 1974.625\n",
            "Number of batches: 2126\n",
            "Number of Sentences Trained on: 34016\n",
            "Training loss epoch: 0.24922434831205378\n",
            "Training accuracy epoch: 1975.5\n",
            "Number of batches: 2127\n",
            "Number of Sentences Trained on: 34032\n",
            "Training loss epoch: 0.24913525599084752\n",
            "Training accuracy epoch: 1976.5\n",
            "Number of batches: 2128\n",
            "Number of Sentences Trained on: 34048\n",
            "Training loss epoch: 0.24906548645133883\n",
            "Training accuracy epoch: 1977.5\n",
            "Number of batches: 2129\n",
            "Number of Sentences Trained on: 34064\n",
            "Training loss epoch: 0.24904561743988468\n",
            "Training accuracy epoch: 1978.4375\n",
            "Number of batches: 2130\n",
            "Number of Sentences Trained on: 34080\n",
            "Training loss epoch: 0.24893708965041403\n",
            "Training accuracy epoch: 1979.4375\n",
            "Number of batches: 2131\n",
            "Number of Sentences Trained on: 34096\n",
            "Training loss epoch: 0.24896177279983087\n",
            "Training accuracy epoch: 1980.25\n",
            "Number of batches: 2132\n",
            "Number of Sentences Trained on: 34112\n",
            "Training loss epoch: 0.2488743769637175\n",
            "Training accuracy epoch: 1981.25\n",
            "Number of batches: 2133\n",
            "Number of Sentences Trained on: 34128\n",
            "Training loss epoch: 0.24884100203754095\n",
            "Training accuracy epoch: 1982.1875\n",
            "Number of batches: 2134\n",
            "Number of Sentences Trained on: 34144\n",
            "Training loss epoch: 0.24913850087340947\n",
            "Training accuracy epoch: 1982.875\n",
            "Number of batches: 2135\n",
            "Number of Sentences Trained on: 34160\n",
            "Training loss epoch: 0.24903464322771932\n",
            "Training accuracy epoch: 1983.875\n",
            "Number of batches: 2136\n",
            "Number of Sentences Trained on: 34176\n",
            "Training loss epoch: 0.24892571523993037\n",
            "Training accuracy epoch: 1984.875\n",
            "Number of batches: 2137\n",
            "Number of Sentences Trained on: 34192\n",
            "Training loss epoch: 0.24881970376112578\n",
            "Training accuracy epoch: 1985.875\n",
            "Number of batches: 2138\n",
            "Number of Sentences Trained on: 34208\n",
            "Training loss epoch: 0.24872291953898984\n",
            "Training accuracy epoch: 1986.875\n",
            "Number of batches: 2139\n",
            "Number of Sentences Trained on: 34224\n",
            "Training loss epoch: 0.24862850389234442\n",
            "Training accuracy epoch: 1987.875\n",
            "Number of batches: 2140\n",
            "Number of Sentences Trained on: 34240\n",
            "Training loss epoch: 0.24860902077751995\n",
            "Training accuracy epoch: 1988.8125\n",
            "Number of batches: 2141\n",
            "Number of Sentences Trained on: 34256\n",
            "Training loss epoch: 0.24855989208062337\n",
            "Training accuracy epoch: 1989.75\n",
            "Number of batches: 2142\n",
            "Number of Sentences Trained on: 34272\n",
            "Training loss epoch: 0.24847738306823383\n",
            "Training accuracy epoch: 1990.75\n",
            "Number of batches: 2143\n",
            "Number of Sentences Trained on: 34288\n",
            "Training loss epoch: 0.24838986499069968\n",
            "Training accuracy epoch: 1991.75\n",
            "Number of batches: 2144\n",
            "Number of Sentences Trained on: 34304\n",
            "Training loss epoch: 0.2483124204487963\n",
            "Training accuracy epoch: 1992.6875\n",
            "Number of batches: 2145\n",
            "Number of Sentences Trained on: 34320\n",
            "Training loss epoch: 0.24830155616594896\n",
            "Training accuracy epoch: 1993.625\n",
            "Number of batches: 2146\n",
            "Number of Sentences Trained on: 34336\n",
            "Training loss epoch: 0.2482596329019917\n",
            "Training accuracy epoch: 1994.625\n",
            "Number of batches: 2147\n",
            "Number of Sentences Trained on: 34352\n",
            "Training loss epoch: 0.2482099855423591\n",
            "Training accuracy epoch: 1995.5625\n",
            "Number of batches: 2148\n",
            "Number of Sentences Trained on: 34368\n",
            "Training loss epoch: 0.24810684764200433\n",
            "Training accuracy epoch: 1996.5625\n",
            "Number of batches: 2149\n",
            "Number of Sentences Trained on: 34384\n",
            "Training loss epoch: 0.24809621996961015\n",
            "Training accuracy epoch: 1997.5\n",
            "Number of batches: 2150\n",
            "Number of Sentences Trained on: 34400\n",
            "Training loss epoch: 0.24799403175841522\n",
            "Training accuracy epoch: 1998.5\n",
            "Number of batches: 2151\n",
            "Number of Sentences Trained on: 34416\n",
            "Training loss epoch: 0.24791892664290646\n",
            "Training accuracy epoch: 1999.5\n",
            "Number of batches: 2152\n",
            "Number of Sentences Trained on: 34432\n",
            "Training loss epoch: 0.24782699147517037\n",
            "Training accuracy epoch: 2000.5\n",
            "Number of batches: 2153\n",
            "Number of Sentences Trained on: 34448\n",
            "Training loss epoch: 0.24801416998424722\n",
            "Training accuracy epoch: 2001.375\n",
            "Number of batches: 2154\n",
            "Number of Sentences Trained on: 34464\n",
            "Training loss epoch: 0.24791795038974687\n",
            "Training accuracy epoch: 2002.375\n",
            "Number of batches: 2155\n",
            "Number of Sentences Trained on: 34480\n",
            "Training loss epoch: 0.2478177496426478\n",
            "Training accuracy epoch: 2003.375\n",
            "Number of batches: 2156\n",
            "Number of Sentences Trained on: 34496\n",
            "Training loss epoch: 0.24773074861090866\n",
            "Training accuracy epoch: 2004.375\n",
            "Number of batches: 2157\n",
            "Number of Sentences Trained on: 34512\n",
            "Training loss epoch: 0.2476490497484672\n",
            "Training accuracy epoch: 2005.375\n",
            "Number of batches: 2158\n",
            "Number of Sentences Trained on: 34528\n",
            "Training loss epoch: 0.247600863531539\n",
            "Training accuracy epoch: 2006.3125\n",
            "Number of batches: 2159\n",
            "Number of Sentences Trained on: 34544\n",
            "Training loss epoch: 0.24757652634332233\n",
            "Training accuracy epoch: 2007.25\n",
            "Number of batches: 2160\n",
            "Number of Sentences Trained on: 34560\n",
            "Training loss epoch: 0.24746766685060415\n",
            "Training accuracy epoch: 2008.25\n",
            "Number of batches: 2161\n",
            "Number of Sentences Trained on: 34576\n",
            "Training loss epoch: 0.2473620476276992\n",
            "Training accuracy epoch: 2009.25\n",
            "Number of batches: 2162\n",
            "Number of Sentences Trained on: 34592\n",
            "Training loss epoch: 0.24742602464371377\n",
            "Training accuracy epoch: 2010.0625\n",
            "Number of batches: 2163\n",
            "Number of Sentences Trained on: 34608\n",
            "Training loss epoch: 0.24754303853611107\n",
            "Training accuracy epoch: 2011.0\n",
            "Number of batches: 2164\n",
            "Number of Sentences Trained on: 34624\n",
            "Training loss epoch: 0.24745325701565032\n",
            "Training accuracy epoch: 2012.0\n",
            "Number of batches: 2165\n",
            "Number of Sentences Trained on: 34640\n",
            "Training loss epoch: 0.24737911397678947\n",
            "Training accuracy epoch: 2013.0\n",
            "Number of batches: 2166\n",
            "Number of Sentences Trained on: 34656\n",
            "Training loss epoch: 0.24727107442478788\n",
            "Training accuracy epoch: 2014.0\n",
            "Number of batches: 2167\n",
            "Number of Sentences Trained on: 34672\n",
            "Training loss epoch: 0.24717684646181734\n",
            "Training accuracy epoch: 2015.0\n",
            "Number of batches: 2168\n",
            "Number of Sentences Trained on: 34688\n",
            "Training loss epoch: 0.24712122986398022\n",
            "Training accuracy epoch: 2015.9375\n",
            "Number of batches: 2169\n",
            "Number of Sentences Trained on: 34704\n",
            "Training loss epoch: 0.24702513331916476\n",
            "Training accuracy epoch: 2016.9375\n",
            "Number of batches: 2170\n",
            "Number of Sentences Trained on: 34720\n",
            "Training loss epoch: 0.24695747175465416\n",
            "Training accuracy epoch: 2017.875\n",
            "Number of batches: 2171\n",
            "Number of Sentences Trained on: 34736\n",
            "Training loss epoch: 0.24694936393552613\n",
            "Training accuracy epoch: 2018.75\n",
            "Number of batches: 2172\n",
            "Number of Sentences Trained on: 34752\n",
            "Training loss epoch: 0.24697965629367966\n",
            "Training accuracy epoch: 2019.625\n",
            "Number of batches: 2173\n",
            "Number of Sentences Trained on: 34768\n",
            "Training loss epoch: 0.2469812698264734\n",
            "Training accuracy epoch: 2020.5625\n",
            "Number of batches: 2174\n",
            "Number of Sentences Trained on: 34784\n",
            "Training loss epoch: 0.246880073134584\n",
            "Training accuracy epoch: 2021.5625\n",
            "Number of batches: 2175\n",
            "Number of Sentences Trained on: 34800\n",
            "Training loss epoch: 0.24677490760651005\n",
            "Training accuracy epoch: 2022.5625\n",
            "Number of batches: 2176\n",
            "Number of Sentences Trained on: 34816\n",
            "Training loss epoch: 0.24678786374717526\n",
            "Training accuracy epoch: 2023.375\n",
            "Number of batches: 2177\n",
            "Number of Sentences Trained on: 34832\n",
            "Training loss epoch: 0.24670780574538573\n",
            "Training accuracy epoch: 2024.375\n",
            "Number of batches: 2178\n",
            "Number of Sentences Trained on: 34848\n",
            "Training loss epoch: 0.24661464108809822\n",
            "Training accuracy epoch: 2025.375\n",
            "Number of batches: 2179\n",
            "Number of Sentences Trained on: 34864\n",
            "Training loss epoch: 0.24658919551384148\n",
            "Training accuracy epoch: 2026.375\n",
            "Number of batches: 2180\n",
            "Number of Sentences Trained on: 34880\n",
            "Training loss epoch: 0.2464990662978787\n",
            "Training accuracy epoch: 2027.375\n",
            "Number of batches: 2181\n",
            "Number of Sentences Trained on: 34896\n",
            "Training loss epoch: 0.24643176747870132\n",
            "Training accuracy epoch: 2028.375\n",
            "Number of batches: 2182\n",
            "Number of Sentences Trained on: 34912\n",
            "Training loss epoch: 0.24648723642687786\n",
            "Training accuracy epoch: 2029.25\n",
            "Number of batches: 2183\n",
            "Number of Sentences Trained on: 34928\n",
            "Training loss epoch: 0.2464648416025339\n",
            "Training accuracy epoch: 2030.125\n",
            "Number of batches: 2184\n",
            "Number of Sentences Trained on: 34944\n",
            "Training loss epoch: 0.2463598033093615\n",
            "Training accuracy epoch: 2031.125\n",
            "Number of batches: 2185\n",
            "Number of Sentences Trained on: 34960\n",
            "Training loss epoch: 0.24663272079963097\n",
            "Training accuracy epoch: 2031.875\n",
            "Number of batches: 2186\n",
            "Number of Sentences Trained on: 34976\n",
            "Training loss epoch: 0.2465435576782305\n",
            "Training accuracy epoch: 2032.875\n",
            "Number of batches: 2187\n",
            "Number of Sentences Trained on: 34992\n",
            "Training loss epoch: 0.24653525756743533\n",
            "Training accuracy epoch: 2033.8125\n",
            "Number of batches: 2188\n",
            "Number of Sentences Trained on: 35008\n",
            "Training loss epoch: 0.2464376740062302\n",
            "Training accuracy epoch: 2034.8125\n",
            "Number of batches: 2189\n",
            "Number of Sentences Trained on: 35024\n",
            "Training loss epoch: 0.24633251805130613\n",
            "Training accuracy epoch: 2035.8125\n",
            "Number of batches: 2190\n",
            "Number of Sentences Trained on: 35040\n",
            "Training loss epoch: 0.24632841005448688\n",
            "Training accuracy epoch: 2036.75\n",
            "Number of batches: 2191\n",
            "Number of Sentences Trained on: 35056\n",
            "Training loss epoch: 0.24625071667620818\n",
            "Training accuracy epoch: 2037.75\n",
            "Number of batches: 2192\n",
            "Number of Sentences Trained on: 35072\n",
            "Training loss epoch: 0.24648525659207252\n",
            "Training accuracy epoch: 2038.4375\n",
            "Number of batches: 2193\n",
            "Number of Sentences Trained on: 35088\n",
            "Training loss epoch: 0.24643251716788372\n",
            "Training accuracy epoch: 2039.375\n",
            "Number of batches: 2194\n",
            "Number of Sentences Trained on: 35104\n",
            "Training loss epoch: 0.2464012403821823\n",
            "Training accuracy epoch: 2040.3125\n",
            "Number of batches: 2195\n",
            "Number of Sentences Trained on: 35120\n",
            "Training loss epoch: 0.24646692606238801\n",
            "Training accuracy epoch: 2041.1875\n",
            "Number of batches: 2196\n",
            "Number of Sentences Trained on: 35136\n",
            "Training loss epoch: 0.24644761470400398\n",
            "Training accuracy epoch: 2042.0625\n",
            "Number of batches: 2197\n",
            "Number of Sentences Trained on: 35152\n",
            "Training loss epoch: 0.24645366148770642\n",
            "Training accuracy epoch: 2042.9375\n",
            "Number of batches: 2198\n",
            "Number of Sentences Trained on: 35168\n",
            "Training loss epoch: 0.24652205789042447\n",
            "Training accuracy epoch: 2043.8125\n",
            "Number of batches: 2199\n",
            "Number of Sentences Trained on: 35184\n",
            "Training loss epoch: 0.24649640918370674\n",
            "Training accuracy epoch: 2044.6875\n",
            "Number of batches: 2200\n",
            "Number of Sentences Trained on: 35200\n",
            "Training loss per 100 training steps: 0.2465043230069423\n",
            "Training loss epoch: 0.2465043230069423\n",
            "Training accuracy epoch: 2045.5625\n",
            "Number of batches: 2201\n",
            "Number of Sentences Trained on: 35216\n",
            "Training loss epoch: 0.24641253839718089\n",
            "Training accuracy epoch: 2046.5625\n",
            "Number of batches: 2202\n",
            "Number of Sentences Trained on: 35232\n",
            "Training loss epoch: 0.24642207713216827\n",
            "Training accuracy epoch: 2047.5\n",
            "Number of batches: 2203\n",
            "Number of Sentences Trained on: 35248\n",
            "Training loss epoch: 0.24633543555136303\n",
            "Training accuracy epoch: 2048.5\n",
            "Number of batches: 2204\n",
            "Number of Sentences Trained on: 35264\n",
            "Training loss epoch: 0.2463120453963542\n",
            "Training accuracy epoch: 2049.4375\n",
            "Number of batches: 2205\n",
            "Number of Sentences Trained on: 35280\n",
            "Training loss epoch: 0.2462777798617105\n",
            "Training accuracy epoch: 2050.3125\n",
            "Number of batches: 2206\n",
            "Number of Sentences Trained on: 35296\n",
            "Training loss epoch: 0.24622447649713253\n",
            "Training accuracy epoch: 2051.25\n",
            "Number of batches: 2207\n",
            "Number of Sentences Trained on: 35312\n",
            "Training loss epoch: 0.24620867129904556\n",
            "Training accuracy epoch: 2052.1875\n",
            "Number of batches: 2208\n",
            "Number of Sentences Trained on: 35328\n",
            "Training loss epoch: 0.2461252145733956\n",
            "Training accuracy epoch: 2053.1875\n",
            "Number of batches: 2209\n",
            "Number of Sentences Trained on: 35344\n",
            "Training loss epoch: 0.24603997685976023\n",
            "Training accuracy epoch: 2054.1875\n",
            "Number of batches: 2210\n",
            "Number of Sentences Trained on: 35360\n",
            "Training loss epoch: 0.24617206145790652\n",
            "Training accuracy epoch: 2055.0625\n",
            "Number of batches: 2211\n",
            "Number of Sentences Trained on: 35376\n",
            "Training loss epoch: 0.24610627963246398\n",
            "Training accuracy epoch: 2056.0625\n",
            "Number of batches: 2212\n",
            "Number of Sentences Trained on: 35392\n",
            "Training loss epoch: 0.2460302848964795\n",
            "Training accuracy epoch: 2057.0\n",
            "Number of batches: 2213\n",
            "Number of Sentences Trained on: 35408\n",
            "Training loss epoch: 0.24595558856947933\n",
            "Training accuracy epoch: 2058.0\n",
            "Number of batches: 2214\n",
            "Number of Sentences Trained on: 35424\n",
            "Training loss epoch: 0.24616357387709967\n",
            "Training accuracy epoch: 2058.8125\n",
            "Number of batches: 2215\n",
            "Number of Sentences Trained on: 35440\n",
            "Training loss epoch: 0.24606043397573357\n",
            "Training accuracy epoch: 2059.8125\n",
            "Number of batches: 2216\n",
            "Number of Sentences Trained on: 35456\n",
            "Training loss epoch: 0.24614220551128155\n",
            "Training accuracy epoch: 2060.6875\n",
            "Number of batches: 2217\n",
            "Number of Sentences Trained on: 35472\n",
            "Training loss epoch: 0.24610408241008186\n",
            "Training accuracy epoch: 2061.625\n",
            "Number of batches: 2218\n",
            "Number of Sentences Trained on: 35488\n",
            "Training loss epoch: 0.24614759387197446\n",
            "Training accuracy epoch: 2062.5\n",
            "Number of batches: 2219\n",
            "Number of Sentences Trained on: 35504\n",
            "Training loss epoch: 0.24604527952300537\n",
            "Training accuracy epoch: 2063.5\n",
            "Number of batches: 2220\n",
            "Number of Sentences Trained on: 35520\n",
            "Training loss epoch: 0.2459881879936087\n",
            "Training accuracy epoch: 2064.4375\n",
            "Number of batches: 2221\n",
            "Number of Sentences Trained on: 35536\n",
            "Training loss epoch: 0.2459008253383191\n",
            "Training accuracy epoch: 2065.4375\n",
            "Number of batches: 2222\n",
            "Number of Sentences Trained on: 35552\n",
            "Training loss epoch: 0.24581795192675124\n",
            "Training accuracy epoch: 2066.4375\n",
            "Number of batches: 2223\n",
            "Number of Sentences Trained on: 35568\n",
            "Training loss epoch: 0.24588127926548126\n",
            "Training accuracy epoch: 2067.25\n",
            "Number of batches: 2224\n",
            "Number of Sentences Trained on: 35584\n",
            "Training loss epoch: 0.2459318563827638\n",
            "Training accuracy epoch: 2068.125\n",
            "Number of batches: 2225\n",
            "Number of Sentences Trained on: 35600\n",
            "Training loss epoch: 0.24600497911527738\n",
            "Training accuracy epoch: 2069.0\n",
            "Number of batches: 2226\n",
            "Number of Sentences Trained on: 35616\n",
            "Training loss epoch: 0.24592017656050832\n",
            "Training accuracy epoch: 2070.0\n",
            "Number of batches: 2227\n",
            "Number of Sentences Trained on: 35632\n",
            "Training loss epoch: 0.2458488846689302\n",
            "Training accuracy epoch: 2071.0\n",
            "Number of batches: 2228\n",
            "Number of Sentences Trained on: 35648\n",
            "Training loss epoch: 0.24582848614002353\n",
            "Training accuracy epoch: 2071.9375\n",
            "Number of batches: 2229\n",
            "Number of Sentences Trained on: 35664\n",
            "Training loss epoch: 0.24573853221709419\n",
            "Training accuracy epoch: 2072.9375\n",
            "Number of batches: 2230\n",
            "Number of Sentences Trained on: 35680\n",
            "Training loss epoch: 0.24566818855231998\n",
            "Training accuracy epoch: 2073.875\n",
            "Number of batches: 2231\n",
            "Number of Sentences Trained on: 35696\n",
            "Training loss epoch: 0.2455654127830406\n",
            "Training accuracy epoch: 2074.875\n",
            "Number of batches: 2232\n",
            "Number of Sentences Trained on: 35712\n",
            "Training loss epoch: 0.24546494992409007\n",
            "Training accuracy epoch: 2075.875\n",
            "Number of batches: 2233\n",
            "Number of Sentences Trained on: 35728\n",
            "Training loss epoch: 0.2454063886978339\n",
            "Training accuracy epoch: 2076.875\n",
            "Number of batches: 2234\n",
            "Number of Sentences Trained on: 35744\n",
            "Training loss epoch: 0.24555764157800067\n",
            "Training accuracy epoch: 2077.75\n",
            "Number of batches: 2235\n",
            "Number of Sentences Trained on: 35760\n",
            "Training loss epoch: 0.24546487236058936\n",
            "Training accuracy epoch: 2078.75\n",
            "Number of batches: 2236\n",
            "Number of Sentences Trained on: 35776\n",
            "Training loss epoch: 0.2454394576359152\n",
            "Training accuracy epoch: 2079.6875\n",
            "Number of batches: 2237\n",
            "Number of Sentences Trained on: 35792\n",
            "Training loss epoch: 0.2453923738737182\n",
            "Training accuracy epoch: 2080.625\n",
            "Number of batches: 2238\n",
            "Number of Sentences Trained on: 35808\n",
            "Training loss epoch: 0.2455104511845133\n",
            "Training accuracy epoch: 2081.4375\n",
            "Number of batches: 2239\n",
            "Number of Sentences Trained on: 35824\n",
            "Training loss epoch: 0.24541234922861413\n",
            "Training accuracy epoch: 2082.4375\n",
            "Number of batches: 2240\n",
            "Number of Sentences Trained on: 35840\n",
            "Training loss epoch: 0.24537963762260984\n",
            "Training accuracy epoch: 2083.375\n",
            "Number of batches: 2241\n",
            "Number of Sentences Trained on: 35856\n",
            "Training loss epoch: 0.24542723575492292\n",
            "Training accuracy epoch: 2084.25\n",
            "Number of batches: 2242\n",
            "Number of Sentences Trained on: 35872\n",
            "Training loss epoch: 0.2453812027928875\n",
            "Training accuracy epoch: 2085.1875\n",
            "Number of batches: 2243\n",
            "Number of Sentences Trained on: 35888\n",
            "Training loss epoch: 0.24542762547625152\n",
            "Training accuracy epoch: 2086.0625\n",
            "Number of batches: 2244\n",
            "Number of Sentences Trained on: 35904\n",
            "Training loss epoch: 0.24533301140689903\n",
            "Training accuracy epoch: 2087.0625\n",
            "Number of batches: 2245\n",
            "Number of Sentences Trained on: 35920\n",
            "Training loss epoch: 0.24533788812241433\n",
            "Training accuracy epoch: 2088.0\n",
            "Number of batches: 2246\n",
            "Number of Sentences Trained on: 35936\n",
            "Training loss epoch: 0.24525428747880124\n",
            "Training accuracy epoch: 2089.0\n",
            "Number of batches: 2247\n",
            "Number of Sentences Trained on: 35952\n",
            "Training loss epoch: 0.24519069321301112\n",
            "Training accuracy epoch: 2089.9375\n",
            "Number of batches: 2248\n",
            "Number of Sentences Trained on: 35968\n",
            "Training loss epoch: 0.24516290401904886\n",
            "Training accuracy epoch: 2090.875\n",
            "Number of batches: 2249\n",
            "Number of Sentences Trained on: 35984\n",
            "Training loss epoch: 0.24507063256535266\n",
            "Training accuracy epoch: 2091.875\n",
            "Number of batches: 2250\n",
            "Number of Sentences Trained on: 36000\n",
            "Training loss epoch: 0.24496533599963907\n",
            "Training accuracy epoch: 2092.875\n",
            "Number of batches: 2251\n",
            "Number of Sentences Trained on: 36016\n",
            "Training loss epoch: 0.24487869063934864\n",
            "Training accuracy epoch: 2093.875\n",
            "Number of batches: 2252\n",
            "Number of Sentences Trained on: 36032\n",
            "Training loss epoch: 0.2448727240520573\n",
            "Training accuracy epoch: 2094.75\n",
            "Number of batches: 2253\n",
            "Number of Sentences Trained on: 36048\n",
            "Training loss epoch: 0.2449611776335679\n",
            "Training accuracy epoch: 2095.625\n",
            "Number of batches: 2254\n",
            "Number of Sentences Trained on: 36064\n",
            "Training loss epoch: 0.2449844392623645\n",
            "Training accuracy epoch: 2096.5625\n",
            "Number of batches: 2255\n",
            "Number of Sentences Trained on: 36080\n",
            "Training loss epoch: 0.24513732798063984\n",
            "Training accuracy epoch: 2097.375\n",
            "Number of batches: 2256\n",
            "Number of Sentences Trained on: 36096\n",
            "Training loss epoch: 0.24507045195803978\n",
            "Training accuracy epoch: 2098.3125\n",
            "Number of batches: 2257\n",
            "Number of Sentences Trained on: 36112\n",
            "Training loss epoch: 0.24501778765361298\n",
            "Training accuracy epoch: 2099.3125\n",
            "Number of batches: 2258\n",
            "Number of Sentences Trained on: 36128\n",
            "Training loss epoch: 0.24495845048178372\n",
            "Training accuracy epoch: 2100.3125\n",
            "Number of batches: 2259\n",
            "Number of Sentences Trained on: 36144\n",
            "Training loss epoch: 0.24485801624101217\n",
            "Training accuracy epoch: 2101.3125\n",
            "Number of batches: 2260\n",
            "Number of Sentences Trained on: 36160\n",
            "Training loss epoch: 0.24476561023022078\n",
            "Training accuracy epoch: 2102.3125\n",
            "Number of batches: 2261\n",
            "Number of Sentences Trained on: 36176\n",
            "Training loss epoch: 0.24467387014076075\n",
            "Training accuracy epoch: 2103.3125\n",
            "Number of batches: 2262\n",
            "Number of Sentences Trained on: 36192\n",
            "Training loss epoch: 0.24467730911887303\n",
            "Training accuracy epoch: 2104.25\n",
            "Number of batches: 2263\n",
            "Number of Sentences Trained on: 36208\n",
            "Training loss epoch: 0.24462012425904217\n",
            "Training accuracy epoch: 2105.25\n",
            "Number of batches: 2264\n",
            "Number of Sentences Trained on: 36224\n",
            "Training loss epoch: 0.24457295540202137\n",
            "Training accuracy epoch: 2106.1875\n",
            "Number of batches: 2265\n",
            "Number of Sentences Trained on: 36240\n",
            "Training loss epoch: 0.24448507794303143\n",
            "Training accuracy epoch: 2107.1875\n",
            "Number of batches: 2266\n",
            "Number of Sentences Trained on: 36256\n",
            "Training loss epoch: 0.24447244239694152\n",
            "Training accuracy epoch: 2108.125\n",
            "Number of batches: 2267\n",
            "Number of Sentences Trained on: 36272\n",
            "Training loss epoch: 0.24441803038379792\n",
            "Training accuracy epoch: 2109.125\n",
            "Number of batches: 2268\n",
            "Number of Sentences Trained on: 36288\n",
            "Training loss epoch: 0.2444696324130823\n",
            "Training accuracy epoch: 2110.0625\n",
            "Number of batches: 2269\n",
            "Number of Sentences Trained on: 36304\n",
            "Training loss epoch: 0.244374426958943\n",
            "Training accuracy epoch: 2111.0625\n",
            "Number of batches: 2270\n",
            "Number of Sentences Trained on: 36320\n",
            "Training loss epoch: 0.24429416950859328\n",
            "Training accuracy epoch: 2112.0625\n",
            "Number of batches: 2271\n",
            "Number of Sentences Trained on: 36336\n",
            "Training loss epoch: 0.24422790744738707\n",
            "Training accuracy epoch: 2113.0625\n",
            "Number of batches: 2272\n",
            "Number of Sentences Trained on: 36352\n",
            "Training loss epoch: 0.24428080211549644\n",
            "Training accuracy epoch: 2114.0\n",
            "Number of batches: 2273\n",
            "Number of Sentences Trained on: 36368\n",
            "Training loss epoch: 0.24419298068445794\n",
            "Training accuracy epoch: 2115.0\n",
            "Number of batches: 2274\n",
            "Number of Sentences Trained on: 36384\n",
            "Training loss epoch: 0.24418669074691915\n",
            "Training accuracy epoch: 2115.875\n",
            "Number of batches: 2275\n",
            "Number of Sentences Trained on: 36400\n",
            "Training loss epoch: 0.24416045778674877\n",
            "Training accuracy epoch: 2116.8125\n",
            "Number of batches: 2276\n",
            "Number of Sentences Trained on: 36416\n",
            "Training loss epoch: 0.24406481131692045\n",
            "Training accuracy epoch: 2117.8125\n",
            "Number of batches: 2277\n",
            "Number of Sentences Trained on: 36432\n",
            "Training loss epoch: 0.24402038964563105\n",
            "Training accuracy epoch: 2118.75\n",
            "Number of batches: 2278\n",
            "Number of Sentences Trained on: 36448\n",
            "Training loss epoch: 0.24393302322123828\n",
            "Training accuracy epoch: 2119.75\n",
            "Number of batches: 2279\n",
            "Number of Sentences Trained on: 36464\n",
            "Training loss epoch: 0.243877613027335\n",
            "Training accuracy epoch: 2120.6875\n",
            "Number of batches: 2280\n",
            "Number of Sentences Trained on: 36480\n",
            "Training loss epoch: 0.24389427864302646\n",
            "Training accuracy epoch: 2121.5625\n",
            "Number of batches: 2281\n",
            "Number of Sentences Trained on: 36496\n",
            "Training loss epoch: 0.24381669926049807\n",
            "Training accuracy epoch: 2122.5625\n",
            "Number of batches: 2282\n",
            "Number of Sentences Trained on: 36512\n",
            "Training loss epoch: 0.24375640729746562\n",
            "Training accuracy epoch: 2123.5\n",
            "Number of batches: 2283\n",
            "Number of Sentences Trained on: 36528\n",
            "Training loss epoch: 0.2438380913686614\n",
            "Training accuracy epoch: 2124.4375\n",
            "Number of batches: 2284\n",
            "Number of Sentences Trained on: 36544\n",
            "Training loss epoch: 0.2437795802910101\n",
            "Training accuracy epoch: 2125.375\n",
            "Number of batches: 2285\n",
            "Number of Sentences Trained on: 36560\n",
            "Training loss epoch: 0.24376941490462106\n",
            "Training accuracy epoch: 2126.3125\n",
            "Number of batches: 2286\n",
            "Number of Sentences Trained on: 36576\n",
            "Training loss epoch: 0.24366659627413717\n",
            "Training accuracy epoch: 2127.3125\n",
            "Number of batches: 2287\n",
            "Number of Sentences Trained on: 36592\n",
            "Training loss epoch: 0.24363769700594404\n",
            "Training accuracy epoch: 2128.25\n",
            "Number of batches: 2288\n",
            "Number of Sentences Trained on: 36608\n",
            "Training loss epoch: 0.24359706677935852\n",
            "Training accuracy epoch: 2129.1875\n",
            "Number of batches: 2289\n",
            "Number of Sentences Trained on: 36624\n",
            "Training loss epoch: 0.243605835465755\n",
            "Training accuracy epoch: 2130.125\n",
            "Number of batches: 2290\n",
            "Number of Sentences Trained on: 36640\n",
            "Training loss epoch: 0.24351507105951173\n",
            "Training accuracy epoch: 2131.125\n",
            "Number of batches: 2291\n",
            "Number of Sentences Trained on: 36656\n",
            "Training loss epoch: 0.24348858865481812\n",
            "Training accuracy epoch: 2132.0625\n",
            "Number of batches: 2292\n",
            "Number of Sentences Trained on: 36672\n",
            "Training loss epoch: 0.24340607296079203\n",
            "Training accuracy epoch: 2133.0625\n",
            "Number of batches: 2293\n",
            "Number of Sentences Trained on: 36688\n",
            "Training loss epoch: 0.24342846181227418\n",
            "Training accuracy epoch: 2134.0\n",
            "Number of batches: 2294\n",
            "Number of Sentences Trained on: 36704\n",
            "Training loss epoch: 0.24361455829444795\n",
            "Training accuracy epoch: 2134.875\n",
            "Number of batches: 2295\n",
            "Number of Sentences Trained on: 36720\n",
            "Training loss epoch: 0.24352110891914455\n",
            "Training accuracy epoch: 2135.875\n",
            "Number of batches: 2296\n",
            "Number of Sentences Trained on: 36736\n",
            "Training loss epoch: 0.2435578550595237\n",
            "Training accuracy epoch: 2136.75\n",
            "Number of batches: 2297\n",
            "Number of Sentences Trained on: 36752\n",
            "Training loss epoch: 0.24346413621249358\n",
            "Training accuracy epoch: 2137.75\n",
            "Number of batches: 2298\n",
            "Number of Sentences Trained on: 36768\n",
            "Training loss epoch: 0.24349229283737794\n",
            "Training accuracy epoch: 2138.625\n",
            "Number of batches: 2299\n",
            "Number of Sentences Trained on: 36784\n",
            "Training loss epoch: 0.24343074474645698\n",
            "Training accuracy epoch: 2139.5625\n",
            "Number of batches: 2300\n",
            "Number of Sentences Trained on: 36800\n",
            "Training loss per 100 training steps: 0.24346309457328827\n",
            "Training loss epoch: 0.24346309457328827\n",
            "Training accuracy epoch: 2140.4375\n",
            "Number of batches: 2301\n",
            "Number of Sentences Trained on: 36816\n",
            "Training loss epoch: 0.24337967687798623\n",
            "Training accuracy epoch: 2141.4375\n",
            "Number of batches: 2302\n",
            "Number of Sentences Trained on: 36832\n",
            "Training loss epoch: 0.24336887886355454\n",
            "Training accuracy epoch: 2142.3125\n",
            "Number of batches: 2303\n",
            "Number of Sentences Trained on: 36848\n",
            "Training loss epoch: 0.2433374942176872\n",
            "Training accuracy epoch: 2143.1875\n",
            "Number of batches: 2304\n",
            "Number of Sentences Trained on: 36864\n",
            "Training loss epoch: 0.24326296402808126\n",
            "Training accuracy epoch: 2144.1875\n",
            "Number of batches: 2305\n",
            "Number of Sentences Trained on: 36880\n",
            "Training loss epoch: 0.24331774171005616\n",
            "Training accuracy epoch: 2145.0625\n",
            "Number of batches: 2306\n",
            "Number of Sentences Trained on: 36896\n",
            "Training loss epoch: 0.2432526216543904\n",
            "Training accuracy epoch: 2146.0625\n",
            "Number of batches: 2307\n",
            "Number of Sentences Trained on: 36912\n",
            "Training loss epoch: 0.2432067643502401\n",
            "Training accuracy epoch: 2146.9375\n",
            "Number of batches: 2308\n",
            "Number of Sentences Trained on: 36928\n",
            "Training loss epoch: 0.24311315014251722\n",
            "Training accuracy epoch: 2147.9375\n",
            "Number of batches: 2309\n",
            "Number of Sentences Trained on: 36944\n",
            "Training loss epoch: 0.2430994530381772\n",
            "Training accuracy epoch: 2148.875\n",
            "Number of batches: 2310\n",
            "Number of Sentences Trained on: 36960\n",
            "Training loss epoch: 0.24310119255284296\n",
            "Training accuracy epoch: 2149.75\n",
            "Number of batches: 2311\n",
            "Number of Sentences Trained on: 36976\n",
            "Training loss epoch: 0.24305960979348107\n",
            "Training accuracy epoch: 2150.6875\n",
            "Number of batches: 2312\n",
            "Number of Sentences Trained on: 36992\n",
            "Training loss epoch: 0.24295836312424954\n",
            "Training accuracy epoch: 2151.6875\n",
            "Number of batches: 2313\n",
            "Number of Sentences Trained on: 37008\n",
            "Training loss epoch: 0.24286432281843257\n",
            "Training accuracy epoch: 2152.6875\n",
            "Number of batches: 2314\n",
            "Number of Sentences Trained on: 37024\n",
            "Training loss epoch: 0.24288107507840023\n",
            "Training accuracy epoch: 2153.5625\n",
            "Number of batches: 2315\n",
            "Number of Sentences Trained on: 37040\n",
            "Training loss epoch: 0.24290606537423642\n",
            "Training accuracy epoch: 2154.5\n",
            "Number of batches: 2316\n",
            "Number of Sentences Trained on: 37056\n",
            "Training loss epoch: 0.24282783657162868\n",
            "Training accuracy epoch: 2155.5\n",
            "Number of batches: 2317\n",
            "Number of Sentences Trained on: 37072\n",
            "Training loss epoch: 0.24275269907175195\n",
            "Training accuracy epoch: 2156.5\n",
            "Number of batches: 2318\n",
            "Number of Sentences Trained on: 37088\n",
            "Training loss epoch: 0.2426927100626873\n",
            "Training accuracy epoch: 2157.4375\n",
            "Number of batches: 2319\n",
            "Number of Sentences Trained on: 37104\n",
            "Training loss epoch: 0.24262107086713136\n",
            "Training accuracy epoch: 2158.375\n",
            "Number of batches: 2320\n",
            "Number of Sentences Trained on: 37120\n",
            "Training loss epoch: 0.24265198837901525\n",
            "Training accuracy epoch: 2159.3125\n",
            "Number of batches: 2321\n",
            "Number of Sentences Trained on: 37136\n",
            "Training loss epoch: 0.2426652373897648\n",
            "Training accuracy epoch: 2160.1875\n",
            "Number of batches: 2322\n",
            "Number of Sentences Trained on: 37152\n",
            "Training loss epoch: 0.24265141386917208\n",
            "Training accuracy epoch: 2161.0625\n",
            "Number of batches: 2323\n",
            "Number of Sentences Trained on: 37168\n",
            "Training loss epoch: 0.24285731503215782\n",
            "Training accuracy epoch: 2161.875\n",
            "Number of batches: 2324\n",
            "Number of Sentences Trained on: 37184\n",
            "Training loss epoch: 0.24292556128314427\n",
            "Training accuracy epoch: 2162.8125\n",
            "Number of batches: 2325\n",
            "Number of Sentences Trained on: 37200\n",
            "Training loss epoch: 0.24310642823069226\n",
            "Training accuracy epoch: 2163.6875\n",
            "Number of batches: 2326\n",
            "Number of Sentences Trained on: 37216\n",
            "Training loss epoch: 0.24311439108446106\n",
            "Training accuracy epoch: 2164.625\n",
            "Number of batches: 2327\n",
            "Number of Sentences Trained on: 37232\n",
            "Training loss epoch: 0.2430468319616796\n",
            "Training accuracy epoch: 2165.625\n",
            "Number of batches: 2328\n",
            "Number of Sentences Trained on: 37248\n",
            "Training loss epoch: 0.24309911643954282\n",
            "Training accuracy epoch: 2166.5\n",
            "Number of batches: 2329\n",
            "Number of Sentences Trained on: 37264\n",
            "Training loss epoch: 0.24305340245549964\n",
            "Training accuracy epoch: 2167.5\n",
            "Number of batches: 2330\n",
            "Number of Sentences Trained on: 37280\n",
            "Training loss epoch: 0.2430582818427774\n",
            "Training accuracy epoch: 2168.4375\n",
            "Number of batches: 2331\n",
            "Number of Sentences Trained on: 37296\n",
            "Training loss epoch: 0.24301741275525818\n",
            "Training accuracy epoch: 2169.375\n",
            "Number of batches: 2332\n",
            "Number of Sentences Trained on: 37312\n",
            "Training loss epoch: 0.24292549984613446\n",
            "Training accuracy epoch: 2170.375\n",
            "Number of batches: 2333\n",
            "Number of Sentences Trained on: 37328\n",
            "Training loss epoch: 0.24289015921203047\n",
            "Training accuracy epoch: 2171.3125\n",
            "Number of batches: 2334\n",
            "Number of Sentences Trained on: 37344\n",
            "Training loss epoch: 0.2429008001684983\n",
            "Training accuracy epoch: 2172.125\n",
            "Number of batches: 2335\n",
            "Number of Sentences Trained on: 37360\n",
            "Training loss epoch: 0.242856093522477\n",
            "Training accuracy epoch: 2173.0625\n",
            "Number of batches: 2336\n",
            "Number of Sentences Trained on: 37376\n",
            "Training loss epoch: 0.24280874857023058\n",
            "Training accuracy epoch: 2174.0\n",
            "Number of batches: 2337\n",
            "Number of Sentences Trained on: 37392\n",
            "Training loss epoch: 0.24275372053677385\n",
            "Training accuracy epoch: 2175.0\n",
            "Number of batches: 2338\n",
            "Number of Sentences Trained on: 37408\n",
            "Training loss epoch: 0.2426737414930986\n",
            "Training accuracy epoch: 2176.0\n",
            "Number of batches: 2339\n",
            "Number of Sentences Trained on: 37424\n",
            "Training loss epoch: 0.24267347740431308\n",
            "Training accuracy epoch: 2176.9375\n",
            "Number of batches: 2340\n",
            "Number of Sentences Trained on: 37440\n",
            "Training loss epoch: 0.24260492380453844\n",
            "Training accuracy epoch: 2177.9375\n",
            "Number of batches: 2341\n",
            "Number of Sentences Trained on: 37456\n",
            "Training loss epoch: 0.24252676831424733\n",
            "Training accuracy epoch: 2178.9375\n",
            "Number of batches: 2342\n",
            "Number of Sentences Trained on: 37472\n",
            "Training loss epoch: 0.24249979888569567\n",
            "Training accuracy epoch: 2179.875\n",
            "Number of batches: 2343\n",
            "Number of Sentences Trained on: 37488\n",
            "Training loss epoch: 0.24252094700257054\n",
            "Training accuracy epoch: 2180.75\n",
            "Number of batches: 2344\n",
            "Number of Sentences Trained on: 37504\n",
            "Training loss epoch: 0.24247012701012624\n",
            "Training accuracy epoch: 2181.6875\n",
            "Number of batches: 2345\n",
            "Number of Sentences Trained on: 37520\n",
            "Training loss epoch: 0.24238912606189458\n",
            "Training accuracy epoch: 2182.6875\n",
            "Number of batches: 2346\n",
            "Number of Sentences Trained on: 37536\n",
            "Training loss epoch: 0.24231570428590485\n",
            "Training accuracy epoch: 2183.6875\n",
            "Number of batches: 2347\n",
            "Number of Sentences Trained on: 37552\n",
            "Training loss epoch: 0.24225597199349264\n",
            "Training accuracy epoch: 2184.625\n",
            "Number of batches: 2348\n",
            "Number of Sentences Trained on: 37568\n",
            "Training loss epoch: 0.2421643645441173\n",
            "Training accuracy epoch: 2185.625\n",
            "Number of batches: 2349\n",
            "Number of Sentences Trained on: 37584\n",
            "Training loss epoch: 0.24206733284637014\n",
            "Training accuracy epoch: 2186.625\n",
            "Number of batches: 2350\n",
            "Number of Sentences Trained on: 37600\n",
            "Training loss epoch: 0.24197834941211127\n",
            "Training accuracy epoch: 2187.625\n",
            "Number of batches: 2351\n",
            "Number of Sentences Trained on: 37616\n",
            "Training loss epoch: 0.2419302076747527\n",
            "Training accuracy epoch: 2188.5625\n",
            "Number of batches: 2352\n",
            "Number of Sentences Trained on: 37632\n",
            "Training loss epoch: 0.2418654728482948\n",
            "Training accuracy epoch: 2189.5625\n",
            "Number of batches: 2353\n",
            "Number of Sentences Trained on: 37648\n",
            "Training loss epoch: 0.2419125369244343\n",
            "Training accuracy epoch: 2190.4375\n",
            "Number of batches: 2354\n",
            "Number of Sentences Trained on: 37664\n",
            "Training loss epoch: 0.24183289193708427\n",
            "Training accuracy epoch: 2191.4375\n",
            "Number of batches: 2355\n",
            "Number of Sentences Trained on: 37680\n",
            "Training loss epoch: 0.24184529850313183\n",
            "Training accuracy epoch: 2192.375\n",
            "Number of batches: 2356\n",
            "Number of Sentences Trained on: 37696\n",
            "Training loss epoch: 0.24178937460798858\n",
            "Training accuracy epoch: 2193.375\n",
            "Number of batches: 2357\n",
            "Number of Sentences Trained on: 37712\n",
            "Training loss epoch: 0.2417921426961162\n",
            "Training accuracy epoch: 2194.3125\n",
            "Number of batches: 2358\n",
            "Number of Sentences Trained on: 37728\n",
            "Training loss epoch: 0.24169971985681651\n",
            "Training accuracy epoch: 2195.3125\n",
            "Number of batches: 2359\n",
            "Number of Sentences Trained on: 37744\n",
            "Training loss epoch: 0.24192607929472323\n",
            "Training accuracy epoch: 2196.125\n",
            "Number of batches: 2360\n",
            "Number of Sentences Trained on: 37760\n",
            "Training loss epoch: 0.24194334282735833\n",
            "Training accuracy epoch: 2197.0625\n",
            "Number of batches: 2361\n",
            "Number of Sentences Trained on: 37776\n",
            "Training loss epoch: 0.2419984520852225\n",
            "Training accuracy epoch: 2198.0\n",
            "Number of batches: 2362\n",
            "Number of Sentences Trained on: 37792\n",
            "Training loss epoch: 0.24190076230581767\n",
            "Training accuracy epoch: 2199.0\n",
            "Number of batches: 2363\n",
            "Number of Sentences Trained on: 37808\n",
            "Training loss epoch: 0.241844925528326\n",
            "Training accuracy epoch: 2199.9375\n",
            "Number of batches: 2364\n",
            "Number of Sentences Trained on: 37824\n",
            "Training loss epoch: 0.24175781419866937\n",
            "Training accuracy epoch: 2200.9375\n",
            "Number of batches: 2365\n",
            "Number of Sentences Trained on: 37840\n",
            "Training loss epoch: 0.2417637509052167\n",
            "Training accuracy epoch: 2201.875\n",
            "Number of batches: 2366\n",
            "Number of Sentences Trained on: 37856\n",
            "Training loss epoch: 0.24168233402045053\n",
            "Training accuracy epoch: 2202.875\n",
            "Number of batches: 2367\n",
            "Number of Sentences Trained on: 37872\n",
            "Training loss epoch: 0.24165207206592518\n",
            "Training accuracy epoch: 2203.8125\n",
            "Number of batches: 2368\n",
            "Number of Sentences Trained on: 37888\n",
            "Training loss epoch: 0.2415921785371396\n",
            "Training accuracy epoch: 2204.8125\n",
            "Number of batches: 2369\n",
            "Number of Sentences Trained on: 37904\n",
            "Training loss epoch: 0.24158814384558785\n",
            "Training accuracy epoch: 2205.75\n",
            "Number of batches: 2370\n",
            "Number of Sentences Trained on: 37920\n",
            "Training loss epoch: 0.2415210953173206\n",
            "Training accuracy epoch: 2206.6875\n",
            "Number of batches: 2371\n",
            "Number of Sentences Trained on: 37936\n",
            "Training loss epoch: 0.24154050236891383\n",
            "Training accuracy epoch: 2207.625\n",
            "Number of batches: 2372\n",
            "Number of Sentences Trained on: 37952\n",
            "Training loss epoch: 0.24151418617272297\n",
            "Training accuracy epoch: 2208.5625\n",
            "Number of batches: 2373\n",
            "Number of Sentences Trained on: 37968\n",
            "Training loss epoch: 0.24152484098833804\n",
            "Training accuracy epoch: 2209.4375\n",
            "Number of batches: 2374\n",
            "Number of Sentences Trained on: 37984\n",
            "Training loss epoch: 0.24145238186026874\n",
            "Training accuracy epoch: 2210.4375\n",
            "Number of batches: 2375\n",
            "Number of Sentences Trained on: 38000\n",
            "Training loss epoch: 0.2413988897015626\n",
            "Training accuracy epoch: 2211.375\n",
            "Number of batches: 2376\n",
            "Number of Sentences Trained on: 38016\n",
            "Training loss epoch: 0.24137298333849233\n",
            "Training accuracy epoch: 2212.3125\n",
            "Number of batches: 2377\n",
            "Number of Sentences Trained on: 38032\n",
            "Training loss epoch: 0.24137759724235563\n",
            "Training accuracy epoch: 2213.1875\n",
            "Number of batches: 2378\n",
            "Number of Sentences Trained on: 38048\n",
            "Training loss epoch: 0.24149700075218836\n",
            "Training accuracy epoch: 2213.9375\n",
            "Number of batches: 2379\n",
            "Number of Sentences Trained on: 38056\n"
          ]
        }
      ],
      "source": [
        "#train model\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    batch_loss,epoch_loss, batch_acc, epoch_acc = train(model, epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E1KOqFIKPiC"
      },
      "outputs": [],
      "source": [
        "#Plot training loss per batch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metric(metric, title):\n",
        "    x0 = list(range(1, len(metric) + 1))\n",
        "    plt.figure(figsize =(10, 5))\n",
        "    plt.plot(x0, metric)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "qQ-JfrFuLq6P",
        "outputId": "f5ac1788-bd3b-44dc-d67e-b9d884514031"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_metric(batch_loss, 'Cumulative Average Loss for all Batches Trained')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "zZ-ZLaNMUpOP",
        "outputId": "e9cbc48c-1adb-40e0-f750-7afe6603e93b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_metric(batch_acc, 'Cumulative Average Accuracy for all Batches Trained')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRBUJOvUV8Ro"
      },
      "outputs": [],
      "source": [
        "#evaluate model\n",
        "def valid(eval_model, testing_loader):\n",
        "    model = eval_model\n",
        "\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    loss_per_epoch, acc_per_epoch = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            \n",
        "            loss = model(input_ids=ids, attention_mask=mask, labels=labels)[0]\n",
        "            eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)[1]\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(labels)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [id_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [id_to_labels[id.item()] for id in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "\n",
        "    loss_per_epoch.append(eval_loss)\n",
        "    acc_per_epoch.append(eval_accuracy)\n",
        "    \n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions, loss_per_epoch, acc_per_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb0xBKZcr5ol",
        "outputId": "b4c64dc2-b7ff-41ba-cd49-6e6bb7881e22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss per 100 evaluation steps: 0.16198831796646118\n",
            "Validation loss per 100 evaluation steps: 0.15873854809744967\n",
            "Validation loss per 100 evaluation steps: 0.1711404803232174\n",
            "Validation loss per 100 evaluation steps: 0.16799391192489263\n",
            "Validation loss per 100 evaluation steps: 0.17055855031900963\n",
            "Validation loss per 100 evaluation steps: 0.16958752136534366\n",
            "Validation Loss: 0.17052917142570834\n",
            "Validation Accuracy: 0.9466482047364401\n"
          ]
        }
      ],
      "source": [
        "labels, predictions, epoch_loss, epoch_accuracy = valid(model, testing_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jNJ9X1QK4XD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8EnMsnBdl6-",
        "outputId": "6a7a6c9f-3f88-49f9-b19f-31587c338bb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-geo       0.76      0.86      0.81       650\n",
            "       B-gpe       0.89      0.91      0.90       603\n",
            "       B-org       0.79      0.58      0.67       534\n",
            "       B-per       0.91      0.93      0.92       796\n",
            "       B-tim       0.93      0.80      0.86        99\n",
            "           O       0.99      0.99      0.99      6833\n",
            "\n",
            "    accuracy                           0.95      9515\n",
            "   macro avg       0.88      0.85      0.86      9515\n",
            "weighted avg       0.95      0.95      0.95      9515\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.classification_report(labels, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9P2hDRLsPyT"
      },
      "outputs": [],
      "source": [
        "def perform_NER(model, text):\n",
        "\n",
        "  \n",
        "  NER_model = model\n",
        "  sentence = text\n",
        "\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "\n",
        "  inputs = tokenizer(sentence.split(),\n",
        "                      return_offsets_mapping=True, \n",
        "                      padding='max_length', \n",
        "                      truncation=True, \n",
        "                      max_length=MAX_LEN,\n",
        "                      return_tensors=\"pt\")\n",
        "\n",
        "  # move to gpu\n",
        "  ids = inputs[\"input_ids\"].to(device)\n",
        "  mask = inputs[\"attention_mask\"].to(device)\n",
        "  # forward pass\n",
        "  outputs = NER_model(ids, attention_mask=mask)\n",
        "  logits = outputs[0]\n",
        "\n",
        "  active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "  flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
        "\n",
        "  #import pdb \n",
        "  #pdb.set_trace()\n",
        "\n",
        "  tokens = tokenizer.convert_ids_to_tokens(ids.reshape(-1).tolist())\n",
        "  token_predictions = [id_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
        "  wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
        "\n",
        "  prediction = []\n",
        "\n",
        "  for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].view(len(wp_preds),2).tolist()):\n",
        "\n",
        "    \n",
        "    #only predictions on first word pieces are important\n",
        "    if mapping[0] == 0 and mapping[1] != 0:\n",
        "      prediction.append(token_pred[1])\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  print(sentence.split())\n",
        "  print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNE67fe5YoNt",
        "outputId": "9488a808-464e-4655-cf20-5368096bd923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['John', 'Smith', 'is', 'a', 'software', 'engineer', 'at', 'Microsoft', 'Corporation.', 'He', 'lives', 'in', 'Seattle,', 'Washington', 'and', 'enjoys', 'hiking', 'in', 'the', 'nearby', 'mountains.', 'He', 'has', 'a', 'Bachelor’s', 'degree', 'in', 'Computer', 'Science', 'from', 'the', 'University', 'of', 'Washington.']\n",
            "['B-per', 'B-per', 'O', 'O', 'O', 'O', 'O', 'B-org', 'B-org', 'O', 'O', 'O', 'B-geo', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"John Smith is a software engineer at Microsoft Corporation. He lives in Seattle, \"\\\n",
        "            \"Washington and enjoys hiking in the nearby mountains. \"\\\n",
        "            \"He has a Bachelor’s degree in Computer Science from the University of Washington.\"\n",
        "\n",
        "perform_NER(model, sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxTwFFeGsbIp"
      },
      "outputs": [],
      "source": [
        "def save_model(m, dir):\n",
        "  \n",
        "  import os\n",
        "\n",
        "  model = m\n",
        "  directory = dir\n",
        "\n",
        "  if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "  # save vocabulary of the tokenizer\n",
        "  tokenizer.save_vocabulary(directory)\n",
        "  # save the model weights and its configuration file\n",
        "  model.save_pretrained(directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_b_UVumNeux"
      },
      "outputs": [],
      "source": [
        "save_model(model, \"model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_9B94MRD2YO"
      },
      "source": [
        "# 6. Train Model for 4 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2nuSIbFMoP-",
        "outputId": "c608f177-b018-43af-8e65-213943ddbfe7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training loss epoch: 0.07788566605672725\n",
            "Training accuracy epoch: 1103.4375\n",
            "Number of batches: 1133\n",
            "Number of Sentences Trained on: 18128\n",
            "Training loss epoch: 0.07790315446066687\n",
            "Training accuracy epoch: 1104.375\n",
            "Number of batches: 1134\n",
            "Number of Sentences Trained on: 18144\n",
            "Training loss epoch: 0.07784784879911863\n",
            "Training accuracy epoch: 1105.375\n",
            "Number of batches: 1135\n",
            "Number of Sentences Trained on: 18160\n",
            "Training loss epoch: 0.07784394625324098\n",
            "Training accuracy epoch: 1106.3125\n",
            "Number of batches: 1136\n",
            "Number of Sentences Trained on: 18176\n",
            "Training loss epoch: 0.07782224147278928\n",
            "Training accuracy epoch: 1107.3125\n",
            "Number of batches: 1137\n",
            "Number of Sentences Trained on: 18192\n",
            "Training loss epoch: 0.07778012476670589\n",
            "Training accuracy epoch: 1108.3125\n",
            "Number of batches: 1138\n",
            "Number of Sentences Trained on: 18208\n",
            "Training loss epoch: 0.07771570974227271\n",
            "Training accuracy epoch: 1109.3125\n",
            "Number of batches: 1139\n",
            "Number of Sentences Trained on: 18224\n",
            "Training loss epoch: 0.07780093872452996\n",
            "Training accuracy epoch: 1110.25\n",
            "Number of batches: 1140\n",
            "Number of Sentences Trained on: 18240\n",
            "Training loss epoch: 0.07781443289985332\n",
            "Training accuracy epoch: 1111.1875\n",
            "Number of batches: 1141\n",
            "Number of Sentences Trained on: 18256\n",
            "Training loss epoch: 0.07775782494684037\n",
            "Training accuracy epoch: 1112.1875\n",
            "Number of batches: 1142\n",
            "Number of Sentences Trained on: 18272\n",
            "Training loss epoch: 0.07769031748574876\n",
            "Training accuracy epoch: 1113.1875\n",
            "Number of batches: 1143\n",
            "Number of Sentences Trained on: 18288\n",
            "Training loss epoch: 0.07762368104580979\n",
            "Training accuracy epoch: 1114.1875\n",
            "Number of batches: 1144\n",
            "Number of Sentences Trained on: 18304\n",
            "Training loss epoch: 0.07777164824831596\n",
            "Training accuracy epoch: 1115.125\n",
            "Number of batches: 1145\n",
            "Number of Sentences Trained on: 18320\n",
            "Training loss epoch: 0.07781129559671854\n",
            "Training accuracy epoch: 1116.125\n",
            "Number of batches: 1146\n",
            "Number of Sentences Trained on: 18336\n",
            "Training loss epoch: 0.07782378641838969\n",
            "Training accuracy epoch: 1117.125\n",
            "Number of batches: 1147\n",
            "Number of Sentences Trained on: 18352\n",
            "Training loss epoch: 0.07817424563565276\n",
            "Training accuracy epoch: 1117.9375\n",
            "Number of batches: 1148\n",
            "Number of Sentences Trained on: 18368\n",
            "Training loss epoch: 0.0781160755258449\n",
            "Training accuracy epoch: 1118.9375\n",
            "Number of batches: 1149\n",
            "Number of Sentences Trained on: 18384\n",
            "Training loss epoch: 0.07807769984513035\n",
            "Training accuracy epoch: 1119.9375\n",
            "Number of batches: 1150\n",
            "Number of Sentences Trained on: 18400\n",
            "Training loss epoch: 0.07819309331445132\n",
            "Training accuracy epoch: 1120.875\n",
            "Number of batches: 1151\n",
            "Number of Sentences Trained on: 18416\n",
            "Training loss epoch: 0.07812842709507903\n",
            "Training accuracy epoch: 1121.875\n",
            "Number of batches: 1152\n",
            "Number of Sentences Trained on: 18432\n",
            "Training loss epoch: 0.07807083766207144\n",
            "Training accuracy epoch: 1122.875\n",
            "Number of batches: 1153\n",
            "Number of Sentences Trained on: 18448\n",
            "Training loss epoch: 0.07808960172040243\n",
            "Training accuracy epoch: 1123.8125\n",
            "Number of batches: 1154\n",
            "Number of Sentences Trained on: 18464\n",
            "Training loss epoch: 0.07806300245793898\n",
            "Training accuracy epoch: 1124.8125\n",
            "Number of batches: 1155\n",
            "Number of Sentences Trained on: 18480\n",
            "Training loss epoch: 0.07803516157288072\n",
            "Training accuracy epoch: 1125.8125\n",
            "Number of batches: 1156\n",
            "Number of Sentences Trained on: 18496\n",
            "Training loss epoch: 0.0780066134749816\n",
            "Training accuracy epoch: 1126.8125\n",
            "Number of batches: 1157\n",
            "Number of Sentences Trained on: 18512\n",
            "Training loss epoch: 0.07801167573639077\n",
            "Training accuracy epoch: 1127.8125\n",
            "Number of batches: 1158\n",
            "Number of Sentences Trained on: 18528\n",
            "Training loss epoch: 0.07796345030180982\n",
            "Training accuracy epoch: 1128.8125\n",
            "Number of batches: 1159\n",
            "Number of Sentences Trained on: 18544\n",
            "Training loss epoch: 0.07790213317052333\n",
            "Training accuracy epoch: 1129.8125\n",
            "Number of batches: 1160\n",
            "Number of Sentences Trained on: 18560\n",
            "Training loss epoch: 0.0780700523196499\n",
            "Training accuracy epoch: 1130.6875\n",
            "Number of batches: 1161\n",
            "Number of Sentences Trained on: 18576\n",
            "Training loss epoch: 0.07800716709200511\n",
            "Training accuracy epoch: 1131.6875\n",
            "Number of batches: 1162\n",
            "Number of Sentences Trained on: 18592\n",
            "Training loss epoch: 0.07799840595405821\n",
            "Training accuracy epoch: 1132.625\n",
            "Number of batches: 1163\n",
            "Number of Sentences Trained on: 18608\n",
            "Training loss epoch: 0.07800633938326196\n",
            "Training accuracy epoch: 1133.625\n",
            "Number of batches: 1164\n",
            "Number of Sentences Trained on: 18624\n",
            "Training loss epoch: 0.07836701094108942\n",
            "Training accuracy epoch: 1134.5\n",
            "Number of batches: 1165\n",
            "Number of Sentences Trained on: 18640\n",
            "Training loss epoch: 0.07831675499277493\n",
            "Training accuracy epoch: 1135.5\n",
            "Number of batches: 1166\n",
            "Number of Sentences Trained on: 18656\n",
            "Training loss epoch: 0.07831326555504584\n",
            "Training accuracy epoch: 1136.5\n",
            "Number of batches: 1167\n",
            "Number of Sentences Trained on: 18672\n",
            "Training loss epoch: 0.07829545728271438\n",
            "Training accuracy epoch: 1137.5\n",
            "Number of batches: 1168\n",
            "Number of Sentences Trained on: 18688\n",
            "Training loss epoch: 0.07823802284729867\n",
            "Training accuracy epoch: 1138.5\n",
            "Number of batches: 1169\n",
            "Number of Sentences Trained on: 18704\n",
            "Training loss epoch: 0.0782423463385377\n",
            "Training accuracy epoch: 1139.5\n",
            "Number of batches: 1170\n",
            "Number of Sentences Trained on: 18720\n",
            "Training loss epoch: 0.0781989327011937\n",
            "Training accuracy epoch: 1140.5\n",
            "Number of batches: 1171\n",
            "Number of Sentences Trained on: 18736\n",
            "Training loss epoch: 0.07820464312571733\n",
            "Training accuracy epoch: 1141.5\n",
            "Number of batches: 1172\n",
            "Number of Sentences Trained on: 18752\n",
            "Training loss epoch: 0.07814564062492844\n",
            "Training accuracy epoch: 1142.5\n",
            "Number of batches: 1173\n",
            "Number of Sentences Trained on: 18768\n",
            "Training loss epoch: 0.07809398479238501\n",
            "Training accuracy epoch: 1143.5\n",
            "Number of batches: 1174\n",
            "Number of Sentences Trained on: 18784\n",
            "Training loss epoch: 0.07805605706521013\n",
            "Training accuracy epoch: 1144.4375\n",
            "Number of batches: 1175\n",
            "Number of Sentences Trained on: 18800\n",
            "Training loss epoch: 0.0781042805639631\n",
            "Training accuracy epoch: 1145.375\n",
            "Number of batches: 1176\n",
            "Number of Sentences Trained on: 18816\n",
            "Training loss epoch: 0.07806096188292799\n",
            "Training accuracy epoch: 1146.375\n",
            "Number of batches: 1177\n",
            "Number of Sentences Trained on: 18832\n",
            "Training loss epoch: 0.0781217126208571\n",
            "Training accuracy epoch: 1147.3125\n",
            "Number of batches: 1178\n",
            "Number of Sentences Trained on: 18848\n",
            "Training loss epoch: 0.07807595737454615\n",
            "Training accuracy epoch: 1148.3125\n",
            "Number of batches: 1179\n",
            "Number of Sentences Trained on: 18864\n",
            "Training loss epoch: 0.07811747194895269\n",
            "Training accuracy epoch: 1149.25\n",
            "Number of batches: 1180\n",
            "Number of Sentences Trained on: 18880\n",
            "Training loss epoch: 0.07806240483407943\n",
            "Training accuracy epoch: 1150.25\n",
            "Number of batches: 1181\n",
            "Number of Sentences Trained on: 18896\n",
            "Training loss epoch: 0.07800751703404452\n",
            "Training accuracy epoch: 1151.25\n",
            "Number of batches: 1182\n",
            "Number of Sentences Trained on: 18912\n",
            "Training loss epoch: 0.07797961319451924\n",
            "Training accuracy epoch: 1152.25\n",
            "Number of batches: 1183\n",
            "Number of Sentences Trained on: 18928\n",
            "Training loss epoch: 0.07795511935408875\n",
            "Training accuracy epoch: 1153.25\n",
            "Number of batches: 1184\n",
            "Number of Sentences Trained on: 18944\n",
            "Training loss epoch: 0.07789896581821018\n",
            "Training accuracy epoch: 1154.25\n",
            "Number of batches: 1185\n",
            "Number of Sentences Trained on: 18960\n",
            "Training loss epoch: 0.07794991312576002\n",
            "Training accuracy epoch: 1155.1875\n",
            "Number of batches: 1186\n",
            "Number of Sentences Trained on: 18976\n",
            "Training loss epoch: 0.07794237295467729\n",
            "Training accuracy epoch: 1156.1875\n",
            "Number of batches: 1187\n",
            "Number of Sentences Trained on: 18992\n",
            "Training loss epoch: 0.0778958690915216\n",
            "Training accuracy epoch: 1157.1875\n",
            "Number of batches: 1188\n",
            "Number of Sentences Trained on: 19008\n",
            "Training loss epoch: 0.07784254060642241\n",
            "Training accuracy epoch: 1158.1875\n",
            "Number of batches: 1189\n",
            "Number of Sentences Trained on: 19024\n",
            "Training loss epoch: 0.07778669542558647\n",
            "Training accuracy epoch: 1159.1875\n",
            "Number of batches: 1190\n",
            "Number of Sentences Trained on: 19040\n",
            "Training loss epoch: 0.07775303044616019\n",
            "Training accuracy epoch: 1160.1875\n",
            "Number of batches: 1191\n",
            "Number of Sentences Trained on: 19056\n",
            "Training loss epoch: 0.0777106782561715\n",
            "Training accuracy epoch: 1161.125\n",
            "Number of batches: 1192\n",
            "Number of Sentences Trained on: 19072\n",
            "Training loss epoch: 0.07769221273424183\n",
            "Training accuracy epoch: 1162.0625\n",
            "Number of batches: 1193\n",
            "Number of Sentences Trained on: 19088\n",
            "Training loss epoch: 0.07763265012361739\n",
            "Training accuracy epoch: 1163.0625\n",
            "Number of batches: 1194\n",
            "Number of Sentences Trained on: 19104\n",
            "Training loss epoch: 0.07768571450585444\n",
            "Training accuracy epoch: 1164.0\n",
            "Number of batches: 1195\n",
            "Number of Sentences Trained on: 19120\n",
            "Training loss epoch: 0.0778301127874417\n",
            "Training accuracy epoch: 1164.9375\n",
            "Number of batches: 1196\n",
            "Number of Sentences Trained on: 19136\n",
            "Training loss epoch: 0.07779535408358967\n",
            "Training accuracy epoch: 1165.9375\n",
            "Number of batches: 1197\n",
            "Number of Sentences Trained on: 19152\n",
            "Training loss epoch: 0.07775287242829396\n",
            "Training accuracy epoch: 1166.9375\n",
            "Number of batches: 1198\n",
            "Number of Sentences Trained on: 19168\n",
            "Training loss epoch: 0.07770216688292382\n",
            "Training accuracy epoch: 1167.9375\n",
            "Number of batches: 1199\n",
            "Number of Sentences Trained on: 19184\n",
            "Training loss epoch: 0.07769678694099033\n",
            "Training accuracy epoch: 1168.875\n",
            "Number of batches: 1200\n",
            "Number of Sentences Trained on: 19200\n",
            "Training loss per 100 training steps: 0.07772294580405245\n",
            "Training loss epoch: 0.07772294580405245\n",
            "Training accuracy epoch: 1169.8125\n",
            "Number of batches: 1201\n",
            "Number of Sentences Trained on: 19216\n",
            "Training loss epoch: 0.07768888139191507\n",
            "Training accuracy epoch: 1170.8125\n",
            "Number of batches: 1202\n",
            "Number of Sentences Trained on: 19232\n",
            "Training loss epoch: 0.07763805001542895\n",
            "Training accuracy epoch: 1171.8125\n",
            "Number of batches: 1203\n",
            "Number of Sentences Trained on: 19248\n",
            "Training loss epoch: 0.07763979364510631\n",
            "Training accuracy epoch: 1172.75\n",
            "Number of batches: 1204\n",
            "Number of Sentences Trained on: 19264\n",
            "Training loss epoch: 0.07758986096871684\n",
            "Training accuracy epoch: 1173.75\n",
            "Number of batches: 1205\n",
            "Number of Sentences Trained on: 19280\n",
            "Training loss epoch: 0.07756183133866008\n",
            "Training accuracy epoch: 1174.75\n",
            "Number of batches: 1206\n",
            "Number of Sentences Trained on: 19296\n",
            "Training loss epoch: 0.07754894263803953\n",
            "Training accuracy epoch: 1175.6875\n",
            "Number of batches: 1207\n",
            "Number of Sentences Trained on: 19312\n",
            "Training loss epoch: 0.07749274051929392\n",
            "Training accuracy epoch: 1176.6875\n",
            "Number of batches: 1208\n",
            "Number of Sentences Trained on: 19328\n",
            "Training loss epoch: 0.0774292568560625\n",
            "Training accuracy epoch: 1177.6875\n",
            "Number of batches: 1209\n",
            "Number of Sentences Trained on: 19344\n",
            "Training loss epoch: 0.07741062608855384\n",
            "Training accuracy epoch: 1178.6875\n",
            "Number of batches: 1210\n",
            "Number of Sentences Trained on: 19360\n",
            "Training loss epoch: 0.07736777900937332\n",
            "Training accuracy epoch: 1179.6875\n",
            "Number of batches: 1211\n",
            "Number of Sentences Trained on: 19376\n",
            "Training loss epoch: 0.07730483031034832\n",
            "Training accuracy epoch: 1180.6875\n",
            "Number of batches: 1212\n",
            "Number of Sentences Trained on: 19392\n",
            "Training loss epoch: 0.07725053996224684\n",
            "Training accuracy epoch: 1181.6875\n",
            "Number of batches: 1213\n",
            "Number of Sentences Trained on: 19408\n",
            "Training loss epoch: 0.07720202792346127\n",
            "Training accuracy epoch: 1182.6875\n",
            "Number of batches: 1214\n",
            "Number of Sentences Trained on: 19424\n",
            "Training loss epoch: 0.07728982894535755\n",
            "Training accuracy epoch: 1183.625\n",
            "Number of batches: 1215\n",
            "Number of Sentences Trained on: 19440\n",
            "Training loss epoch: 0.0773675705627523\n",
            "Training accuracy epoch: 1184.5625\n",
            "Number of batches: 1216\n",
            "Number of Sentences Trained on: 19456\n",
            "Training loss epoch: 0.07754954871309855\n",
            "Training accuracy epoch: 1185.4375\n",
            "Number of batches: 1217\n",
            "Number of Sentences Trained on: 19472\n",
            "Training loss epoch: 0.07748694956239179\n",
            "Training accuracy epoch: 1186.4375\n",
            "Number of batches: 1218\n",
            "Number of Sentences Trained on: 19488\n",
            "Training loss epoch: 0.0775094165631369\n",
            "Training accuracy epoch: 1187.375\n",
            "Number of batches: 1219\n",
            "Number of Sentences Trained on: 19504\n",
            "Training loss epoch: 0.0774594615818551\n",
            "Training accuracy epoch: 1188.375\n",
            "Number of batches: 1220\n",
            "Number of Sentences Trained on: 19520\n",
            "Training loss epoch: 0.077401344945279\n",
            "Training accuracy epoch: 1189.375\n",
            "Number of batches: 1221\n",
            "Number of Sentences Trained on: 19536\n",
            "Training loss epoch: 0.07734359955661298\n",
            "Training accuracy epoch: 1190.375\n",
            "Number of batches: 1222\n",
            "Number of Sentences Trained on: 19552\n",
            "Training loss epoch: 0.07758491089699282\n",
            "Training accuracy epoch: 1191.25\n",
            "Number of batches: 1223\n",
            "Number of Sentences Trained on: 19568\n",
            "Training loss epoch: 0.07778880303231786\n",
            "Training accuracy epoch: 1192.125\n",
            "Number of batches: 1224\n",
            "Number of Sentences Trained on: 19584\n",
            "Training loss epoch: 0.07772668801077014\n",
            "Training accuracy epoch: 1193.125\n",
            "Number of batches: 1225\n",
            "Number of Sentences Trained on: 19600\n",
            "Training loss epoch: 0.07779001463094475\n",
            "Training accuracy epoch: 1194.0\n",
            "Number of batches: 1226\n",
            "Number of Sentences Trained on: 19616\n",
            "Training loss epoch: 0.07776256439962198\n",
            "Training accuracy epoch: 1195.0\n",
            "Number of batches: 1227\n",
            "Number of Sentences Trained on: 19632\n",
            "Training loss epoch: 0.07771921332403016\n",
            "Training accuracy epoch: 1196.0\n",
            "Number of batches: 1228\n",
            "Number of Sentences Trained on: 19648\n",
            "Training loss epoch: 0.07785477431470557\n",
            "Training accuracy epoch: 1196.9375\n",
            "Number of batches: 1229\n",
            "Number of Sentences Trained on: 19664\n",
            "Training loss epoch: 0.07781369336445369\n",
            "Training accuracy epoch: 1197.9375\n",
            "Number of batches: 1230\n",
            "Number of Sentences Trained on: 19680\n",
            "Training loss epoch: 0.07777803976321025\n",
            "Training accuracy epoch: 1198.875\n",
            "Number of batches: 1231\n",
            "Number of Sentences Trained on: 19696\n",
            "Training loss epoch: 0.0779024883603916\n",
            "Training accuracy epoch: 1199.8125\n",
            "Number of batches: 1232\n",
            "Number of Sentences Trained on: 19712\n",
            "Training loss epoch: 0.0778465275926514\n",
            "Training accuracy epoch: 1200.8125\n",
            "Number of batches: 1233\n",
            "Number of Sentences Trained on: 19728\n",
            "Training loss epoch: 0.07782572584292388\n",
            "Training accuracy epoch: 1201.8125\n",
            "Number of batches: 1234\n",
            "Number of Sentences Trained on: 19744\n",
            "Training loss epoch: 0.07777118793620845\n",
            "Training accuracy epoch: 1202.8125\n",
            "Number of batches: 1235\n",
            "Number of Sentences Trained on: 19760\n",
            "Training loss epoch: 0.07771835318208192\n",
            "Training accuracy epoch: 1203.8125\n",
            "Number of batches: 1236\n",
            "Number of Sentences Trained on: 19776\n",
            "Training loss epoch: 0.07767441860782608\n",
            "Training accuracy epoch: 1204.8125\n",
            "Number of batches: 1237\n",
            "Number of Sentences Trained on: 19792\n",
            "Training loss epoch: 0.0777284689110317\n",
            "Training accuracy epoch: 1205.6875\n",
            "Number of batches: 1238\n",
            "Number of Sentences Trained on: 19808\n",
            "Training loss epoch: 0.07795938130680853\n",
            "Training accuracy epoch: 1206.625\n",
            "Number of batches: 1239\n",
            "Number of Sentences Trained on: 19824\n",
            "Training loss epoch: 0.0780464003446527\n",
            "Training accuracy epoch: 1207.5\n",
            "Number of batches: 1240\n",
            "Number of Sentences Trained on: 19840\n",
            "Training loss epoch: 0.07798544627330645\n",
            "Training accuracy epoch: 1208.5\n",
            "Number of batches: 1241\n",
            "Number of Sentences Trained on: 19856\n",
            "Training loss epoch: 0.07805422396603792\n",
            "Training accuracy epoch: 1209.4375\n",
            "Number of batches: 1242\n",
            "Number of Sentences Trained on: 19872\n",
            "Training loss epoch: 0.07801492378915108\n",
            "Training accuracy epoch: 1210.4375\n",
            "Number of batches: 1243\n",
            "Number of Sentences Trained on: 19888\n",
            "Training loss epoch: 0.07798043682844112\n",
            "Training accuracy epoch: 1211.4375\n",
            "Number of batches: 1244\n",
            "Number of Sentences Trained on: 19904\n",
            "Training loss epoch: 0.07797698047083156\n",
            "Training accuracy epoch: 1212.375\n",
            "Number of batches: 1245\n",
            "Number of Sentences Trained on: 19920\n",
            "Training loss epoch: 0.07792169652953236\n",
            "Training accuracy epoch: 1213.375\n",
            "Number of batches: 1246\n",
            "Number of Sentences Trained on: 19936\n",
            "Training loss epoch: 0.07787247357233529\n",
            "Training accuracy epoch: 1214.375\n",
            "Number of batches: 1247\n",
            "Number of Sentences Trained on: 19952\n",
            "Training loss epoch: 0.07782923721895667\n",
            "Training accuracy epoch: 1215.375\n",
            "Number of batches: 1248\n",
            "Number of Sentences Trained on: 19968\n",
            "Training loss epoch: 0.07784373126234308\n",
            "Training accuracy epoch: 1216.375\n",
            "Number of batches: 1249\n",
            "Number of Sentences Trained on: 19984\n",
            "Training loss epoch: 0.07780240100529044\n",
            "Training accuracy epoch: 1217.375\n",
            "Number of batches: 1250\n",
            "Number of Sentences Trained on: 20000\n",
            "Training loss epoch: 0.07774420684076203\n",
            "Training accuracy epoch: 1218.375\n",
            "Number of batches: 1251\n",
            "Number of Sentences Trained on: 20016\n",
            "Training loss epoch: 0.07771836499108549\n",
            "Training accuracy epoch: 1219.375\n",
            "Number of batches: 1252\n",
            "Number of Sentences Trained on: 20032\n",
            "Training loss epoch: 0.07765703970236193\n",
            "Training accuracy epoch: 1220.375\n",
            "Number of batches: 1253\n",
            "Number of Sentences Trained on: 20048\n",
            "Training loss epoch: 0.07777918217122737\n",
            "Training accuracy epoch: 1221.25\n",
            "Number of batches: 1254\n",
            "Number of Sentences Trained on: 20064\n",
            "Training loss epoch: 0.07785673564391775\n",
            "Training accuracy epoch: 1222.1875\n",
            "Number of batches: 1255\n",
            "Number of Sentences Trained on: 20080\n",
            "Training loss epoch: 0.07782576708815463\n",
            "Training accuracy epoch: 1223.125\n",
            "Number of batches: 1256\n",
            "Number of Sentences Trained on: 20096\n",
            "Training loss epoch: 0.077770331512139\n",
            "Training accuracy epoch: 1224.125\n",
            "Number of batches: 1257\n",
            "Number of Sentences Trained on: 20112\n",
            "Training loss epoch: 0.0778217787708808\n",
            "Training accuracy epoch: 1225.0625\n",
            "Number of batches: 1258\n",
            "Number of Sentences Trained on: 20128\n",
            "Training loss epoch: 0.07776602203054339\n",
            "Training accuracy epoch: 1226.0625\n",
            "Number of batches: 1259\n",
            "Number of Sentences Trained on: 20144\n",
            "Training loss epoch: 0.07779869486772759\n",
            "Training accuracy epoch: 1227.0\n",
            "Number of batches: 1260\n",
            "Number of Sentences Trained on: 20160\n",
            "Training loss epoch: 0.07777122538613412\n",
            "Training accuracy epoch: 1228.0\n",
            "Number of batches: 1261\n",
            "Number of Sentences Trained on: 20176\n",
            "Training loss epoch: 0.07771751938180935\n",
            "Training accuracy epoch: 1229.0\n",
            "Number of batches: 1262\n",
            "Number of Sentences Trained on: 20192\n",
            "Training loss epoch: 0.0776651775724253\n",
            "Training accuracy epoch: 1230.0\n",
            "Number of batches: 1263\n",
            "Number of Sentences Trained on: 20208\n",
            "Training loss epoch: 0.07771683870950129\n",
            "Training accuracy epoch: 1230.9375\n",
            "Number of batches: 1264\n",
            "Number of Sentences Trained on: 20224\n",
            "Training loss epoch: 0.07767408718663293\n",
            "Training accuracy epoch: 1231.9375\n",
            "Number of batches: 1265\n",
            "Number of Sentences Trained on: 20240\n",
            "Training loss epoch: 0.07761888358020885\n",
            "Training accuracy epoch: 1232.9375\n",
            "Number of batches: 1266\n",
            "Number of Sentences Trained on: 20256\n",
            "Training loss epoch: 0.07760136867534323\n",
            "Training accuracy epoch: 1233.875\n",
            "Number of batches: 1267\n",
            "Number of Sentences Trained on: 20272\n",
            "Training loss epoch: 0.07762344530154694\n",
            "Training accuracy epoch: 1234.875\n",
            "Number of batches: 1268\n",
            "Number of Sentences Trained on: 20288\n",
            "Training loss epoch: 0.07764443855139502\n",
            "Training accuracy epoch: 1235.8125\n",
            "Number of batches: 1269\n",
            "Number of Sentences Trained on: 20304\n",
            "Training loss epoch: 0.07759354921515231\n",
            "Training accuracy epoch: 1236.8125\n",
            "Number of batches: 1270\n",
            "Number of Sentences Trained on: 20320\n",
            "Training loss epoch: 0.07756979133376711\n",
            "Training accuracy epoch: 1237.75\n",
            "Number of batches: 1271\n",
            "Number of Sentences Trained on: 20336\n",
            "Training loss epoch: 0.07752380046129348\n",
            "Training accuracy epoch: 1238.75\n",
            "Number of batches: 1272\n",
            "Number of Sentences Trained on: 20352\n",
            "Training loss epoch: 0.07747407527035981\n",
            "Training accuracy epoch: 1239.75\n",
            "Number of batches: 1273\n",
            "Number of Sentences Trained on: 20368\n",
            "Training loss epoch: 0.07741536059192224\n",
            "Training accuracy epoch: 1240.75\n",
            "Number of batches: 1274\n",
            "Number of Sentences Trained on: 20384\n",
            "Training loss epoch: 0.07735875190486766\n",
            "Training accuracy epoch: 1241.75\n",
            "Number of batches: 1275\n",
            "Number of Sentences Trained on: 20400\n",
            "Training loss epoch: 0.0773128675441327\n",
            "Training accuracy epoch: 1242.75\n",
            "Number of batches: 1276\n",
            "Number of Sentences Trained on: 20416\n",
            "Training loss epoch: 0.07725427792834763\n",
            "Training accuracy epoch: 1243.75\n",
            "Number of batches: 1277\n",
            "Number of Sentences Trained on: 20432\n",
            "Training loss epoch: 0.07719535744272235\n",
            "Training accuracy epoch: 1244.75\n",
            "Number of batches: 1278\n",
            "Number of Sentences Trained on: 20448\n",
            "Training loss epoch: 0.07714955854473382\n",
            "Training accuracy epoch: 1245.75\n",
            "Number of batches: 1279\n",
            "Number of Sentences Trained on: 20464\n",
            "Training loss epoch: 0.07711113583677616\n",
            "Training accuracy epoch: 1246.75\n",
            "Number of batches: 1280\n",
            "Number of Sentences Trained on: 20480\n",
            "Training loss epoch: 0.07705441231716309\n",
            "Training accuracy epoch: 1247.75\n",
            "Number of batches: 1281\n",
            "Number of Sentences Trained on: 20496\n",
            "Training loss epoch: 0.07705372958853601\n",
            "Training accuracy epoch: 1248.6875\n",
            "Number of batches: 1282\n",
            "Number of Sentences Trained on: 20512\n",
            "Training loss epoch: 0.0770312917729733\n",
            "Training accuracy epoch: 1249.6875\n",
            "Number of batches: 1283\n",
            "Number of Sentences Trained on: 20528\n",
            "Training loss epoch: 0.07704862874037349\n",
            "Training accuracy epoch: 1250.625\n",
            "Number of batches: 1284\n",
            "Number of Sentences Trained on: 20544\n",
            "Training loss epoch: 0.07709350361690788\n",
            "Training accuracy epoch: 1251.5625\n",
            "Number of batches: 1285\n",
            "Number of Sentences Trained on: 20560\n",
            "Training loss epoch: 0.07706497048934333\n",
            "Training accuracy epoch: 1252.5625\n",
            "Number of batches: 1286\n",
            "Number of Sentences Trained on: 20576\n",
            "Training loss epoch: 0.07701422584693765\n",
            "Training accuracy epoch: 1253.5625\n",
            "Number of batches: 1287\n",
            "Number of Sentences Trained on: 20592\n",
            "Training loss epoch: 0.07702031727841012\n",
            "Training accuracy epoch: 1254.5\n",
            "Number of batches: 1288\n",
            "Number of Sentences Trained on: 20608\n",
            "Training loss epoch: 0.07717390376650905\n",
            "Training accuracy epoch: 1255.4375\n",
            "Number of batches: 1289\n",
            "Number of Sentences Trained on: 20624\n",
            "Training loss epoch: 0.07720961353859807\n",
            "Training accuracy epoch: 1256.375\n",
            "Number of batches: 1290\n",
            "Number of Sentences Trained on: 20640\n",
            "Training loss epoch: 0.0771562192656405\n",
            "Training accuracy epoch: 1257.375\n",
            "Number of batches: 1291\n",
            "Number of Sentences Trained on: 20656\n",
            "Training loss epoch: 0.07711325655686821\n",
            "Training accuracy epoch: 1258.375\n",
            "Number of batches: 1292\n",
            "Number of Sentences Trained on: 20672\n",
            "Training loss epoch: 0.07705982271591792\n",
            "Training accuracy epoch: 1259.375\n",
            "Number of batches: 1293\n",
            "Number of Sentences Trained on: 20688\n",
            "Training loss epoch: 0.07708709635889666\n",
            "Training accuracy epoch: 1260.3125\n",
            "Number of batches: 1294\n",
            "Number of Sentences Trained on: 20704\n",
            "Training loss epoch: 0.07704280274394273\n",
            "Training accuracy epoch: 1261.3125\n",
            "Number of batches: 1295\n",
            "Number of Sentences Trained on: 20720\n",
            "Training loss epoch: 0.07699129847376887\n",
            "Training accuracy epoch: 1262.3125\n",
            "Number of batches: 1296\n",
            "Number of Sentences Trained on: 20736\n",
            "Training loss epoch: 0.0771358264724612\n",
            "Training accuracy epoch: 1263.25\n",
            "Number of batches: 1297\n",
            "Number of Sentences Trained on: 20752\n",
            "Training loss epoch: 0.0770937005977325\n",
            "Training accuracy epoch: 1264.25\n",
            "Number of batches: 1298\n",
            "Number of Sentences Trained on: 20768\n",
            "Training loss epoch: 0.0770380276862838\n",
            "Training accuracy epoch: 1265.25\n",
            "Number of batches: 1299\n",
            "Number of Sentences Trained on: 20784\n",
            "Training loss epoch: 0.07700322073414169\n",
            "Training accuracy epoch: 1266.25\n",
            "Number of batches: 1300\n",
            "Number of Sentences Trained on: 20800\n",
            "Training loss per 100 training steps: 0.07704453589676555\n",
            "Training loss epoch: 0.07704453589676555\n",
            "Training accuracy epoch: 1267.1875\n",
            "Number of batches: 1301\n",
            "Number of Sentences Trained on: 20816\n",
            "Training loss epoch: 0.07698597795797259\n",
            "Training accuracy epoch: 1268.1875\n",
            "Number of batches: 1302\n",
            "Number of Sentences Trained on: 20832\n",
            "Training loss epoch: 0.0769314168313506\n",
            "Training accuracy epoch: 1269.1875\n",
            "Number of batches: 1303\n",
            "Number of Sentences Trained on: 20848\n",
            "Training loss epoch: 0.0768978416641972\n",
            "Training accuracy epoch: 1270.1875\n",
            "Number of batches: 1304\n",
            "Number of Sentences Trained on: 20864\n",
            "Training loss epoch: 0.07700088972754783\n",
            "Training accuracy epoch: 1271.125\n",
            "Number of batches: 1305\n",
            "Number of Sentences Trained on: 20880\n",
            "Training loss epoch: 0.07703494235815639\n",
            "Training accuracy epoch: 1272.125\n",
            "Number of batches: 1306\n",
            "Number of Sentences Trained on: 20896\n",
            "Training loss epoch: 0.07697827451351663\n",
            "Training accuracy epoch: 1273.125\n",
            "Number of batches: 1307\n",
            "Number of Sentences Trained on: 20912\n",
            "Training loss epoch: 0.07702198976353387\n",
            "Training accuracy epoch: 1274.0625\n",
            "Number of batches: 1308\n",
            "Number of Sentences Trained on: 20928\n",
            "Training loss epoch: 0.07711952370065858\n",
            "Training accuracy epoch: 1275.0\n",
            "Number of batches: 1309\n",
            "Number of Sentences Trained on: 20944\n",
            "Training loss epoch: 0.07708607457355127\n",
            "Training accuracy epoch: 1276.0\n",
            "Number of batches: 1310\n",
            "Number of Sentences Trained on: 20960\n",
            "Training loss epoch: 0.07703260421504471\n",
            "Training accuracy epoch: 1277.0\n",
            "Number of batches: 1311\n",
            "Number of Sentences Trained on: 20976\n",
            "Training loss epoch: 0.0769936334073885\n",
            "Training accuracy epoch: 1278.0\n",
            "Number of batches: 1312\n",
            "Number of Sentences Trained on: 20992\n",
            "Training loss epoch: 0.07697205595543213\n",
            "Training accuracy epoch: 1279.0\n",
            "Number of batches: 1313\n",
            "Number of Sentences Trained on: 21008\n",
            "Training loss epoch: 0.07708634046524589\n",
            "Training accuracy epoch: 1279.9375\n",
            "Number of batches: 1314\n",
            "Number of Sentences Trained on: 21024\n",
            "Training loss epoch: 0.07704158202738823\n",
            "Training accuracy epoch: 1280.9375\n",
            "Number of batches: 1315\n",
            "Number of Sentences Trained on: 21040\n",
            "Training loss epoch: 0.07699782071330996\n",
            "Training accuracy epoch: 1281.9375\n",
            "Number of batches: 1316\n",
            "Number of Sentences Trained on: 21056\n",
            "Training loss epoch: 0.07694304070207231\n",
            "Training accuracy epoch: 1282.9375\n",
            "Number of batches: 1317\n",
            "Number of Sentences Trained on: 21072\n",
            "Training loss epoch: 0.07689360959342488\n",
            "Training accuracy epoch: 1283.9375\n",
            "Number of batches: 1318\n",
            "Number of Sentences Trained on: 21088\n",
            "Training loss epoch: 0.0768615187137738\n",
            "Training accuracy epoch: 1284.9375\n",
            "Number of batches: 1319\n",
            "Number of Sentences Trained on: 21104\n",
            "Training loss epoch: 0.07681015279522399\n",
            "Training accuracy epoch: 1285.9375\n",
            "Number of batches: 1320\n",
            "Number of Sentences Trained on: 21120\n",
            "Training loss epoch: 0.0768042268245347\n",
            "Training accuracy epoch: 1286.9375\n",
            "Number of batches: 1321\n",
            "Number of Sentences Trained on: 21136\n",
            "Training loss epoch: 0.07676004304952318\n",
            "Training accuracy epoch: 1287.9375\n",
            "Number of batches: 1322\n",
            "Number of Sentences Trained on: 21152\n",
            "Training loss epoch: 0.07671845168511834\n",
            "Training accuracy epoch: 1288.875\n",
            "Number of batches: 1323\n",
            "Number of Sentences Trained on: 21168\n",
            "Training loss epoch: 0.07667731474336761\n",
            "Training accuracy epoch: 1289.875\n",
            "Number of batches: 1324\n",
            "Number of Sentences Trained on: 21184\n",
            "Training loss epoch: 0.07665239478377976\n",
            "Training accuracy epoch: 1290.875\n",
            "Number of batches: 1325\n",
            "Number of Sentences Trained on: 21200\n",
            "Training loss epoch: 0.07660539921968901\n",
            "Training accuracy epoch: 1291.875\n",
            "Number of batches: 1326\n",
            "Number of Sentences Trained on: 21216\n",
            "Training loss epoch: 0.0767030390243105\n",
            "Training accuracy epoch: 1292.75\n",
            "Number of batches: 1327\n",
            "Number of Sentences Trained on: 21232\n",
            "Training loss epoch: 0.07688520073997458\n",
            "Training accuracy epoch: 1293.6875\n",
            "Number of batches: 1328\n",
            "Number of Sentences Trained on: 21248\n",
            "Training loss epoch: 0.07683187095344836\n",
            "Training accuracy epoch: 1294.6875\n",
            "Number of batches: 1329\n",
            "Number of Sentences Trained on: 21264\n",
            "Training loss epoch: 0.07679109018146327\n",
            "Training accuracy epoch: 1295.6875\n",
            "Number of batches: 1330\n",
            "Number of Sentences Trained on: 21280\n",
            "Training loss epoch: 0.07693901539880434\n",
            "Training accuracy epoch: 1296.625\n",
            "Number of batches: 1331\n",
            "Number of Sentences Trained on: 21296\n",
            "Training loss epoch: 0.07700315097074392\n",
            "Training accuracy epoch: 1297.5625\n",
            "Number of batches: 1332\n",
            "Number of Sentences Trained on: 21312\n",
            "Training loss epoch: 0.07703010323101973\n",
            "Training accuracy epoch: 1298.4375\n",
            "Number of batches: 1333\n",
            "Number of Sentences Trained on: 21328\n",
            "Training loss epoch: 0.07698948422836585\n",
            "Training accuracy epoch: 1299.4375\n",
            "Number of batches: 1334\n",
            "Number of Sentences Trained on: 21344\n",
            "Training loss epoch: 0.07706458780165476\n",
            "Training accuracy epoch: 1300.4375\n",
            "Number of batches: 1335\n",
            "Number of Sentences Trained on: 21360\n",
            "Training loss epoch: 0.07702051903736187\n",
            "Training accuracy epoch: 1301.4375\n",
            "Number of batches: 1336\n",
            "Number of Sentences Trained on: 21376\n",
            "Training loss epoch: 0.07696646538364722\n",
            "Training accuracy epoch: 1302.4375\n",
            "Number of batches: 1337\n",
            "Number of Sentences Trained on: 21392\n",
            "Training loss epoch: 0.07695857759030718\n",
            "Training accuracy epoch: 1303.4375\n",
            "Number of batches: 1338\n",
            "Number of Sentences Trained on: 21408\n",
            "Training loss epoch: 0.0769291389111483\n",
            "Training accuracy epoch: 1304.375\n",
            "Number of batches: 1339\n",
            "Number of Sentences Trained on: 21424\n",
            "Training loss epoch: 0.07696147104903339\n",
            "Training accuracy epoch: 1305.3125\n",
            "Number of batches: 1340\n",
            "Number of Sentences Trained on: 21440\n",
            "Training loss epoch: 0.07706688527587494\n",
            "Training accuracy epoch: 1306.25\n",
            "Number of batches: 1341\n",
            "Number of Sentences Trained on: 21456\n",
            "Training loss epoch: 0.0770124613936597\n",
            "Training accuracy epoch: 1307.25\n",
            "Number of batches: 1342\n",
            "Number of Sentences Trained on: 21472\n",
            "Training loss epoch: 0.07696049878057695\n",
            "Training accuracy epoch: 1308.25\n",
            "Number of batches: 1343\n",
            "Number of Sentences Trained on: 21488\n",
            "Training loss epoch: 0.07699744875264428\n",
            "Training accuracy epoch: 1309.1875\n",
            "Number of batches: 1344\n",
            "Number of Sentences Trained on: 21504\n",
            "Training loss epoch: 0.0771713814729752\n",
            "Training accuracy epoch: 1310.0625\n",
            "Number of batches: 1345\n",
            "Number of Sentences Trained on: 21520\n",
            "Training loss epoch: 0.07712335764043161\n",
            "Training accuracy epoch: 1311.0625\n",
            "Number of batches: 1346\n",
            "Number of Sentences Trained on: 21536\n",
            "Training loss epoch: 0.07708171769461\n",
            "Training accuracy epoch: 1312.0625\n",
            "Number of batches: 1347\n",
            "Number of Sentences Trained on: 21552\n",
            "Training loss epoch: 0.077030062072025\n",
            "Training accuracy epoch: 1313.0625\n",
            "Number of batches: 1348\n",
            "Number of Sentences Trained on: 21568\n",
            "Training loss epoch: 0.0770176267938732\n",
            "Training accuracy epoch: 1314.0625\n",
            "Number of batches: 1349\n",
            "Number of Sentences Trained on: 21584\n",
            "Training loss epoch: 0.07696503604663088\n",
            "Training accuracy epoch: 1315.0625\n",
            "Number of batches: 1350\n",
            "Number of Sentences Trained on: 21600\n",
            "Training loss epoch: 0.07692351623020031\n",
            "Training accuracy epoch: 1316.0625\n",
            "Number of batches: 1351\n",
            "Number of Sentences Trained on: 21616\n",
            "Training loss epoch: 0.07701162603817578\n",
            "Training accuracy epoch: 1317.0\n",
            "Number of batches: 1352\n",
            "Number of Sentences Trained on: 21632\n",
            "Training loss epoch: 0.07695991628967998\n",
            "Training accuracy epoch: 1318.0\n",
            "Number of batches: 1353\n",
            "Number of Sentences Trained on: 21648\n",
            "Training loss epoch: 0.07690654637260559\n",
            "Training accuracy epoch: 1319.0\n",
            "Number of batches: 1354\n",
            "Number of Sentences Trained on: 21664\n",
            "Training loss epoch: 0.07687484348176488\n",
            "Training accuracy epoch: 1319.9375\n",
            "Number of batches: 1355\n",
            "Number of Sentences Trained on: 21680\n",
            "Training loss epoch: 0.07685980352146068\n",
            "Training accuracy epoch: 1320.9375\n",
            "Number of batches: 1356\n",
            "Number of Sentences Trained on: 21696\n",
            "Training loss epoch: 0.07684423779975588\n",
            "Training accuracy epoch: 1321.9375\n",
            "Number of batches: 1357\n",
            "Number of Sentences Trained on: 21712\n",
            "Training loss epoch: 0.07683085770088534\n",
            "Training accuracy epoch: 1322.9375\n",
            "Number of batches: 1358\n",
            "Number of Sentences Trained on: 21728\n",
            "Training loss epoch: 0.07698305068792696\n",
            "Training accuracy epoch: 1323.8125\n",
            "Number of batches: 1359\n",
            "Number of Sentences Trained on: 21744\n",
            "Training loss epoch: 0.07693544373413555\n",
            "Training accuracy epoch: 1324.8125\n",
            "Number of batches: 1360\n",
            "Number of Sentences Trained on: 21760\n",
            "Training loss epoch: 0.07690029325820595\n",
            "Training accuracy epoch: 1325.8125\n",
            "Number of batches: 1361\n",
            "Number of Sentences Trained on: 21776\n",
            "Training loss epoch: 0.07685997051146003\n",
            "Training accuracy epoch: 1326.8125\n",
            "Number of batches: 1362\n",
            "Number of Sentences Trained on: 21792\n",
            "Training loss epoch: 0.07690770015184906\n",
            "Training accuracy epoch: 1327.8125\n",
            "Number of batches: 1363\n",
            "Number of Sentences Trained on: 21808\n",
            "Training loss epoch: 0.07694239958256512\n",
            "Training accuracy epoch: 1328.75\n",
            "Number of batches: 1364\n",
            "Number of Sentences Trained on: 21824\n",
            "Training loss epoch: 0.07689820026007443\n",
            "Training accuracy epoch: 1329.75\n",
            "Number of batches: 1365\n",
            "Number of Sentences Trained on: 21840\n",
            "Training loss epoch: 0.07684877848169647\n",
            "Training accuracy epoch: 1330.75\n",
            "Number of batches: 1366\n",
            "Number of Sentences Trained on: 21856\n",
            "Training loss epoch: 0.07682736571228416\n",
            "Training accuracy epoch: 1331.75\n",
            "Number of batches: 1367\n",
            "Number of Sentences Trained on: 21872\n",
            "Training loss epoch: 0.07677394051104955\n",
            "Training accuracy epoch: 1332.75\n",
            "Number of batches: 1368\n",
            "Number of Sentences Trained on: 21888\n",
            "Training loss epoch: 0.07673823126814368\n",
            "Training accuracy epoch: 1333.75\n",
            "Number of batches: 1369\n",
            "Number of Sentences Trained on: 21904\n",
            "Training loss epoch: 0.07668933519846284\n",
            "Training accuracy epoch: 1334.75\n",
            "Number of batches: 1370\n",
            "Number of Sentences Trained on: 21920\n",
            "Training loss epoch: 0.07663592626344932\n",
            "Training accuracy epoch: 1335.75\n",
            "Number of batches: 1371\n",
            "Number of Sentences Trained on: 21936\n",
            "Training loss epoch: 0.07667395692718665\n",
            "Training accuracy epoch: 1336.6875\n",
            "Number of batches: 1372\n",
            "Number of Sentences Trained on: 21952\n",
            "Training loss epoch: 0.07663072305817112\n",
            "Training accuracy epoch: 1337.6875\n",
            "Number of batches: 1373\n",
            "Number of Sentences Trained on: 21968\n",
            "Training loss epoch: 0.07658109806595456\n",
            "Training accuracy epoch: 1338.6875\n",
            "Number of batches: 1374\n",
            "Number of Sentences Trained on: 21984\n",
            "Training loss epoch: 0.07661669261063533\n",
            "Training accuracy epoch: 1339.625\n",
            "Number of batches: 1375\n",
            "Number of Sentences Trained on: 22000\n",
            "Training loss epoch: 0.0765619361238858\n",
            "Training accuracy epoch: 1340.625\n",
            "Number of batches: 1376\n",
            "Number of Sentences Trained on: 22016\n",
            "Training loss epoch: 0.07653549636637903\n",
            "Training accuracy epoch: 1341.5625\n",
            "Number of batches: 1377\n",
            "Number of Sentences Trained on: 22032\n",
            "Training loss epoch: 0.07649177498105915\n",
            "Training accuracy epoch: 1342.5625\n",
            "Number of batches: 1378\n",
            "Number of Sentences Trained on: 22048\n",
            "Training loss epoch: 0.07644412222581236\n",
            "Training accuracy epoch: 1343.5625\n",
            "Number of batches: 1379\n",
            "Number of Sentences Trained on: 22064\n",
            "Training loss epoch: 0.07648705990881274\n",
            "Training accuracy epoch: 1344.5\n",
            "Number of batches: 1380\n",
            "Number of Sentences Trained on: 22080\n",
            "Training loss epoch: 0.07645852781922755\n",
            "Training accuracy epoch: 1345.4375\n",
            "Number of batches: 1381\n",
            "Number of Sentences Trained on: 22096\n",
            "Training loss epoch: 0.07649075750578445\n",
            "Training accuracy epoch: 1346.375\n",
            "Number of batches: 1382\n",
            "Number of Sentences Trained on: 22112\n",
            "Training loss epoch: 0.07644050181844182\n",
            "Training accuracy epoch: 1347.375\n",
            "Number of batches: 1383\n",
            "Number of Sentences Trained on: 22128\n",
            "Training loss epoch: 0.07639528964172593\n",
            "Training accuracy epoch: 1348.375\n",
            "Number of batches: 1384\n",
            "Number of Sentences Trained on: 22144\n",
            "Training loss epoch: 0.0763580722516211\n",
            "Training accuracy epoch: 1349.375\n",
            "Number of batches: 1385\n",
            "Number of Sentences Trained on: 22160\n",
            "Training loss epoch: 0.07631299439801709\n",
            "Training accuracy epoch: 1350.375\n",
            "Number of batches: 1386\n",
            "Number of Sentences Trained on: 22176\n",
            "Training loss epoch: 0.07647983202776719\n",
            "Training accuracy epoch: 1351.25\n",
            "Number of batches: 1387\n",
            "Number of Sentences Trained on: 22192\n",
            "Training loss epoch: 0.07643020863787195\n",
            "Training accuracy epoch: 1352.25\n",
            "Number of batches: 1388\n",
            "Number of Sentences Trained on: 22208\n",
            "Training loss epoch: 0.07656636488741422\n",
            "Training accuracy epoch: 1353.1875\n",
            "Number of batches: 1389\n",
            "Number of Sentences Trained on: 22224\n",
            "Training loss epoch: 0.07651692518747492\n",
            "Training accuracy epoch: 1354.1875\n",
            "Number of batches: 1390\n",
            "Number of Sentences Trained on: 22240\n",
            "Training loss epoch: 0.07646697350948585\n",
            "Training accuracy epoch: 1355.1875\n",
            "Number of batches: 1391\n",
            "Number of Sentences Trained on: 22256\n",
            "Training loss epoch: 0.07648533360030531\n",
            "Training accuracy epoch: 1356.125\n",
            "Number of batches: 1392\n",
            "Number of Sentences Trained on: 22272\n",
            "Training loss epoch: 0.07647431284535712\n",
            "Training accuracy epoch: 1357.125\n",
            "Number of batches: 1393\n",
            "Number of Sentences Trained on: 22288\n",
            "Training loss epoch: 0.07642875627018786\n",
            "Training accuracy epoch: 1358.125\n",
            "Number of batches: 1394\n",
            "Number of Sentences Trained on: 22304\n",
            "Training loss epoch: 0.07652539955089069\n",
            "Training accuracy epoch: 1359.0625\n",
            "Number of batches: 1395\n",
            "Number of Sentences Trained on: 22320\n",
            "Training loss epoch: 0.07657034842723234\n",
            "Training accuracy epoch: 1360.0\n",
            "Number of batches: 1396\n",
            "Number of Sentences Trained on: 22336\n",
            "Training loss epoch: 0.0765222473451771\n",
            "Training accuracy epoch: 1361.0\n",
            "Number of batches: 1397\n",
            "Number of Sentences Trained on: 22352\n",
            "Training loss epoch: 0.0764713686263773\n",
            "Training accuracy epoch: 1362.0\n",
            "Number of batches: 1398\n",
            "Number of Sentences Trained on: 22368\n",
            "Training loss epoch: 0.07643049279281276\n",
            "Training accuracy epoch: 1363.0\n",
            "Number of batches: 1399\n",
            "Number of Sentences Trained on: 22384\n",
            "Training loss epoch: 0.07641437954406553\n",
            "Training accuracy epoch: 1363.9375\n",
            "Number of batches: 1400\n",
            "Number of Sentences Trained on: 22400\n",
            "Training loss per 100 training steps: 0.07650388689246528\n",
            "Training loss epoch: 0.07650388689246528\n",
            "Training accuracy epoch: 1364.8125\n",
            "Number of batches: 1401\n",
            "Number of Sentences Trained on: 22416\n",
            "Training loss epoch: 0.07661708527529106\n",
            "Training accuracy epoch: 1365.6875\n",
            "Number of batches: 1402\n",
            "Number of Sentences Trained on: 22432\n",
            "Training loss epoch: 0.07666994595448955\n",
            "Training accuracy epoch: 1366.5625\n",
            "Number of batches: 1403\n",
            "Number of Sentences Trained on: 22448\n",
            "Training loss epoch: 0.07673724761178506\n",
            "Training accuracy epoch: 1367.5\n",
            "Number of batches: 1404\n",
            "Number of Sentences Trained on: 22464\n",
            "Training loss epoch: 0.07671777662225365\n",
            "Training accuracy epoch: 1368.5\n",
            "Number of batches: 1405\n",
            "Number of Sentences Trained on: 22480\n",
            "Training loss epoch: 0.07667067728458592\n",
            "Training accuracy epoch: 1369.5\n",
            "Number of batches: 1406\n",
            "Number of Sentences Trained on: 22496\n",
            "Training loss epoch: 0.07668119373350876\n",
            "Training accuracy epoch: 1370.5\n",
            "Number of batches: 1407\n",
            "Number of Sentences Trained on: 22512\n",
            "Training loss epoch: 0.07666974951681285\n",
            "Training accuracy epoch: 1371.5\n",
            "Number of batches: 1408\n",
            "Number of Sentences Trained on: 22528\n",
            "Training loss epoch: 0.076754930425897\n",
            "Training accuracy epoch: 1372.4375\n",
            "Number of batches: 1409\n",
            "Number of Sentences Trained on: 22544\n",
            "Training loss epoch: 0.07670694617162366\n",
            "Training accuracy epoch: 1373.4375\n",
            "Number of batches: 1410\n",
            "Number of Sentences Trained on: 22560\n",
            "Training loss epoch: 0.07665347349251608\n",
            "Training accuracy epoch: 1374.4375\n",
            "Number of batches: 1411\n",
            "Number of Sentences Trained on: 22576\n",
            "Training loss epoch: 0.07660675056293711\n",
            "Training accuracy epoch: 1375.4375\n",
            "Number of batches: 1412\n",
            "Number of Sentences Trained on: 22592\n",
            "Training loss epoch: 0.0765528608281312\n",
            "Training accuracy epoch: 1376.4375\n",
            "Number of batches: 1413\n",
            "Number of Sentences Trained on: 22608\n",
            "Training loss epoch: 0.07650423241964699\n",
            "Training accuracy epoch: 1377.4375\n",
            "Number of batches: 1414\n",
            "Number of Sentences Trained on: 22624\n",
            "Training loss epoch: 0.07645133120403132\n",
            "Training accuracy epoch: 1378.4375\n",
            "Number of batches: 1415\n",
            "Number of Sentences Trained on: 22640\n",
            "Training loss epoch: 0.07644352015962651\n",
            "Training accuracy epoch: 1379.4375\n",
            "Number of batches: 1416\n",
            "Number of Sentences Trained on: 22656\n",
            "Training loss epoch: 0.07641242347581192\n",
            "Training accuracy epoch: 1380.4375\n",
            "Number of batches: 1417\n",
            "Number of Sentences Trained on: 22672\n",
            "Training loss epoch: 0.07645135020653042\n",
            "Training accuracy epoch: 1381.375\n",
            "Number of batches: 1418\n",
            "Number of Sentences Trained on: 22688\n",
            "Training loss epoch: 0.07642473357426803\n",
            "Training accuracy epoch: 1382.375\n",
            "Number of batches: 1419\n",
            "Number of Sentences Trained on: 22704\n",
            "Training loss epoch: 0.0764516116691545\n",
            "Training accuracy epoch: 1383.375\n",
            "Number of batches: 1420\n",
            "Number of Sentences Trained on: 22720\n",
            "Training loss epoch: 0.0765717331193924\n",
            "Training accuracy epoch: 1384.3125\n",
            "Number of batches: 1421\n",
            "Number of Sentences Trained on: 22736\n",
            "Training loss epoch: 0.0765267749240731\n",
            "Training accuracy epoch: 1385.3125\n",
            "Number of batches: 1422\n",
            "Number of Sentences Trained on: 22752\n",
            "Training loss epoch: 0.07650012509594736\n",
            "Training accuracy epoch: 1386.3125\n",
            "Number of batches: 1423\n",
            "Number of Sentences Trained on: 22768\n",
            "Training loss epoch: 0.07649274053470395\n",
            "Training accuracy epoch: 1387.25\n",
            "Number of batches: 1424\n",
            "Number of Sentences Trained on: 22784\n",
            "Training loss epoch: 0.07649528576365443\n",
            "Training accuracy epoch: 1388.1875\n",
            "Number of batches: 1425\n",
            "Number of Sentences Trained on: 22800\n",
            "Training loss epoch: 0.07649315626933655\n",
            "Training accuracy epoch: 1389.125\n",
            "Number of batches: 1426\n",
            "Number of Sentences Trained on: 22816\n",
            "Training loss epoch: 0.07666410146305679\n",
            "Training accuracy epoch: 1390.0625\n",
            "Number of batches: 1427\n",
            "Number of Sentences Trained on: 22832\n",
            "Training loss epoch: 0.07662172746497382\n",
            "Training accuracy epoch: 1391.0625\n",
            "Number of batches: 1428\n",
            "Number of Sentences Trained on: 22848\n",
            "Training loss epoch: 0.07657438621490407\n",
            "Training accuracy epoch: 1392.0625\n",
            "Number of batches: 1429\n",
            "Number of Sentences Trained on: 22864\n",
            "Training loss epoch: 0.07652232514678459\n",
            "Training accuracy epoch: 1393.0625\n",
            "Number of batches: 1430\n",
            "Number of Sentences Trained on: 22880\n",
            "Training loss epoch: 0.07647852924585119\n",
            "Training accuracy epoch: 1394.0625\n",
            "Number of batches: 1431\n",
            "Number of Sentences Trained on: 22896\n",
            "Training loss epoch: 0.07650958486600905\n",
            "Training accuracy epoch: 1395.0\n",
            "Number of batches: 1432\n",
            "Number of Sentences Trained on: 22912\n",
            "Training loss epoch: 0.07649870778964712\n",
            "Training accuracy epoch: 1395.9375\n",
            "Number of batches: 1433\n",
            "Number of Sentences Trained on: 22928\n",
            "Training loss epoch: 0.07644600834746762\n",
            "Training accuracy epoch: 1396.9375\n",
            "Number of batches: 1434\n",
            "Number of Sentences Trained on: 22944\n",
            "Training loss epoch: 0.07640826032975225\n",
            "Training accuracy epoch: 1397.9375\n",
            "Number of batches: 1435\n",
            "Number of Sentences Trained on: 22960\n",
            "Training loss epoch: 0.07673489042568071\n",
            "Training accuracy epoch: 1398.875\n",
            "Number of batches: 1436\n",
            "Number of Sentences Trained on: 22976\n",
            "Training loss epoch: 0.07669090485751091\n",
            "Training accuracy epoch: 1399.875\n",
            "Number of batches: 1437\n",
            "Number of Sentences Trained on: 22992\n",
            "Training loss epoch: 0.07664828533703\n",
            "Training accuracy epoch: 1400.875\n",
            "Number of batches: 1438\n",
            "Number of Sentences Trained on: 23008\n",
            "Training loss epoch: 0.0766290034119697\n",
            "Training accuracy epoch: 1401.8125\n",
            "Number of batches: 1439\n",
            "Number of Sentences Trained on: 23024\n",
            "Training loss epoch: 0.07657942390180526\n",
            "Training accuracy epoch: 1402.8125\n",
            "Number of batches: 1440\n",
            "Number of Sentences Trained on: 23040\n",
            "Training loss epoch: 0.07653219051418601\n",
            "Training accuracy epoch: 1403.8125\n",
            "Number of batches: 1441\n",
            "Number of Sentences Trained on: 23056\n",
            "Training loss epoch: 0.07668928229251225\n",
            "Training accuracy epoch: 1404.75\n",
            "Number of batches: 1442\n",
            "Number of Sentences Trained on: 23072\n",
            "Training loss epoch: 0.07664783647083755\n",
            "Training accuracy epoch: 1405.75\n",
            "Number of batches: 1443\n",
            "Number of Sentences Trained on: 23088\n",
            "Training loss epoch: 0.07698226775930098\n",
            "Training accuracy epoch: 1406.5625\n",
            "Number of batches: 1444\n",
            "Number of Sentences Trained on: 23104\n",
            "Training loss epoch: 0.07704215192163096\n",
            "Training accuracy epoch: 1407.4375\n",
            "Number of batches: 1445\n",
            "Number of Sentences Trained on: 23120\n",
            "Training loss epoch: 0.07699494962890863\n",
            "Training accuracy epoch: 1408.4375\n",
            "Number of batches: 1446\n",
            "Number of Sentences Trained on: 23136\n",
            "Training loss epoch: 0.07695900272616538\n",
            "Training accuracy epoch: 1409.4375\n",
            "Number of batches: 1447\n",
            "Number of Sentences Trained on: 23152\n",
            "Training loss epoch: 0.07691973358186818\n",
            "Training accuracy epoch: 1410.4375\n",
            "Number of batches: 1448\n",
            "Number of Sentences Trained on: 23168\n",
            "Training loss epoch: 0.0769916167943695\n",
            "Training accuracy epoch: 1411.375\n",
            "Number of batches: 1449\n",
            "Number of Sentences Trained on: 23184\n",
            "Training loss epoch: 0.07696588665421586\n",
            "Training accuracy epoch: 1412.375\n",
            "Number of batches: 1450\n",
            "Number of Sentences Trained on: 23200\n",
            "Training loss epoch: 0.07692303014920365\n",
            "Training accuracy epoch: 1413.375\n",
            "Number of batches: 1451\n",
            "Number of Sentences Trained on: 23216\n",
            "Training loss epoch: 0.07689206599119262\n",
            "Training accuracy epoch: 1414.3125\n",
            "Number of batches: 1452\n",
            "Number of Sentences Trained on: 23232\n",
            "Training loss epoch: 0.07687656377172254\n",
            "Training accuracy epoch: 1415.25\n",
            "Number of batches: 1453\n",
            "Number of Sentences Trained on: 23248\n",
            "Training loss epoch: 0.07683093463112885\n",
            "Training accuracy epoch: 1416.25\n",
            "Number of batches: 1454\n",
            "Number of Sentences Trained on: 23264\n",
            "Training loss epoch: 0.07678787623901365\n",
            "Training accuracy epoch: 1417.25\n",
            "Number of batches: 1455\n",
            "Number of Sentences Trained on: 23280\n",
            "Training loss epoch: 0.07697394958571808\n",
            "Training accuracy epoch: 1418.1875\n",
            "Number of batches: 1456\n",
            "Number of Sentences Trained on: 23296\n",
            "Training loss epoch: 0.07692400429778708\n",
            "Training accuracy epoch: 1419.1875\n",
            "Number of batches: 1457\n",
            "Number of Sentences Trained on: 23312\n",
            "Training loss epoch: 0.07688359240695897\n",
            "Training accuracy epoch: 1420.1875\n",
            "Number of batches: 1458\n",
            "Number of Sentences Trained on: 23328\n",
            "Training loss epoch: 0.07683740161160071\n",
            "Training accuracy epoch: 1421.1875\n",
            "Number of batches: 1459\n",
            "Number of Sentences Trained on: 23344\n",
            "Training loss epoch: 0.07678749198490699\n",
            "Training accuracy epoch: 1422.1875\n",
            "Number of batches: 1460\n",
            "Number of Sentences Trained on: 23360\n",
            "Training loss epoch: 0.07677152211053\n",
            "Training accuracy epoch: 1423.125\n",
            "Number of batches: 1461\n",
            "Number of Sentences Trained on: 23376\n",
            "Training loss epoch: 0.0767609641854242\n",
            "Training accuracy epoch: 1424.0625\n",
            "Number of batches: 1462\n",
            "Number of Sentences Trained on: 23392\n",
            "Training loss epoch: 0.07677463674326987\n",
            "Training accuracy epoch: 1425.0\n",
            "Number of batches: 1463\n",
            "Number of Sentences Trained on: 23408\n",
            "Training loss epoch: 0.07673144712485548\n",
            "Training accuracy epoch: 1426.0\n",
            "Number of batches: 1464\n",
            "Number of Sentences Trained on: 23424\n",
            "Training loss epoch: 0.07683834278391462\n",
            "Training accuracy epoch: 1426.9375\n",
            "Number of batches: 1465\n",
            "Number of Sentences Trained on: 23440\n",
            "Training loss epoch: 0.07679040549085288\n",
            "Training accuracy epoch: 1427.9375\n",
            "Number of batches: 1466\n",
            "Number of Sentences Trained on: 23456\n",
            "Training loss epoch: 0.07675966383279784\n",
            "Training accuracy epoch: 1428.9375\n",
            "Number of batches: 1467\n",
            "Number of Sentences Trained on: 23472\n",
            "Training loss epoch: 0.0767120916803159\n",
            "Training accuracy epoch: 1429.9375\n",
            "Number of batches: 1468\n",
            "Number of Sentences Trained on: 23488\n",
            "Training loss epoch: 0.07666457692869917\n",
            "Training accuracy epoch: 1430.9375\n",
            "Number of batches: 1469\n",
            "Number of Sentences Trained on: 23504\n",
            "Training loss epoch: 0.07682737338522586\n",
            "Training accuracy epoch: 1431.8125\n",
            "Number of batches: 1470\n",
            "Number of Sentences Trained on: 23520\n",
            "Training loss epoch: 0.07697297629067643\n",
            "Training accuracy epoch: 1432.75\n",
            "Number of batches: 1471\n",
            "Number of Sentences Trained on: 23536\n",
            "Training loss epoch: 0.07696636933651066\n",
            "Training accuracy epoch: 1433.6875\n",
            "Number of batches: 1472\n",
            "Number of Sentences Trained on: 23552\n",
            "Training loss epoch: 0.07693121902113455\n",
            "Training accuracy epoch: 1434.6875\n",
            "Number of batches: 1473\n",
            "Number of Sentences Trained on: 23568\n",
            "Training loss epoch: 0.07688515096963425\n",
            "Training accuracy epoch: 1435.6875\n",
            "Number of batches: 1474\n",
            "Number of Sentences Trained on: 23584\n",
            "Training loss epoch: 0.07684325320975681\n",
            "Training accuracy epoch: 1436.6875\n",
            "Number of batches: 1475\n",
            "Number of Sentences Trained on: 23600\n",
            "Training loss epoch: 0.07717171442351632\n",
            "Training accuracy epoch: 1437.5625\n",
            "Number of batches: 1476\n",
            "Number of Sentences Trained on: 23616\n",
            "Training loss epoch: 0.07712036001963175\n",
            "Training accuracy epoch: 1438.5625\n",
            "Number of batches: 1477\n",
            "Number of Sentences Trained on: 23632\n",
            "Training loss epoch: 0.0770695539178881\n",
            "Training accuracy epoch: 1439.5625\n",
            "Number of batches: 1478\n",
            "Number of Sentences Trained on: 23648\n",
            "Training loss epoch: 0.07705299992964318\n",
            "Training accuracy epoch: 1440.5625\n",
            "Number of batches: 1479\n",
            "Number of Sentences Trained on: 23664\n",
            "Training loss epoch: 0.07701048985817013\n",
            "Training accuracy epoch: 1441.5625\n",
            "Number of batches: 1480\n",
            "Number of Sentences Trained on: 23680\n",
            "Training loss epoch: 0.07706060957184875\n",
            "Training accuracy epoch: 1442.5\n",
            "Number of batches: 1481\n",
            "Number of Sentences Trained on: 23696\n",
            "Training loss epoch: 0.07701721507378408\n",
            "Training accuracy epoch: 1443.5\n",
            "Number of batches: 1482\n",
            "Number of Sentences Trained on: 23712\n",
            "Training loss epoch: 0.07702054680650944\n",
            "Training accuracy epoch: 1444.4375\n",
            "Number of batches: 1483\n",
            "Number of Sentences Trained on: 23728\n",
            "Training loss epoch: 0.07701931946994935\n",
            "Training accuracy epoch: 1445.375\n",
            "Number of batches: 1484\n",
            "Number of Sentences Trained on: 23744\n",
            "Training loss epoch: 0.0772065682538749\n",
            "Training accuracy epoch: 1446.3125\n",
            "Number of batches: 1485\n",
            "Number of Sentences Trained on: 23760\n",
            "Training loss epoch: 0.0771731989688958\n",
            "Training accuracy epoch: 1447.3125\n",
            "Number of batches: 1486\n",
            "Number of Sentences Trained on: 23776\n",
            "Training loss epoch: 0.07717266688406359\n",
            "Training accuracy epoch: 1448.25\n",
            "Number of batches: 1487\n",
            "Number of Sentences Trained on: 23792\n",
            "Training loss epoch: 0.0771275425064545\n",
            "Training accuracy epoch: 1449.25\n",
            "Number of batches: 1488\n",
            "Number of Sentences Trained on: 23808\n",
            "Training loss epoch: 0.0771014021743454\n",
            "Training accuracy epoch: 1450.25\n",
            "Number of batches: 1489\n",
            "Number of Sentences Trained on: 23824\n",
            "Training loss epoch: 0.07705260587513568\n",
            "Training accuracy epoch: 1451.25\n",
            "Number of batches: 1490\n",
            "Number of Sentences Trained on: 23840\n",
            "Training loss epoch: 0.07717521609889551\n",
            "Training accuracy epoch: 1452.1875\n",
            "Number of batches: 1491\n",
            "Number of Sentences Trained on: 23856\n",
            "Training loss epoch: 0.0772637152928981\n",
            "Training accuracy epoch: 1453.125\n",
            "Number of batches: 1492\n",
            "Number of Sentences Trained on: 23872\n",
            "Training loss epoch: 0.0772228563621842\n",
            "Training accuracy epoch: 1454.125\n",
            "Number of batches: 1493\n",
            "Number of Sentences Trained on: 23888\n",
            "Training loss epoch: 0.07717177875404582\n",
            "Training accuracy epoch: 1455.125\n",
            "Number of batches: 1494\n",
            "Number of Sentences Trained on: 23904\n",
            "Training loss epoch: 0.07729325191965436\n",
            "Training accuracy epoch: 1456.0\n",
            "Number of batches: 1495\n",
            "Number of Sentences Trained on: 23920\n",
            "Training loss epoch: 0.07731319596019544\n",
            "Training accuracy epoch: 1457.0\n",
            "Number of batches: 1496\n",
            "Number of Sentences Trained on: 23936\n",
            "Training loss epoch: 0.07728559187260449\n",
            "Training accuracy epoch: 1458.0\n",
            "Number of batches: 1497\n",
            "Number of Sentences Trained on: 23952\n",
            "Training loss epoch: 0.07732275196389424\n",
            "Training accuracy epoch: 1458.9375\n",
            "Number of batches: 1498\n",
            "Number of Sentences Trained on: 23968\n",
            "Training loss epoch: 0.07737243262373468\n",
            "Training accuracy epoch: 1459.875\n",
            "Number of batches: 1499\n",
            "Number of Sentences Trained on: 23984\n",
            "Training loss epoch: 0.07733565892040496\n",
            "Training accuracy epoch: 1460.875\n",
            "Number of batches: 1500\n",
            "Number of Sentences Trained on: 24000\n",
            "Training loss per 100 training steps: 0.07728922656181716\n",
            "Training loss epoch: 0.07728922656181716\n",
            "Training accuracy epoch: 1461.875\n",
            "Number of batches: 1501\n",
            "Number of Sentences Trained on: 24016\n",
            "Training loss epoch: 0.07725395735042971\n",
            "Training accuracy epoch: 1462.875\n",
            "Number of batches: 1502\n",
            "Number of Sentences Trained on: 24032\n",
            "Training loss epoch: 0.0772035687398954\n",
            "Training accuracy epoch: 1463.875\n",
            "Number of batches: 1503\n",
            "Number of Sentences Trained on: 24048\n",
            "Training loss epoch: 0.07722892352840643\n",
            "Training accuracy epoch: 1464.8125\n",
            "Number of batches: 1504\n",
            "Number of Sentences Trained on: 24064\n",
            "Training loss epoch: 0.07719659516264907\n",
            "Training accuracy epoch: 1465.8125\n",
            "Number of batches: 1505\n",
            "Number of Sentences Trained on: 24080\n",
            "Training loss epoch: 0.07714710404521988\n",
            "Training accuracy epoch: 1466.8125\n",
            "Number of batches: 1506\n",
            "Number of Sentences Trained on: 24096\n",
            "Training loss epoch: 0.0771366554586374\n",
            "Training accuracy epoch: 1467.8125\n",
            "Number of batches: 1507\n",
            "Number of Sentences Trained on: 24112\n",
            "Training loss epoch: 0.07735024017236823\n",
            "Training accuracy epoch: 1468.625\n",
            "Number of batches: 1508\n",
            "Number of Sentences Trained on: 24128\n",
            "Training loss epoch: 0.0773092762112435\n",
            "Training accuracy epoch: 1469.625\n",
            "Number of batches: 1509\n",
            "Number of Sentences Trained on: 24144\n",
            "Training loss epoch: 0.07740070056172152\n",
            "Training accuracy epoch: 1470.5625\n",
            "Number of batches: 1510\n",
            "Number of Sentences Trained on: 24160\n",
            "Training loss epoch: 0.0773535143441826\n",
            "Training accuracy epoch: 1471.5625\n",
            "Number of batches: 1511\n",
            "Number of Sentences Trained on: 24176\n",
            "Training loss epoch: 0.07734712740813617\n",
            "Training accuracy epoch: 1472.5625\n",
            "Number of batches: 1512\n",
            "Number of Sentences Trained on: 24192\n",
            "Training loss epoch: 0.07735220381071141\n",
            "Training accuracy epoch: 1473.5625\n",
            "Number of batches: 1513\n",
            "Number of Sentences Trained on: 24208\n",
            "Training loss epoch: 0.07733879006860911\n",
            "Training accuracy epoch: 1474.5\n",
            "Number of batches: 1514\n",
            "Number of Sentences Trained on: 24224\n",
            "Training loss epoch: 0.07767790970809392\n",
            "Training accuracy epoch: 1475.375\n",
            "Number of batches: 1515\n",
            "Number of Sentences Trained on: 24240\n",
            "Training loss epoch: 0.07762717731434242\n",
            "Training accuracy epoch: 1476.375\n",
            "Number of batches: 1516\n",
            "Number of Sentences Trained on: 24256\n",
            "Training loss epoch: 0.07775369245870467\n",
            "Training accuracy epoch: 1477.3125\n",
            "Number of batches: 1517\n",
            "Number of Sentences Trained on: 24272\n",
            "Training loss epoch: 0.07770982153850749\n",
            "Training accuracy epoch: 1478.3125\n",
            "Number of batches: 1518\n",
            "Number of Sentences Trained on: 24288\n",
            "Training loss epoch: 0.07766116160654997\n",
            "Training accuracy epoch: 1479.3125\n",
            "Number of batches: 1519\n",
            "Number of Sentences Trained on: 24304\n",
            "Training loss epoch: 0.0777000628099861\n",
            "Training accuracy epoch: 1480.25\n",
            "Number of batches: 1520\n",
            "Number of Sentences Trained on: 24320\n",
            "Training loss epoch: 0.07765504159590117\n",
            "Training accuracy epoch: 1481.25\n",
            "Number of batches: 1521\n",
            "Number of Sentences Trained on: 24336\n",
            "Training loss epoch: 0.07773470965925951\n",
            "Training accuracy epoch: 1482.0625\n",
            "Number of batches: 1522\n",
            "Number of Sentences Trained on: 24352\n",
            "Training loss epoch: 0.07773284208725019\n",
            "Training accuracy epoch: 1483.0\n",
            "Number of batches: 1523\n",
            "Number of Sentences Trained on: 24368\n",
            "Training loss epoch: 0.07770449616292016\n",
            "Training accuracy epoch: 1484.0\n",
            "Number of batches: 1524\n",
            "Number of Sentences Trained on: 24384\n",
            "Training loss epoch: 0.0777482461239799\n",
            "Training accuracy epoch: 1484.9375\n",
            "Number of batches: 1525\n",
            "Number of Sentences Trained on: 24400\n",
            "Training loss epoch: 0.07769959481337445\n",
            "Training accuracy epoch: 1485.9375\n",
            "Number of batches: 1526\n",
            "Number of Sentences Trained on: 24416\n",
            "Training loss epoch: 0.07780707073595772\n",
            "Training accuracy epoch: 1486.8125\n",
            "Number of batches: 1527\n",
            "Number of Sentences Trained on: 24432\n",
            "Training loss epoch: 0.07776145335123127\n",
            "Training accuracy epoch: 1487.8125\n",
            "Number of batches: 1528\n",
            "Number of Sentences Trained on: 24448\n",
            "Training loss epoch: 0.07772386922830173\n",
            "Training accuracy epoch: 1488.8125\n",
            "Number of batches: 1529\n",
            "Number of Sentences Trained on: 24464\n",
            "Training loss epoch: 0.07767620090297439\n",
            "Training accuracy epoch: 1489.8125\n",
            "Number of batches: 1530\n",
            "Number of Sentences Trained on: 24480\n",
            "Training loss epoch: 0.07808200231224288\n",
            "Training accuracy epoch: 1490.625\n",
            "Number of batches: 1531\n",
            "Number of Sentences Trained on: 24496\n",
            "Training loss epoch: 0.07809198692212031\n",
            "Training accuracy epoch: 1491.625\n",
            "Number of batches: 1532\n",
            "Number of Sentences Trained on: 24512\n",
            "Training loss epoch: 0.07804165870781343\n",
            "Training accuracy epoch: 1492.625\n",
            "Number of batches: 1533\n",
            "Number of Sentences Trained on: 24528\n",
            "Training loss epoch: 0.07799334073250047\n",
            "Training accuracy epoch: 1493.625\n",
            "Number of batches: 1534\n",
            "Number of Sentences Trained on: 24544\n",
            "Training loss epoch: 0.07804831314021615\n",
            "Training accuracy epoch: 1494.5625\n",
            "Number of batches: 1535\n",
            "Number of Sentences Trained on: 24560\n",
            "Training loss epoch: 0.07805326584447887\n",
            "Training accuracy epoch: 1495.5\n",
            "Number of batches: 1536\n",
            "Number of Sentences Trained on: 24576\n",
            "Training loss epoch: 0.07803016337062772\n",
            "Training accuracy epoch: 1496.5\n",
            "Number of batches: 1537\n",
            "Number of Sentences Trained on: 24592\n",
            "Training loss epoch: 0.07799201571858765\n",
            "Training accuracy epoch: 1497.5\n",
            "Number of batches: 1538\n",
            "Number of Sentences Trained on: 24608\n",
            "Training loss epoch: 0.07796472627200204\n",
            "Training accuracy epoch: 1498.5\n",
            "Number of batches: 1539\n",
            "Number of Sentences Trained on: 24624\n",
            "Training loss epoch: 0.07793760428909294\n",
            "Training accuracy epoch: 1499.5\n",
            "Number of batches: 1540\n",
            "Number of Sentences Trained on: 24640\n",
            "Training loss epoch: 0.0779343308386821\n",
            "Training accuracy epoch: 1500.5\n",
            "Number of batches: 1541\n",
            "Number of Sentences Trained on: 24656\n",
            "Training loss epoch: 0.07789378807279865\n",
            "Training accuracy epoch: 1501.5\n",
            "Number of batches: 1542\n",
            "Number of Sentences Trained on: 24672\n",
            "Training loss epoch: 0.07786684588363411\n",
            "Training accuracy epoch: 1502.5\n",
            "Number of batches: 1543\n",
            "Number of Sentences Trained on: 24688\n",
            "Training loss epoch: 0.07790419119919534\n",
            "Training accuracy epoch: 1503.5\n",
            "Number of batches: 1544\n",
            "Number of Sentences Trained on: 24704\n",
            "Training loss epoch: 0.0779092536177328\n",
            "Training accuracy epoch: 1504.5\n",
            "Number of batches: 1545\n",
            "Number of Sentences Trained on: 24720\n",
            "Training loss epoch: 0.07787693742915905\n",
            "Training accuracy epoch: 1505.5\n",
            "Number of batches: 1546\n",
            "Number of Sentences Trained on: 24736\n",
            "Training loss epoch: 0.07788788172712206\n",
            "Training accuracy epoch: 1506.5\n",
            "Number of batches: 1547\n",
            "Number of Sentences Trained on: 24752\n",
            "Training loss epoch: 0.07792835644537986\n",
            "Training accuracy epoch: 1507.4375\n",
            "Number of batches: 1548\n",
            "Number of Sentences Trained on: 24768\n",
            "Training loss epoch: 0.07788154558167616\n",
            "Training accuracy epoch: 1508.4375\n",
            "Number of batches: 1549\n",
            "Number of Sentences Trained on: 24784\n",
            "Training loss epoch: 0.07784719269773398\n",
            "Training accuracy epoch: 1509.4375\n",
            "Number of batches: 1550\n",
            "Number of Sentences Trained on: 24800\n",
            "Training loss epoch: 0.07781360734263955\n",
            "Training accuracy epoch: 1510.4375\n",
            "Number of batches: 1551\n",
            "Number of Sentences Trained on: 24816\n",
            "Training loss epoch: 0.07777317285770775\n",
            "Training accuracy epoch: 1511.4375\n",
            "Number of batches: 1552\n",
            "Number of Sentences Trained on: 24832\n",
            "Training loss epoch: 0.07780650662787421\n",
            "Training accuracy epoch: 1512.4375\n",
            "Number of batches: 1553\n",
            "Number of Sentences Trained on: 24848\n",
            "Training loss epoch: 0.0777821384706201\n",
            "Training accuracy epoch: 1513.4375\n",
            "Number of batches: 1554\n",
            "Number of Sentences Trained on: 24864\n",
            "Training loss epoch: 0.07775119175867573\n",
            "Training accuracy epoch: 1514.4375\n",
            "Number of batches: 1555\n",
            "Number of Sentences Trained on: 24880\n",
            "Training loss epoch: 0.07773534868652546\n",
            "Training accuracy epoch: 1515.4375\n",
            "Number of batches: 1556\n",
            "Number of Sentences Trained on: 24896\n",
            "Training loss epoch: 0.07781872250233249\n",
            "Training accuracy epoch: 1516.375\n",
            "Number of batches: 1557\n",
            "Number of Sentences Trained on: 24912\n",
            "Training loss epoch: 0.07777019519563534\n",
            "Training accuracy epoch: 1517.375\n",
            "Number of batches: 1558\n",
            "Number of Sentences Trained on: 24928\n",
            "Training loss epoch: 0.07772290797367906\n",
            "Training accuracy epoch: 1518.375\n",
            "Number of batches: 1559\n",
            "Number of Sentences Trained on: 24944\n",
            "Training loss epoch: 0.07770609478994187\n",
            "Training accuracy epoch: 1519.375\n",
            "Number of batches: 1560\n",
            "Number of Sentences Trained on: 24960\n",
            "Training loss epoch: 0.07769820755332321\n",
            "Training accuracy epoch: 1520.375\n",
            "Number of batches: 1561\n",
            "Number of Sentences Trained on: 24976\n",
            "Training loss epoch: 0.07771675124285292\n",
            "Training accuracy epoch: 1521.3125\n",
            "Number of batches: 1562\n",
            "Number of Sentences Trained on: 24992\n",
            "Training loss epoch: 0.0776802551885821\n",
            "Training accuracy epoch: 1522.3125\n",
            "Number of batches: 1563\n",
            "Number of Sentences Trained on: 25008\n",
            "Training loss epoch: 0.0778635822532154\n",
            "Training accuracy epoch: 1523.25\n",
            "Number of batches: 1564\n",
            "Number of Sentences Trained on: 25024\n",
            "Training loss epoch: 0.07781534392954619\n",
            "Training accuracy epoch: 1524.25\n",
            "Number of batches: 1565\n",
            "Number of Sentences Trained on: 25040\n",
            "Training loss epoch: 0.07781850433325806\n",
            "Training accuracy epoch: 1525.1875\n",
            "Number of batches: 1566\n",
            "Number of Sentences Trained on: 25056\n",
            "Training loss epoch: 0.07777340240659084\n",
            "Training accuracy epoch: 1526.1875\n",
            "Number of batches: 1567\n",
            "Number of Sentences Trained on: 25072\n",
            "Training loss epoch: 0.07774960695149609\n",
            "Training accuracy epoch: 1527.1875\n",
            "Number of batches: 1568\n",
            "Number of Sentences Trained on: 25088\n",
            "Training loss epoch: 0.0777900806725774\n",
            "Training accuracy epoch: 1528.0625\n",
            "Number of batches: 1569\n",
            "Number of Sentences Trained on: 25104\n",
            "Training loss epoch: 0.07774739800290587\n",
            "Training accuracy epoch: 1529.0625\n",
            "Number of batches: 1570\n",
            "Number of Sentences Trained on: 25120\n",
            "Training loss epoch: 0.07770475777411696\n",
            "Training accuracy epoch: 1530.0625\n",
            "Number of batches: 1571\n",
            "Number of Sentences Trained on: 25136\n",
            "Training loss epoch: 0.07766809920284115\n",
            "Training accuracy epoch: 1531.0625\n",
            "Number of batches: 1572\n",
            "Number of Sentences Trained on: 25152\n",
            "Training loss epoch: 0.07777651413189539\n",
            "Training accuracy epoch: 1532.0\n",
            "Number of batches: 1573\n",
            "Number of Sentences Trained on: 25168\n",
            "Training loss epoch: 0.07781169297586842\n",
            "Training accuracy epoch: 1532.9375\n",
            "Number of batches: 1574\n",
            "Number of Sentences Trained on: 25184\n",
            "Training loss epoch: 0.07776403746987132\n",
            "Training accuracy epoch: 1533.9375\n",
            "Number of batches: 1575\n",
            "Number of Sentences Trained on: 25200\n",
            "Training loss epoch: 0.07774614383720868\n",
            "Training accuracy epoch: 1534.9375\n",
            "Number of batches: 1576\n",
            "Number of Sentences Trained on: 25216\n",
            "Training loss epoch: 0.0777057286574821\n",
            "Training accuracy epoch: 1535.9375\n",
            "Number of batches: 1577\n",
            "Number of Sentences Trained on: 25232\n",
            "Training loss epoch: 0.07766436663192076\n",
            "Training accuracy epoch: 1536.9375\n",
            "Number of batches: 1578\n",
            "Number of Sentences Trained on: 25248\n",
            "Training loss epoch: 0.07762072003508672\n",
            "Training accuracy epoch: 1537.9375\n",
            "Number of batches: 1579\n",
            "Number of Sentences Trained on: 25264\n",
            "Training loss epoch: 0.07784809647325082\n",
            "Training accuracy epoch: 1538.8125\n",
            "Number of batches: 1580\n",
            "Number of Sentences Trained on: 25280\n",
            "Training loss epoch: 0.07797351343651711\n",
            "Training accuracy epoch: 1539.6875\n",
            "Number of batches: 1581\n",
            "Number of Sentences Trained on: 25296\n",
            "Training loss epoch: 0.07796389083828004\n",
            "Training accuracy epoch: 1540.625\n",
            "Number of batches: 1582\n",
            "Number of Sentences Trained on: 25312\n",
            "Training loss epoch: 0.07794033508033807\n",
            "Training accuracy epoch: 1541.5625\n",
            "Number of batches: 1583\n",
            "Number of Sentences Trained on: 25328\n",
            "Training loss epoch: 0.07789383011989073\n",
            "Training accuracy epoch: 1542.5625\n",
            "Number of batches: 1584\n",
            "Number of Sentences Trained on: 25344\n",
            "Training loss epoch: 0.07785301235897764\n",
            "Training accuracy epoch: 1543.5625\n",
            "Number of batches: 1585\n",
            "Number of Sentences Trained on: 25360\n",
            "Training loss epoch: 0.07782606606851088\n",
            "Training accuracy epoch: 1544.5625\n",
            "Number of batches: 1586\n",
            "Number of Sentences Trained on: 25376\n",
            "Training loss epoch: 0.07778318791078283\n",
            "Training accuracy epoch: 1545.5625\n",
            "Number of batches: 1587\n",
            "Number of Sentences Trained on: 25392\n",
            "Training loss epoch: 0.07781636234925533\n",
            "Training accuracy epoch: 1546.4375\n",
            "Number of batches: 1588\n",
            "Number of Sentences Trained on: 25408\n",
            "Training loss epoch: 0.07777080636508069\n",
            "Training accuracy epoch: 1547.4375\n",
            "Number of batches: 1589\n",
            "Number of Sentences Trained on: 25424\n",
            "Training loss epoch: 0.07772360078457072\n",
            "Training accuracy epoch: 1548.4375\n",
            "Number of batches: 1590\n",
            "Number of Sentences Trained on: 25440\n",
            "Training loss epoch: 0.0776775376166407\n",
            "Training accuracy epoch: 1549.4375\n",
            "Number of batches: 1591\n",
            "Number of Sentences Trained on: 25456\n",
            "Training loss epoch: 0.07770074185734745\n",
            "Training accuracy epoch: 1550.3125\n",
            "Number of batches: 1592\n",
            "Number of Sentences Trained on: 25472\n",
            "Training loss epoch: 0.0776545189614491\n",
            "Training accuracy epoch: 1551.3125\n",
            "Number of batches: 1593\n",
            "Number of Sentences Trained on: 25488\n",
            "Training loss epoch: 0.07762061304084059\n",
            "Training accuracy epoch: 1552.3125\n",
            "Number of batches: 1594\n",
            "Number of Sentences Trained on: 25504\n",
            "Training loss epoch: 0.07763159546136585\n",
            "Training accuracy epoch: 1553.25\n",
            "Number of batches: 1595\n",
            "Number of Sentences Trained on: 25520\n",
            "Training loss epoch: 0.07759550414351209\n",
            "Training accuracy epoch: 1554.25\n",
            "Number of batches: 1596\n",
            "Number of Sentences Trained on: 25536\n",
            "Training loss epoch: 0.07766648327165111\n",
            "Training accuracy epoch: 1555.125\n",
            "Number of batches: 1597\n",
            "Number of Sentences Trained on: 25552\n",
            "Training loss epoch: 0.07763255883196707\n",
            "Training accuracy epoch: 1556.125\n",
            "Number of batches: 1598\n",
            "Number of Sentences Trained on: 25568\n",
            "Training loss epoch: 0.07768694520161692\n",
            "Training accuracy epoch: 1557.0625\n",
            "Number of batches: 1599\n",
            "Number of Sentences Trained on: 25584\n",
            "Training loss epoch: 0.07775005409994265\n",
            "Training accuracy epoch: 1558.0\n",
            "Number of batches: 1600\n",
            "Number of Sentences Trained on: 25600\n",
            "Training loss per 100 training steps: 0.07770259859587408\n",
            "Training loss epoch: 0.07770259859587408\n",
            "Training accuracy epoch: 1559.0\n",
            "Number of batches: 1601\n",
            "Number of Sentences Trained on: 25616\n",
            "Training loss epoch: 0.07782246760018481\n",
            "Training accuracy epoch: 1559.875\n",
            "Number of batches: 1602\n",
            "Number of Sentences Trained on: 25632\n",
            "Training loss epoch: 0.07777461901684965\n",
            "Training accuracy epoch: 1560.875\n",
            "Number of batches: 1603\n",
            "Number of Sentences Trained on: 25648\n",
            "Training loss epoch: 0.07776311315741477\n",
            "Training accuracy epoch: 1561.875\n",
            "Number of batches: 1604\n",
            "Number of Sentences Trained on: 25664\n",
            "Training loss epoch: 0.0778810408711107\n",
            "Training accuracy epoch: 1562.75\n",
            "Number of batches: 1605\n",
            "Number of Sentences Trained on: 25680\n",
            "Training loss epoch: 0.07783370631281794\n",
            "Training accuracy epoch: 1563.75\n",
            "Number of batches: 1606\n",
            "Number of Sentences Trained on: 25696\n",
            "Training loss epoch: 0.07779652063325021\n",
            "Training accuracy epoch: 1564.75\n",
            "Number of batches: 1607\n",
            "Number of Sentences Trained on: 25712\n",
            "Training loss epoch: 0.07774974461712579\n",
            "Training accuracy epoch: 1565.75\n",
            "Number of batches: 1608\n",
            "Number of Sentences Trained on: 25728\n",
            "Training loss epoch: 0.0777324302199378\n",
            "Training accuracy epoch: 1566.75\n",
            "Number of batches: 1609\n",
            "Number of Sentences Trained on: 25744\n",
            "Training loss epoch: 0.07775289946760308\n",
            "Training accuracy epoch: 1567.6875\n",
            "Number of batches: 1610\n",
            "Number of Sentences Trained on: 25760\n",
            "Training loss epoch: 0.07780660450350349\n",
            "Training accuracy epoch: 1568.625\n",
            "Number of batches: 1611\n",
            "Number of Sentences Trained on: 25776\n",
            "Training loss epoch: 0.07786891689041567\n",
            "Training accuracy epoch: 1569.5625\n",
            "Number of batches: 1612\n",
            "Number of Sentences Trained on: 25792\n",
            "Training loss epoch: 0.07783752719200705\n",
            "Training accuracy epoch: 1570.5625\n",
            "Number of batches: 1613\n",
            "Number of Sentences Trained on: 25808\n",
            "Training loss epoch: 0.07781234593965534\n",
            "Training accuracy epoch: 1571.5625\n",
            "Number of batches: 1614\n",
            "Number of Sentences Trained on: 25824\n",
            "Training loss epoch: 0.07780649697423578\n",
            "Training accuracy epoch: 1572.5625\n",
            "Number of batches: 1615\n",
            "Number of Sentences Trained on: 25840\n",
            "Training loss epoch: 0.07778046067484172\n",
            "Training accuracy epoch: 1573.5625\n",
            "Number of batches: 1616\n",
            "Number of Sentences Trained on: 25856\n",
            "Training loss epoch: 0.0777392028971875\n",
            "Training accuracy epoch: 1574.5625\n",
            "Number of batches: 1617\n",
            "Number of Sentences Trained on: 25872\n",
            "Training loss epoch: 0.07771458631846982\n",
            "Training accuracy epoch: 1575.5625\n",
            "Number of batches: 1618\n",
            "Number of Sentences Trained on: 25888\n",
            "Training loss epoch: 0.07777532600462848\n",
            "Training accuracy epoch: 1576.5\n",
            "Number of batches: 1619\n",
            "Number of Sentences Trained on: 25904\n",
            "Training loss epoch: 0.07774168635820208\n",
            "Training accuracy epoch: 1577.4375\n",
            "Number of batches: 1620\n",
            "Number of Sentences Trained on: 25920\n",
            "Training loss epoch: 0.07777677029875057\n",
            "Training accuracy epoch: 1578.4375\n",
            "Number of batches: 1621\n",
            "Number of Sentences Trained on: 25936\n",
            "Training loss epoch: 0.0777392600985517\n",
            "Training accuracy epoch: 1579.375\n",
            "Number of batches: 1622\n",
            "Number of Sentences Trained on: 25952\n",
            "Training loss epoch: 0.07774803526261641\n",
            "Training accuracy epoch: 1580.3125\n",
            "Number of batches: 1623\n",
            "Number of Sentences Trained on: 25968\n",
            "Training loss epoch: 0.077977338126783\n",
            "Training accuracy epoch: 1581.25\n",
            "Number of batches: 1624\n",
            "Number of Sentences Trained on: 25984\n",
            "Training loss epoch: 0.07797296225105842\n",
            "Training accuracy epoch: 1582.25\n",
            "Number of batches: 1625\n",
            "Number of Sentences Trained on: 26000\n",
            "Training loss epoch: 0.07795224618508556\n",
            "Training accuracy epoch: 1583.25\n",
            "Number of batches: 1626\n",
            "Number of Sentences Trained on: 26016\n",
            "Training loss epoch: 0.07791941551365042\n",
            "Training accuracy epoch: 1584.25\n",
            "Number of batches: 1627\n",
            "Number of Sentences Trained on: 26032\n",
            "Training loss epoch: 0.07787970954914339\n",
            "Training accuracy epoch: 1585.25\n",
            "Number of batches: 1628\n",
            "Number of Sentences Trained on: 26048\n",
            "Training loss epoch: 0.07786225437935533\n",
            "Training accuracy epoch: 1586.1875\n",
            "Number of batches: 1629\n",
            "Number of Sentences Trained on: 26064\n",
            "Training loss epoch: 0.07781618768976005\n",
            "Training accuracy epoch: 1587.1875\n",
            "Number of batches: 1630\n",
            "Number of Sentences Trained on: 26080\n",
            "Training loss epoch: 0.07777059794236806\n",
            "Training accuracy epoch: 1588.1875\n",
            "Number of batches: 1631\n",
            "Number of Sentences Trained on: 26096\n",
            "Training loss epoch: 0.07773720732101912\n",
            "Training accuracy epoch: 1589.1875\n",
            "Number of batches: 1632\n",
            "Number of Sentences Trained on: 26112\n",
            "Training loss epoch: 0.0778799695732545\n",
            "Training accuracy epoch: 1590.125\n",
            "Number of batches: 1633\n",
            "Number of Sentences Trained on: 26128\n",
            "Training loss epoch: 0.07812196333409811\n",
            "Training accuracy epoch: 1590.9375\n",
            "Number of batches: 1634\n",
            "Number of Sentences Trained on: 26144\n",
            "Training loss epoch: 0.07808404652320279\n",
            "Training accuracy epoch: 1591.9375\n",
            "Number of batches: 1635\n",
            "Number of Sentences Trained on: 26160\n",
            "Training loss epoch: 0.07805300476500229\n",
            "Training accuracy epoch: 1592.9375\n",
            "Number of batches: 1636\n",
            "Number of Sentences Trained on: 26176\n",
            "Training loss epoch: 0.07800793906367069\n",
            "Training accuracy epoch: 1593.9375\n",
            "Number of batches: 1637\n",
            "Number of Sentences Trained on: 26192\n",
            "Training loss epoch: 0.07821385462434552\n",
            "Training accuracy epoch: 1594.875\n",
            "Number of batches: 1638\n",
            "Number of Sentences Trained on: 26208\n",
            "Training loss epoch: 0.07835088967633107\n",
            "Training accuracy epoch: 1595.8125\n",
            "Number of batches: 1639\n",
            "Number of Sentences Trained on: 26224\n",
            "Training loss epoch: 0.07834166432473533\n",
            "Training accuracy epoch: 1596.8125\n",
            "Number of batches: 1640\n",
            "Number of Sentences Trained on: 26240\n",
            "Training loss epoch: 0.07847202031761859\n",
            "Training accuracy epoch: 1597.75\n",
            "Number of batches: 1641\n",
            "Number of Sentences Trained on: 26256\n",
            "Training loss epoch: 0.07842863586714735\n",
            "Training accuracy epoch: 1598.75\n",
            "Number of batches: 1642\n",
            "Number of Sentences Trained on: 26272\n",
            "Training loss epoch: 0.0784979927219042\n",
            "Training accuracy epoch: 1599.6875\n",
            "Number of batches: 1643\n",
            "Number of Sentences Trained on: 26288\n",
            "Training loss epoch: 0.07850441064371105\n",
            "Training accuracy epoch: 1600.625\n",
            "Number of batches: 1644\n",
            "Number of Sentences Trained on: 26304\n",
            "Training loss epoch: 0.07846679608463196\n",
            "Training accuracy epoch: 1601.625\n",
            "Number of batches: 1645\n",
            "Number of Sentences Trained on: 26320\n",
            "Training loss epoch: 0.07855507019647459\n",
            "Training accuracy epoch: 1602.5625\n",
            "Number of batches: 1646\n",
            "Number of Sentences Trained on: 26336\n",
            "Training loss epoch: 0.07873096531701984\n",
            "Training accuracy epoch: 1603.4375\n",
            "Number of batches: 1647\n",
            "Number of Sentences Trained on: 26352\n",
            "Training loss epoch: 0.07873810793596896\n",
            "Training accuracy epoch: 1604.4375\n",
            "Number of batches: 1648\n",
            "Number of Sentences Trained on: 26368\n",
            "Training loss epoch: 0.07870128310609621\n",
            "Training accuracy epoch: 1605.4375\n",
            "Number of batches: 1649\n",
            "Number of Sentences Trained on: 26384\n",
            "Training loss epoch: 0.07865767396787167\n",
            "Training accuracy epoch: 1606.4375\n",
            "Number of batches: 1650\n",
            "Number of Sentences Trained on: 26400\n",
            "Training loss epoch: 0.0786192578230687\n",
            "Training accuracy epoch: 1607.4375\n",
            "Number of batches: 1651\n",
            "Number of Sentences Trained on: 26416\n",
            "Training loss epoch: 0.07857430904208934\n",
            "Training accuracy epoch: 1608.4375\n",
            "Number of batches: 1652\n",
            "Number of Sentences Trained on: 26432\n",
            "Training loss epoch: 0.07853554156638107\n",
            "Training accuracy epoch: 1609.4375\n",
            "Number of batches: 1653\n",
            "Number of Sentences Trained on: 26448\n",
            "Training loss epoch: 0.07849215038182052\n",
            "Training accuracy epoch: 1610.4375\n",
            "Number of batches: 1654\n",
            "Number of Sentences Trained on: 26464\n",
            "Training loss epoch: 0.0784458009957855\n",
            "Training accuracy epoch: 1611.4375\n",
            "Number of batches: 1655\n",
            "Number of Sentences Trained on: 26480\n",
            "Training loss epoch: 0.0786900144835909\n",
            "Training accuracy epoch: 1612.375\n",
            "Number of batches: 1656\n",
            "Number of Sentences Trained on: 26496\n",
            "Training loss epoch: 0.07875566862678612\n",
            "Training accuracy epoch: 1613.25\n",
            "Number of batches: 1657\n",
            "Number of Sentences Trained on: 26512\n",
            "Training loss epoch: 0.07871332690912813\n",
            "Training accuracy epoch: 1614.25\n",
            "Number of batches: 1658\n",
            "Number of Sentences Trained on: 26528\n",
            "Training loss epoch: 0.07868334268792863\n",
            "Training accuracy epoch: 1615.25\n",
            "Number of batches: 1659\n",
            "Number of Sentences Trained on: 26544\n",
            "Training loss epoch: 0.0786942502055759\n",
            "Training accuracy epoch: 1616.1875\n",
            "Number of batches: 1660\n",
            "Number of Sentences Trained on: 26560\n",
            "Training loss epoch: 0.07868868036741687\n",
            "Training accuracy epoch: 1617.1875\n",
            "Number of batches: 1661\n",
            "Number of Sentences Trained on: 26576\n",
            "Training loss epoch: 0.0786442940520221\n",
            "Training accuracy epoch: 1618.1875\n",
            "Number of batches: 1662\n",
            "Number of Sentences Trained on: 26592\n",
            "Training loss epoch: 0.0786072516415241\n",
            "Training accuracy epoch: 1619.1875\n",
            "Number of batches: 1663\n",
            "Number of Sentences Trained on: 26608\n",
            "Training loss epoch: 0.07856170853313835\n",
            "Training accuracy epoch: 1620.1875\n",
            "Number of batches: 1664\n",
            "Number of Sentences Trained on: 26624\n",
            "Training loss epoch: 0.07853988618742012\n",
            "Training accuracy epoch: 1621.1875\n",
            "Number of batches: 1665\n",
            "Number of Sentences Trained on: 26640\n",
            "Training loss epoch: 0.07860823238874959\n",
            "Training accuracy epoch: 1622.125\n",
            "Number of batches: 1666\n",
            "Number of Sentences Trained on: 26656\n",
            "Training loss epoch: 0.078948452007779\n",
            "Training accuracy epoch: 1623.0\n",
            "Number of batches: 1667\n",
            "Number of Sentences Trained on: 26672\n",
            "Training loss epoch: 0.07902373605164824\n",
            "Training accuracy epoch: 1623.9375\n",
            "Number of batches: 1668\n",
            "Number of Sentences Trained on: 26688\n",
            "Training loss epoch: 0.07908329024179303\n",
            "Training accuracy epoch: 1624.875\n",
            "Number of batches: 1669\n",
            "Number of Sentences Trained on: 26704\n",
            "Training loss epoch: 0.07909811156337264\n",
            "Training accuracy epoch: 1625.875\n",
            "Number of batches: 1670\n",
            "Number of Sentences Trained on: 26720\n",
            "Training loss epoch: 0.07909578354531448\n",
            "Training accuracy epoch: 1626.875\n",
            "Number of batches: 1671\n",
            "Number of Sentences Trained on: 26736\n",
            "Training loss epoch: 0.07913657945621373\n",
            "Training accuracy epoch: 1627.8125\n",
            "Number of batches: 1672\n",
            "Number of Sentences Trained on: 26752\n",
            "Training loss epoch: 0.07910674422347455\n",
            "Training accuracy epoch: 1628.8125\n",
            "Number of batches: 1673\n",
            "Number of Sentences Trained on: 26768\n",
            "Training loss epoch: 0.07907331017152477\n",
            "Training accuracy epoch: 1629.8125\n",
            "Number of batches: 1674\n",
            "Number of Sentences Trained on: 26784\n",
            "Training loss epoch: 0.07903016334314442\n",
            "Training accuracy epoch: 1630.8125\n",
            "Number of batches: 1675\n",
            "Number of Sentences Trained on: 26800\n",
            "Training loss epoch: 0.07902477296825478\n",
            "Training accuracy epoch: 1631.75\n",
            "Number of batches: 1676\n",
            "Number of Sentences Trained on: 26816\n",
            "Training loss epoch: 0.0789907909468638\n",
            "Training accuracy epoch: 1632.75\n",
            "Number of batches: 1677\n",
            "Number of Sentences Trained on: 26832\n",
            "Training loss epoch: 0.0790710066749149\n",
            "Training accuracy epoch: 1633.6875\n",
            "Number of batches: 1678\n",
            "Number of Sentences Trained on: 26848\n",
            "Training loss epoch: 0.079065553875575\n",
            "Training accuracy epoch: 1634.6875\n",
            "Number of batches: 1679\n",
            "Number of Sentences Trained on: 26864\n",
            "Training loss epoch: 0.07911255779242007\n",
            "Training accuracy epoch: 1635.625\n",
            "Number of batches: 1680\n",
            "Number of Sentences Trained on: 26880\n",
            "Training loss epoch: 0.07908840629545784\n",
            "Training accuracy epoch: 1636.625\n",
            "Number of batches: 1681\n",
            "Number of Sentences Trained on: 26896\n",
            "Training loss epoch: 0.07930733375615004\n",
            "Training accuracy epoch: 1637.5625\n",
            "Number of batches: 1682\n",
            "Number of Sentences Trained on: 26912\n",
            "Training loss epoch: 0.07934198802168203\n",
            "Training accuracy epoch: 1638.5625\n",
            "Number of batches: 1683\n",
            "Number of Sentences Trained on: 26928\n",
            "Training loss epoch: 0.0794238932480345\n",
            "Training accuracy epoch: 1639.5\n",
            "Number of batches: 1684\n",
            "Number of Sentences Trained on: 26944\n",
            "Training loss epoch: 0.0794217775683423\n",
            "Training accuracy epoch: 1640.4375\n",
            "Number of batches: 1685\n",
            "Number of Sentences Trained on: 26960\n",
            "Training loss epoch: 0.07957240158783077\n",
            "Training accuracy epoch: 1641.3125\n",
            "Number of batches: 1686\n",
            "Number of Sentences Trained on: 26976\n",
            "Training loss epoch: 0.07969941808351826\n",
            "Training accuracy epoch: 1642.25\n",
            "Number of batches: 1687\n",
            "Number of Sentences Trained on: 26992\n",
            "Training loss epoch: 0.07965856788609818\n",
            "Training accuracy epoch: 1643.25\n",
            "Number of batches: 1688\n",
            "Number of Sentences Trained on: 27008\n",
            "Training loss epoch: 0.07969879554257571\n",
            "Training accuracy epoch: 1644.1875\n",
            "Number of batches: 1689\n",
            "Number of Sentences Trained on: 27024\n",
            "Training loss epoch: 0.07965351553417166\n",
            "Training accuracy epoch: 1645.1875\n",
            "Number of batches: 1690\n",
            "Number of Sentences Trained on: 27040\n",
            "Training loss epoch: 0.07972198833546151\n",
            "Training accuracy epoch: 1646.0625\n",
            "Number of batches: 1691\n",
            "Number of Sentences Trained on: 27056\n",
            "Training loss epoch: 0.0796753408756399\n",
            "Training accuracy epoch: 1647.0625\n",
            "Number of batches: 1692\n",
            "Number of Sentences Trained on: 27072\n",
            "Training loss epoch: 0.07963594418314805\n",
            "Training accuracy epoch: 1648.0625\n",
            "Number of batches: 1693\n",
            "Number of Sentences Trained on: 27088\n",
            "Training loss epoch: 0.07966846121080154\n",
            "Training accuracy epoch: 1649.0\n",
            "Number of batches: 1694\n",
            "Number of Sentences Trained on: 27104\n",
            "Training loss epoch: 0.07964011447754507\n",
            "Training accuracy epoch: 1650.0\n",
            "Number of batches: 1695\n",
            "Number of Sentences Trained on: 27120\n",
            "Training loss epoch: 0.07960427523448743\n",
            "Training accuracy epoch: 1651.0\n",
            "Number of batches: 1696\n",
            "Number of Sentences Trained on: 27136\n",
            "Training loss epoch: 0.07958099909402053\n",
            "Training accuracy epoch: 1652.0\n",
            "Number of batches: 1697\n",
            "Number of Sentences Trained on: 27152\n",
            "Training loss epoch: 0.0796217288429531\n",
            "Training accuracy epoch: 1652.9375\n",
            "Number of batches: 1698\n",
            "Number of Sentences Trained on: 27168\n",
            "Training loss epoch: 0.07958269342509117\n",
            "Training accuracy epoch: 1653.9375\n",
            "Number of batches: 1699\n",
            "Number of Sentences Trained on: 27184\n",
            "Training loss epoch: 0.07954906173853327\n",
            "Training accuracy epoch: 1654.9375\n",
            "Number of batches: 1700\n",
            "Number of Sentences Trained on: 27200\n",
            "Training loss per 100 training steps: 0.07953194548236082\n",
            "Training loss epoch: 0.07953194548236082\n",
            "Training accuracy epoch: 1655.9375\n",
            "Number of batches: 1701\n",
            "Number of Sentences Trained on: 27216\n",
            "Training loss epoch: 0.07954270100381708\n",
            "Training accuracy epoch: 1656.9375\n",
            "Number of batches: 1702\n",
            "Number of Sentences Trained on: 27232\n",
            "Training loss epoch: 0.07951338782491799\n",
            "Training accuracy epoch: 1657.9375\n",
            "Number of batches: 1703\n",
            "Number of Sentences Trained on: 27248\n",
            "Training loss epoch: 0.07948967129653202\n",
            "Training accuracy epoch: 1658.9375\n",
            "Number of batches: 1704\n",
            "Number of Sentences Trained on: 27264\n",
            "Training loss epoch: 0.07944559509686025\n",
            "Training accuracy epoch: 1659.9375\n",
            "Number of batches: 1705\n",
            "Number of Sentences Trained on: 27280\n",
            "Training loss epoch: 0.07953316362551563\n",
            "Training accuracy epoch: 1660.875\n",
            "Number of batches: 1706\n",
            "Number of Sentences Trained on: 27296\n",
            "Training loss epoch: 0.07949760977016021\n",
            "Training accuracy epoch: 1661.875\n",
            "Number of batches: 1707\n",
            "Number of Sentences Trained on: 27312\n",
            "Training loss epoch: 0.07957852162473981\n",
            "Training accuracy epoch: 1662.8125\n",
            "Number of batches: 1708\n",
            "Number of Sentences Trained on: 27328\n",
            "Training loss epoch: 0.07956300950560442\n",
            "Training accuracy epoch: 1663.75\n",
            "Number of batches: 1709\n",
            "Number of Sentences Trained on: 27344\n",
            "Training loss epoch: 0.07952276309062005\n",
            "Training accuracy epoch: 1664.75\n",
            "Number of batches: 1710\n",
            "Number of Sentences Trained on: 27360\n",
            "Training loss epoch: 0.0794810633277168\n",
            "Training accuracy epoch: 1665.75\n",
            "Number of batches: 1711\n",
            "Number of Sentences Trained on: 27376\n",
            "Training loss epoch: 0.0794901000605941\n",
            "Training accuracy epoch: 1666.625\n",
            "Number of batches: 1712\n",
            "Number of Sentences Trained on: 27392\n",
            "Training loss epoch: 0.0795104807753028\n",
            "Training accuracy epoch: 1667.5625\n",
            "Number of batches: 1713\n",
            "Number of Sentences Trained on: 27408\n",
            "Training loss epoch: 0.07948557415126012\n",
            "Training accuracy epoch: 1668.5625\n",
            "Number of batches: 1714\n",
            "Number of Sentences Trained on: 27424\n",
            "Training loss epoch: 0.07944079866049385\n",
            "Training accuracy epoch: 1669.5625\n",
            "Number of batches: 1715\n",
            "Number of Sentences Trained on: 27440\n",
            "Training loss epoch: 0.07940307687835386\n",
            "Training accuracy epoch: 1670.5625\n",
            "Number of batches: 1716\n",
            "Number of Sentences Trained on: 27456\n",
            "Training loss epoch: 0.07935787659297706\n",
            "Training accuracy epoch: 1671.5625\n",
            "Number of batches: 1717\n",
            "Number of Sentences Trained on: 27472\n",
            "Training loss epoch: 0.07936068068318976\n",
            "Training accuracy epoch: 1672.5625\n",
            "Number of batches: 1718\n",
            "Number of Sentences Trained on: 27488\n",
            "Training loss epoch: 0.0794824595680985\n",
            "Training accuracy epoch: 1673.375\n",
            "Number of batches: 1719\n",
            "Number of Sentences Trained on: 27504\n",
            "Training loss epoch: 0.07960372468448842\n",
            "Training accuracy epoch: 1674.1875\n",
            "Number of batches: 1720\n",
            "Number of Sentences Trained on: 27520\n",
            "Training loss epoch: 0.0795692942849622\n",
            "Training accuracy epoch: 1675.1875\n",
            "Number of batches: 1721\n",
            "Number of Sentences Trained on: 27536\n",
            "Training loss epoch: 0.07954523156883596\n",
            "Training accuracy epoch: 1676.1875\n",
            "Number of batches: 1722\n",
            "Number of Sentences Trained on: 27552\n",
            "Training loss epoch: 0.07950652920305137\n",
            "Training accuracy epoch: 1677.1875\n",
            "Number of batches: 1723\n",
            "Number of Sentences Trained on: 27568\n",
            "Training loss epoch: 0.07946749346575815\n",
            "Training accuracy epoch: 1678.1875\n",
            "Number of batches: 1724\n",
            "Number of Sentences Trained on: 27584\n",
            "Training loss epoch: 0.07959285358137425\n",
            "Training accuracy epoch: 1679.0625\n",
            "Number of batches: 1725\n",
            "Number of Sentences Trained on: 27600\n",
            "Training loss epoch: 0.07959512180413283\n",
            "Training accuracy epoch: 1680.0625\n",
            "Number of batches: 1726\n",
            "Number of Sentences Trained on: 27616\n",
            "Training loss epoch: 0.07955683855031219\n",
            "Training accuracy epoch: 1681.0625\n",
            "Number of batches: 1727\n",
            "Number of Sentences Trained on: 27632\n",
            "Training loss epoch: 0.07953221526051696\n",
            "Training accuracy epoch: 1682.0625\n",
            "Number of batches: 1728\n",
            "Number of Sentences Trained on: 27648\n",
            "Training loss epoch: 0.07949513912951602\n",
            "Training accuracy epoch: 1683.0625\n",
            "Number of batches: 1729\n",
            "Number of Sentences Trained on: 27664\n",
            "Training loss epoch: 0.07946269891416248\n",
            "Training accuracy epoch: 1684.0625\n",
            "Number of batches: 1730\n",
            "Number of Sentences Trained on: 27680\n",
            "Training loss epoch: 0.07943246152087122\n",
            "Training accuracy epoch: 1685.0625\n",
            "Number of batches: 1731\n",
            "Number of Sentences Trained on: 27696\n",
            "Training loss epoch: 0.07938845758334753\n",
            "Training accuracy epoch: 1686.0625\n",
            "Number of batches: 1732\n",
            "Number of Sentences Trained on: 27712\n",
            "Training loss epoch: 0.07935822178202652\n",
            "Training accuracy epoch: 1687.0625\n",
            "Number of batches: 1733\n",
            "Number of Sentences Trained on: 27728\n",
            "Training loss epoch: 0.07931277573472488\n",
            "Training accuracy epoch: 1688.0625\n",
            "Number of batches: 1734\n",
            "Number of Sentences Trained on: 27744\n",
            "Training loss epoch: 0.07926790913550508\n",
            "Training accuracy epoch: 1689.0625\n",
            "Number of batches: 1735\n",
            "Number of Sentences Trained on: 27760\n",
            "Training loss epoch: 0.0792326000168208\n",
            "Training accuracy epoch: 1690.0625\n",
            "Number of batches: 1736\n",
            "Number of Sentences Trained on: 27776\n",
            "Training loss epoch: 0.079188223927057\n",
            "Training accuracy epoch: 1691.0625\n",
            "Number of batches: 1737\n",
            "Number of Sentences Trained on: 27792\n",
            "Training loss epoch: 0.07915622899589791\n",
            "Training accuracy epoch: 1692.0625\n",
            "Number of batches: 1738\n",
            "Number of Sentences Trained on: 27808\n",
            "Training loss epoch: 0.07918433620531984\n",
            "Training accuracy epoch: 1693.0\n",
            "Number of batches: 1739\n",
            "Number of Sentences Trained on: 27824\n",
            "Training loss epoch: 0.07927533984175902\n",
            "Training accuracy epoch: 1693.875\n",
            "Number of batches: 1740\n",
            "Number of Sentences Trained on: 27840\n",
            "Training loss epoch: 0.07923040957778726\n",
            "Training accuracy epoch: 1694.875\n",
            "Number of batches: 1741\n",
            "Number of Sentences Trained on: 27856\n",
            "Training loss epoch: 0.07918630336548897\n",
            "Training accuracy epoch: 1695.875\n",
            "Number of batches: 1742\n",
            "Number of Sentences Trained on: 27872\n",
            "Training loss epoch: 0.0791450763755988\n",
            "Training accuracy epoch: 1696.875\n",
            "Number of batches: 1743\n",
            "Number of Sentences Trained on: 27888\n",
            "Training loss epoch: 0.07925470977083557\n",
            "Training accuracy epoch: 1697.8125\n",
            "Number of batches: 1744\n",
            "Number of Sentences Trained on: 27904\n",
            "Training loss epoch: 0.07928138757556515\n",
            "Training accuracy epoch: 1698.75\n",
            "Number of batches: 1745\n",
            "Number of Sentences Trained on: 27920\n",
            "Training loss epoch: 0.07929508901101642\n",
            "Training accuracy epoch: 1699.6875\n",
            "Number of batches: 1746\n",
            "Number of Sentences Trained on: 27936\n",
            "Training loss epoch: 0.07925494445085568\n",
            "Training accuracy epoch: 1700.6875\n",
            "Number of batches: 1747\n",
            "Number of Sentences Trained on: 27952\n",
            "Training loss epoch: 0.07922364353954255\n",
            "Training accuracy epoch: 1701.625\n",
            "Number of batches: 1748\n",
            "Number of Sentences Trained on: 27968\n",
            "Training loss epoch: 0.07921063807212748\n",
            "Training accuracy epoch: 1702.625\n",
            "Number of batches: 1749\n",
            "Number of Sentences Trained on: 27984\n",
            "Training loss epoch: 0.07917704864463303\n",
            "Training accuracy epoch: 1703.625\n",
            "Number of batches: 1750\n",
            "Number of Sentences Trained on: 28000\n",
            "Training loss epoch: 0.07917845941024437\n",
            "Training accuracy epoch: 1704.5\n",
            "Number of batches: 1751\n",
            "Number of Sentences Trained on: 28016\n",
            "Training loss epoch: 0.07924559682620076\n",
            "Training accuracy epoch: 1705.4375\n",
            "Number of batches: 1752\n",
            "Number of Sentences Trained on: 28032\n",
            "Training loss epoch: 0.07923043889428343\n",
            "Training accuracy epoch: 1706.4375\n",
            "Number of batches: 1753\n",
            "Number of Sentences Trained on: 28048\n",
            "Training loss epoch: 0.07931972886313232\n",
            "Training accuracy epoch: 1707.3125\n",
            "Number of batches: 1754\n",
            "Number of Sentences Trained on: 28064\n",
            "Training loss epoch: 0.07931077232366451\n",
            "Training accuracy epoch: 1708.3125\n",
            "Number of batches: 1755\n",
            "Number of Sentences Trained on: 28080\n",
            "Training loss epoch: 0.07938088703208647\n",
            "Training accuracy epoch: 1709.25\n",
            "Number of batches: 1756\n",
            "Number of Sentences Trained on: 28096\n",
            "Training loss epoch: 0.07946909884341455\n",
            "Training accuracy epoch: 1710.1875\n",
            "Number of batches: 1757\n",
            "Number of Sentences Trained on: 28112\n",
            "Training loss epoch: 0.07944530410092074\n",
            "Training accuracy epoch: 1711.1875\n",
            "Number of batches: 1758\n",
            "Number of Sentences Trained on: 28128\n",
            "Training loss epoch: 0.07945218514534248\n",
            "Training accuracy epoch: 1712.125\n",
            "Number of batches: 1759\n",
            "Number of Sentences Trained on: 28144\n",
            "Training loss epoch: 0.07943281793776061\n",
            "Training accuracy epoch: 1713.0625\n",
            "Number of batches: 1760\n",
            "Number of Sentences Trained on: 28160\n",
            "Training loss epoch: 0.0794028571927924\n",
            "Training accuracy epoch: 1714.0625\n",
            "Number of batches: 1761\n",
            "Number of Sentences Trained on: 28176\n",
            "Training loss epoch: 0.07940286568320204\n",
            "Training accuracy epoch: 1715.0625\n",
            "Number of batches: 1762\n",
            "Number of Sentences Trained on: 28192\n",
            "Training loss epoch: 0.07936130477526168\n",
            "Training accuracy epoch: 1716.0625\n",
            "Number of batches: 1763\n",
            "Number of Sentences Trained on: 28208\n",
            "Training loss epoch: 0.07940984598356401\n",
            "Training accuracy epoch: 1717.0\n",
            "Number of batches: 1764\n",
            "Number of Sentences Trained on: 28224\n",
            "Training loss epoch: 0.07947387837073713\n",
            "Training accuracy epoch: 1717.9375\n",
            "Number of batches: 1765\n",
            "Number of Sentences Trained on: 28240\n",
            "Training loss epoch: 0.0794563080859153\n",
            "Training accuracy epoch: 1718.875\n",
            "Number of batches: 1766\n",
            "Number of Sentences Trained on: 28256\n",
            "Training loss epoch: 0.07954882853919755\n",
            "Training accuracy epoch: 1719.75\n",
            "Number of batches: 1767\n",
            "Number of Sentences Trained on: 28272\n",
            "Training loss epoch: 0.07959150061426076\n",
            "Training accuracy epoch: 1720.6875\n",
            "Number of batches: 1768\n",
            "Number of Sentences Trained on: 28288\n",
            "Training loss epoch: 0.07955622378875393\n",
            "Training accuracy epoch: 1721.6875\n",
            "Number of batches: 1769\n",
            "Number of Sentences Trained on: 28304\n",
            "Training loss epoch: 0.0795742896425447\n",
            "Training accuracy epoch: 1722.625\n",
            "Number of batches: 1770\n",
            "Number of Sentences Trained on: 28320\n",
            "Training loss epoch: 0.07958672523218442\n",
            "Training accuracy epoch: 1723.625\n",
            "Number of batches: 1771\n",
            "Number of Sentences Trained on: 28336\n",
            "Training loss epoch: 0.07956535925039311\n",
            "Training accuracy epoch: 1724.625\n",
            "Number of batches: 1772\n",
            "Number of Sentences Trained on: 28352\n",
            "Training loss epoch: 0.07960788714029386\n",
            "Training accuracy epoch: 1725.5625\n",
            "Number of batches: 1773\n",
            "Number of Sentences Trained on: 28368\n",
            "Training loss epoch: 0.07959997372600638\n",
            "Training accuracy epoch: 1726.5\n",
            "Number of batches: 1774\n",
            "Number of Sentences Trained on: 28384\n",
            "Training loss epoch: 0.07956396924599048\n",
            "Training accuracy epoch: 1727.5\n",
            "Number of batches: 1775\n",
            "Number of Sentences Trained on: 28400\n",
            "Training loss epoch: 0.07953499988704878\n",
            "Training accuracy epoch: 1728.5\n",
            "Number of batches: 1776\n",
            "Number of Sentences Trained on: 28416\n",
            "Training loss epoch: 0.07967192202430891\n",
            "Training accuracy epoch: 1729.3125\n",
            "Number of batches: 1777\n",
            "Number of Sentences Trained on: 28432\n",
            "Training loss epoch: 0.079687005341892\n",
            "Training accuracy epoch: 1730.25\n",
            "Number of batches: 1778\n",
            "Number of Sentences Trained on: 28448\n",
            "Training loss epoch: 0.07964272376967156\n",
            "Training accuracy epoch: 1731.25\n",
            "Number of batches: 1779\n",
            "Number of Sentences Trained on: 28464\n",
            "Training loss epoch: 0.07978046664870357\n",
            "Training accuracy epoch: 1732.125\n",
            "Number of batches: 1780\n",
            "Number of Sentences Trained on: 28480\n",
            "Training loss epoch: 0.07980455895249496\n",
            "Training accuracy epoch: 1733.125\n",
            "Number of batches: 1781\n",
            "Number of Sentences Trained on: 28496\n",
            "Training loss epoch: 0.07976940114133221\n",
            "Training accuracy epoch: 1734.125\n",
            "Number of batches: 1782\n",
            "Number of Sentences Trained on: 28512\n",
            "Training loss epoch: 0.07972588475280211\n",
            "Training accuracy epoch: 1735.125\n",
            "Number of batches: 1783\n",
            "Number of Sentences Trained on: 28528\n",
            "Training loss epoch: 0.0796998733200083\n",
            "Training accuracy epoch: 1736.125\n",
            "Number of batches: 1784\n",
            "Number of Sentences Trained on: 28544\n",
            "Training loss epoch: 0.07976835120173566\n",
            "Training accuracy epoch: 1737.0625\n",
            "Number of batches: 1785\n",
            "Number of Sentences Trained on: 28560\n",
            "Training loss epoch: 0.07976556700042724\n",
            "Training accuracy epoch: 1738.0\n",
            "Number of batches: 1786\n",
            "Number of Sentences Trained on: 28576\n",
            "Training loss epoch: 0.07972521761409945\n",
            "Training accuracy epoch: 1739.0\n",
            "Number of batches: 1787\n",
            "Number of Sentences Trained on: 28592\n",
            "Training loss epoch: 0.07970800195974352\n",
            "Training accuracy epoch: 1740.0\n",
            "Number of batches: 1788\n",
            "Number of Sentences Trained on: 28608\n",
            "Training loss epoch: 0.0797913993988505\n",
            "Training accuracy epoch: 1740.875\n",
            "Number of batches: 1789\n",
            "Number of Sentences Trained on: 28624\n",
            "Training loss epoch: 0.07981272309229322\n",
            "Training accuracy epoch: 1741.875\n",
            "Number of batches: 1790\n",
            "Number of Sentences Trained on: 28640\n",
            "Training loss epoch: 0.07978029766588082\n",
            "Training accuracy epoch: 1742.875\n",
            "Number of batches: 1791\n",
            "Number of Sentences Trained on: 28656\n",
            "Training loss epoch: 0.0797730726429141\n",
            "Training accuracy epoch: 1743.875\n",
            "Number of batches: 1792\n",
            "Number of Sentences Trained on: 28672\n",
            "Training loss epoch: 0.0797607468309358\n",
            "Training accuracy epoch: 1744.8125\n",
            "Number of batches: 1793\n",
            "Number of Sentences Trained on: 28688\n",
            "Training loss epoch: 0.07974872929061264\n",
            "Training accuracy epoch: 1745.8125\n",
            "Number of batches: 1794\n",
            "Number of Sentences Trained on: 28704\n",
            "Training loss epoch: 0.07970852800192568\n",
            "Training accuracy epoch: 1746.8125\n",
            "Number of batches: 1795\n",
            "Number of Sentences Trained on: 28720\n",
            "Training loss epoch: 0.0797294775057406\n",
            "Training accuracy epoch: 1747.75\n",
            "Number of batches: 1796\n",
            "Number of Sentences Trained on: 28736\n",
            "Training loss epoch: 0.07969462873211582\n",
            "Training accuracy epoch: 1748.75\n",
            "Number of batches: 1797\n",
            "Number of Sentences Trained on: 28752\n",
            "Training loss epoch: 0.07967210677894285\n",
            "Training accuracy epoch: 1749.75\n",
            "Number of batches: 1798\n",
            "Number of Sentences Trained on: 28768\n",
            "Training loss epoch: 0.07964022727213707\n",
            "Training accuracy epoch: 1750.75\n",
            "Number of batches: 1799\n",
            "Number of Sentences Trained on: 28784\n",
            "Training loss epoch: 0.07966564663997916\n",
            "Training accuracy epoch: 1751.6875\n",
            "Number of batches: 1800\n",
            "Number of Sentences Trained on: 28800\n",
            "Training loss per 100 training steps: 0.07963113462464066\n",
            "Training loss epoch: 0.07963113462464066\n",
            "Training accuracy epoch: 1752.6875\n",
            "Number of batches: 1801\n",
            "Number of Sentences Trained on: 28816\n",
            "Training loss epoch: 0.07960116858704268\n",
            "Training accuracy epoch: 1753.6875\n",
            "Number of batches: 1802\n",
            "Number of Sentences Trained on: 28832\n",
            "Training loss epoch: 0.07955882244854011\n",
            "Training accuracy epoch: 1754.6875\n",
            "Number of batches: 1803\n",
            "Number of Sentences Trained on: 28848\n",
            "Training loss epoch: 0.07951665316176193\n",
            "Training accuracy epoch: 1755.6875\n",
            "Number of batches: 1804\n",
            "Number of Sentences Trained on: 28864\n",
            "Training loss epoch: 0.07952590720172406\n",
            "Training accuracy epoch: 1756.6875\n",
            "Number of batches: 1805\n",
            "Number of Sentences Trained on: 28880\n",
            "Training loss epoch: 0.07952538659928714\n",
            "Training accuracy epoch: 1757.6875\n",
            "Number of batches: 1806\n",
            "Number of Sentences Trained on: 28896\n",
            "Training loss epoch: 0.07948769732417658\n",
            "Training accuracy epoch: 1758.6875\n",
            "Number of batches: 1807\n",
            "Number of Sentences Trained on: 28912\n",
            "Training loss epoch: 0.07944570140826139\n",
            "Training accuracy epoch: 1759.6875\n",
            "Number of batches: 1808\n",
            "Number of Sentences Trained on: 28928\n",
            "Training loss epoch: 0.07941662605375738\n",
            "Training accuracy epoch: 1760.6875\n",
            "Number of batches: 1809\n",
            "Number of Sentences Trained on: 28944\n",
            "Training loss epoch: 0.07937485342679702\n",
            "Training accuracy epoch: 1761.6875\n",
            "Number of batches: 1810\n",
            "Number of Sentences Trained on: 28960\n",
            "Training loss epoch: 0.0793317398040478\n",
            "Training accuracy epoch: 1762.6875\n",
            "Number of batches: 1811\n",
            "Number of Sentences Trained on: 28976\n",
            "Training loss epoch: 0.07931440299018412\n",
            "Training accuracy epoch: 1763.6875\n",
            "Number of batches: 1812\n",
            "Number of Sentences Trained on: 28992\n",
            "Training loss epoch: 0.0793278177134248\n",
            "Training accuracy epoch: 1764.6875\n",
            "Number of batches: 1813\n",
            "Number of Sentences Trained on: 29008\n",
            "Training loss epoch: 0.07934429326583772\n",
            "Training accuracy epoch: 1765.625\n",
            "Number of batches: 1814\n",
            "Number of Sentences Trained on: 29024\n",
            "Training loss epoch: 0.07934616510713019\n",
            "Training accuracy epoch: 1766.625\n",
            "Number of batches: 1815\n",
            "Number of Sentences Trained on: 29040\n",
            "Training loss epoch: 0.07938148794867345\n",
            "Training accuracy epoch: 1767.625\n",
            "Number of batches: 1816\n",
            "Number of Sentences Trained on: 29056\n",
            "Training loss epoch: 0.07940084367563338\n",
            "Training accuracy epoch: 1768.5\n",
            "Number of batches: 1817\n",
            "Number of Sentences Trained on: 29072\n",
            "Training loss epoch: 0.0794525419477184\n",
            "Training accuracy epoch: 1769.4375\n",
            "Number of batches: 1818\n",
            "Number of Sentences Trained on: 29088\n",
            "Training loss epoch: 0.07949609385717646\n",
            "Training accuracy epoch: 1770.3125\n",
            "Number of batches: 1819\n",
            "Number of Sentences Trained on: 29104\n",
            "Training loss epoch: 0.07958277023116263\n",
            "Training accuracy epoch: 1771.25\n",
            "Number of batches: 1820\n",
            "Number of Sentences Trained on: 29120\n",
            "Training loss epoch: 0.07954087087387239\n",
            "Training accuracy epoch: 1772.25\n",
            "Number of batches: 1821\n",
            "Number of Sentences Trained on: 29136\n",
            "Training loss epoch: 0.07950036649387568\n",
            "Training accuracy epoch: 1773.25\n",
            "Number of batches: 1822\n",
            "Number of Sentences Trained on: 29152\n",
            "Training loss epoch: 0.07952470835577045\n",
            "Training accuracy epoch: 1774.25\n",
            "Number of batches: 1823\n",
            "Number of Sentences Trained on: 29168\n",
            "Training loss epoch: 0.0794830947384789\n",
            "Training accuracy epoch: 1775.25\n",
            "Number of batches: 1824\n",
            "Number of Sentences Trained on: 29184\n",
            "Training loss epoch: 0.07945123977852309\n",
            "Training accuracy epoch: 1776.25\n",
            "Number of batches: 1825\n",
            "Number of Sentences Trained on: 29200\n",
            "Training loss epoch: 0.07946887911014121\n",
            "Training accuracy epoch: 1777.1875\n",
            "Number of batches: 1826\n",
            "Number of Sentences Trained on: 29216\n",
            "Training loss epoch: 0.07946035494288053\n",
            "Training accuracy epoch: 1778.1875\n",
            "Number of batches: 1827\n",
            "Number of Sentences Trained on: 29232\n",
            "Training loss epoch: 0.07941881174989225\n",
            "Training accuracy epoch: 1779.1875\n",
            "Number of batches: 1828\n",
            "Number of Sentences Trained on: 29248\n",
            "Training loss epoch: 0.07951979963037928\n",
            "Training accuracy epoch: 1780.125\n",
            "Number of batches: 1829\n",
            "Number of Sentences Trained on: 29264\n",
            "Training loss epoch: 0.07954326590640494\n",
            "Training accuracy epoch: 1781.0625\n",
            "Number of batches: 1830\n",
            "Number of Sentences Trained on: 29280\n",
            "Training loss epoch: 0.07954368930191306\n",
            "Training accuracy epoch: 1782.0625\n",
            "Number of batches: 1831\n",
            "Number of Sentences Trained on: 29296\n",
            "Training loss epoch: 0.07950864036920631\n",
            "Training accuracy epoch: 1783.0625\n",
            "Number of batches: 1832\n",
            "Number of Sentences Trained on: 29312\n",
            "Training loss epoch: 0.07947368685362707\n",
            "Training accuracy epoch: 1784.0625\n",
            "Number of batches: 1833\n",
            "Number of Sentences Trained on: 29328\n",
            "Training loss epoch: 0.07953359577911762\n",
            "Training accuracy epoch: 1785.0\n",
            "Number of batches: 1834\n",
            "Number of Sentences Trained on: 29344\n",
            "Training loss epoch: 0.07949136563417203\n",
            "Training accuracy epoch: 1786.0\n",
            "Number of batches: 1835\n",
            "Number of Sentences Trained on: 29360\n",
            "Training loss epoch: 0.07946330188613597\n",
            "Training accuracy epoch: 1787.0\n",
            "Number of batches: 1836\n",
            "Number of Sentences Trained on: 29376\n",
            "Training loss epoch: 0.07943220812772912\n",
            "Training accuracy epoch: 1788.0\n",
            "Number of batches: 1837\n",
            "Number of Sentences Trained on: 29392\n",
            "Training loss epoch: 0.07939599169199636\n",
            "Training accuracy epoch: 1789.0\n",
            "Number of batches: 1838\n",
            "Number of Sentences Trained on: 29408\n",
            "Training loss epoch: 0.07936098274267218\n",
            "Training accuracy epoch: 1790.0\n",
            "Number of batches: 1839\n",
            "Number of Sentences Trained on: 29424\n",
            "Training loss epoch: 0.07932684524815198\n",
            "Training accuracy epoch: 1791.0\n",
            "Number of batches: 1840\n",
            "Number of Sentences Trained on: 29440\n",
            "Training loss epoch: 0.0792942992304845\n",
            "Training accuracy epoch: 1792.0\n",
            "Number of batches: 1841\n",
            "Number of Sentences Trained on: 29456\n",
            "Training loss epoch: 0.07925321752565798\n",
            "Training accuracy epoch: 1793.0\n",
            "Number of batches: 1842\n",
            "Number of Sentences Trained on: 29472\n",
            "Training loss epoch: 0.07937065540109302\n",
            "Training accuracy epoch: 1793.875\n",
            "Number of batches: 1843\n",
            "Number of Sentences Trained on: 29488\n",
            "Training loss epoch: 0.0793426835415573\n",
            "Training accuracy epoch: 1794.875\n",
            "Number of batches: 1844\n",
            "Number of Sentences Trained on: 29504\n",
            "Training loss epoch: 0.07934923426680038\n",
            "Training accuracy epoch: 1795.875\n",
            "Number of batches: 1845\n",
            "Number of Sentences Trained on: 29520\n",
            "Training loss epoch: 0.07942403301885535\n",
            "Training accuracy epoch: 1796.8125\n",
            "Number of batches: 1846\n",
            "Number of Sentences Trained on: 29536\n",
            "Training loss epoch: 0.07944055410141655\n",
            "Training accuracy epoch: 1797.75\n",
            "Number of batches: 1847\n",
            "Number of Sentences Trained on: 29552\n",
            "Training loss epoch: 0.07945189262847469\n",
            "Training accuracy epoch: 1798.6875\n",
            "Number of batches: 1848\n",
            "Number of Sentences Trained on: 29568\n",
            "Training loss epoch: 0.0794113689349072\n",
            "Training accuracy epoch: 1799.6875\n",
            "Number of batches: 1849\n",
            "Number of Sentences Trained on: 29584\n",
            "Training loss epoch: 0.07938686214483381\n",
            "Training accuracy epoch: 1800.6875\n",
            "Number of batches: 1850\n",
            "Number of Sentences Trained on: 29600\n",
            "Training loss epoch: 0.07945152184041254\n",
            "Training accuracy epoch: 1801.625\n",
            "Number of batches: 1851\n",
            "Number of Sentences Trained on: 29616\n",
            "Training loss epoch: 0.0794451123346419\n",
            "Training accuracy epoch: 1802.5625\n",
            "Number of batches: 1852\n",
            "Number of Sentences Trained on: 29632\n",
            "Training loss epoch: 0.07940449198040796\n",
            "Training accuracy epoch: 1803.5625\n",
            "Number of batches: 1853\n",
            "Number of Sentences Trained on: 29648\n",
            "Training loss epoch: 0.07937149631262679\n",
            "Training accuracy epoch: 1804.5625\n",
            "Number of batches: 1854\n",
            "Number of Sentences Trained on: 29664\n",
            "Training loss epoch: 0.07936019336252795\n",
            "Training accuracy epoch: 1805.5\n",
            "Number of batches: 1855\n",
            "Number of Sentences Trained on: 29680\n",
            "Training loss epoch: 0.07935945449115275\n",
            "Training accuracy epoch: 1806.5\n",
            "Number of batches: 1856\n",
            "Number of Sentences Trained on: 29696\n",
            "Training loss epoch: 0.07931806355287314\n",
            "Training accuracy epoch: 1807.5\n",
            "Number of batches: 1857\n",
            "Number of Sentences Trained on: 29712\n",
            "Training loss epoch: 0.0793810987598229\n",
            "Training accuracy epoch: 1808.4375\n",
            "Number of batches: 1858\n",
            "Number of Sentences Trained on: 29728\n",
            "Training loss epoch: 0.07941691015091862\n",
            "Training accuracy epoch: 1809.375\n",
            "Number of batches: 1859\n",
            "Number of Sentences Trained on: 29744\n",
            "Training loss epoch: 0.07937559066773756\n",
            "Training accuracy epoch: 1810.375\n",
            "Number of batches: 1860\n",
            "Number of Sentences Trained on: 29760\n",
            "Training loss epoch: 0.079366458936541\n",
            "Training accuracy epoch: 1811.375\n",
            "Number of batches: 1861\n",
            "Number of Sentences Trained on: 29776\n",
            "Training loss epoch: 0.07933410444198676\n",
            "Training accuracy epoch: 1812.375\n",
            "Number of batches: 1862\n",
            "Number of Sentences Trained on: 29792\n",
            "Training loss epoch: 0.07931048668837822\n",
            "Training accuracy epoch: 1813.375\n",
            "Number of batches: 1863\n",
            "Number of Sentences Trained on: 29808\n",
            "Training loss epoch: 0.07927950593683673\n",
            "Training accuracy epoch: 1814.375\n",
            "Number of batches: 1864\n",
            "Number of Sentences Trained on: 29824\n",
            "Training loss epoch: 0.07927301647648748\n",
            "Training accuracy epoch: 1815.375\n",
            "Number of batches: 1865\n",
            "Number of Sentences Trained on: 29840\n",
            "Training loss epoch: 0.07925180445726482\n",
            "Training accuracy epoch: 1816.375\n",
            "Number of batches: 1866\n",
            "Number of Sentences Trained on: 29856\n",
            "Training loss epoch: 0.07922133654348833\n",
            "Training accuracy epoch: 1817.375\n",
            "Number of batches: 1867\n",
            "Number of Sentences Trained on: 29872\n",
            "Training loss epoch: 0.07919744007537509\n",
            "Training accuracy epoch: 1818.375\n",
            "Number of batches: 1868\n",
            "Number of Sentences Trained on: 29888\n",
            "Training loss epoch: 0.0791563140987831\n",
            "Training accuracy epoch: 1819.375\n",
            "Number of batches: 1869\n",
            "Number of Sentences Trained on: 29904\n",
            "Training loss epoch: 0.07918878253121339\n",
            "Training accuracy epoch: 1820.3125\n",
            "Number of batches: 1870\n",
            "Number of Sentences Trained on: 29920\n",
            "Training loss epoch: 0.07915112832586335\n",
            "Training accuracy epoch: 1821.3125\n",
            "Number of batches: 1871\n",
            "Number of Sentences Trained on: 29936\n",
            "Training loss epoch: 0.07920107809035021\n",
            "Training accuracy epoch: 1822.25\n",
            "Number of batches: 1872\n",
            "Number of Sentences Trained on: 29952\n",
            "Training loss epoch: 0.07916650957673367\n",
            "Training accuracy epoch: 1823.25\n",
            "Number of batches: 1873\n",
            "Number of Sentences Trained on: 29968\n",
            "Training loss epoch: 0.07912750225887212\n",
            "Training accuracy epoch: 1824.25\n",
            "Number of batches: 1874\n",
            "Number of Sentences Trained on: 29984\n",
            "Training loss epoch: 0.07909619446789536\n",
            "Training accuracy epoch: 1825.25\n",
            "Number of batches: 1875\n",
            "Number of Sentences Trained on: 30000\n",
            "Training loss epoch: 0.0790646897394118\n",
            "Training accuracy epoch: 1826.25\n",
            "Number of batches: 1876\n",
            "Number of Sentences Trained on: 30016\n",
            "Training loss epoch: 0.07915970399280448\n",
            "Training accuracy epoch: 1827.125\n",
            "Number of batches: 1877\n",
            "Number of Sentences Trained on: 30032\n",
            "Training loss epoch: 0.0791551149801875\n",
            "Training accuracy epoch: 1828.0625\n",
            "Number of batches: 1878\n",
            "Number of Sentences Trained on: 30048\n",
            "Training loss epoch: 0.07925219231138257\n",
            "Training accuracy epoch: 1829.0\n",
            "Number of batches: 1879\n",
            "Number of Sentences Trained on: 30064\n",
            "Training loss epoch: 0.07922833052637523\n",
            "Training accuracy epoch: 1830.0\n",
            "Number of batches: 1880\n",
            "Number of Sentences Trained on: 30080\n",
            "Training loss epoch: 0.07920950178818761\n",
            "Training accuracy epoch: 1830.9375\n",
            "Number of batches: 1881\n",
            "Number of Sentences Trained on: 30096\n",
            "Training loss epoch: 0.07917087332723927\n",
            "Training accuracy epoch: 1831.9375\n",
            "Number of batches: 1882\n",
            "Number of Sentences Trained on: 30112\n",
            "Training loss epoch: 0.07912916015908486\n",
            "Training accuracy epoch: 1832.9375\n",
            "Number of batches: 1883\n",
            "Number of Sentences Trained on: 30128\n",
            "Training loss epoch: 0.07909400655863984\n",
            "Training accuracy epoch: 1833.9375\n",
            "Number of batches: 1884\n",
            "Number of Sentences Trained on: 30144\n",
            "Training loss epoch: 0.07905685235170304\n",
            "Training accuracy epoch: 1834.9375\n",
            "Number of batches: 1885\n",
            "Number of Sentences Trained on: 30160\n",
            "Training loss epoch: 0.07908462007655152\n",
            "Training accuracy epoch: 1835.9375\n",
            "Number of batches: 1886\n",
            "Number of Sentences Trained on: 30176\n",
            "Training loss epoch: 0.0791113458104918\n",
            "Training accuracy epoch: 1836.875\n",
            "Number of batches: 1887\n",
            "Number of Sentences Trained on: 30192\n",
            "Training loss epoch: 0.07915299666691973\n",
            "Training accuracy epoch: 1837.8125\n",
            "Number of batches: 1888\n",
            "Number of Sentences Trained on: 30208\n",
            "Training loss epoch: 0.0791737183235267\n",
            "Training accuracy epoch: 1838.75\n",
            "Number of batches: 1889\n",
            "Number of Sentences Trained on: 30224\n",
            "Training loss epoch: 0.07919968531333667\n",
            "Training accuracy epoch: 1839.6875\n",
            "Number of batches: 1890\n",
            "Number of Sentences Trained on: 30240\n",
            "Training loss epoch: 0.07916305584804154\n",
            "Training accuracy epoch: 1840.6875\n",
            "Number of batches: 1891\n",
            "Number of Sentences Trained on: 30256\n",
            "Training loss epoch: 0.0791386033674124\n",
            "Training accuracy epoch: 1841.6875\n",
            "Number of batches: 1892\n",
            "Number of Sentences Trained on: 30272\n",
            "Training loss epoch: 0.07918933826506744\n",
            "Training accuracy epoch: 1842.625\n",
            "Number of batches: 1893\n",
            "Number of Sentences Trained on: 30288\n",
            "Training loss epoch: 0.07922669470097479\n",
            "Training accuracy epoch: 1843.5625\n",
            "Number of batches: 1894\n",
            "Number of Sentences Trained on: 30304\n",
            "Training loss epoch: 0.07919252130740793\n",
            "Training accuracy epoch: 1844.5625\n",
            "Number of batches: 1895\n",
            "Number of Sentences Trained on: 30320\n",
            "Training loss epoch: 0.07924399969397584\n",
            "Training accuracy epoch: 1845.5\n",
            "Number of batches: 1896\n",
            "Number of Sentences Trained on: 30336\n",
            "Training loss epoch: 0.0793796059416756\n",
            "Training accuracy epoch: 1846.4375\n",
            "Number of batches: 1897\n",
            "Number of Sentences Trained on: 30352\n",
            "Training loss epoch: 0.07946998706495415\n",
            "Training accuracy epoch: 1847.375\n",
            "Number of batches: 1898\n",
            "Number of Sentences Trained on: 30368\n",
            "Training loss epoch: 0.07945759158184923\n",
            "Training accuracy epoch: 1848.375\n",
            "Number of batches: 1899\n",
            "Number of Sentences Trained on: 30384\n",
            "Training loss epoch: 0.07953057080536383\n",
            "Training accuracy epoch: 1849.1875\n",
            "Number of batches: 1900\n",
            "Number of Sentences Trained on: 30400\n",
            "Training loss per 100 training steps: 0.0794932169456598\n",
            "Training loss epoch: 0.0794932169456598\n",
            "Training accuracy epoch: 1850.1875\n",
            "Number of batches: 1901\n",
            "Number of Sentences Trained on: 30416\n",
            "Training loss epoch: 0.07948022881320731\n",
            "Training accuracy epoch: 1851.1875\n",
            "Number of batches: 1902\n",
            "Number of Sentences Trained on: 30432\n",
            "Training loss epoch: 0.07948366038802425\n",
            "Training accuracy epoch: 1852.1875\n",
            "Number of batches: 1903\n",
            "Number of Sentences Trained on: 30448\n",
            "Training loss epoch: 0.07944285979274278\n",
            "Training accuracy epoch: 1853.1875\n",
            "Number of batches: 1904\n",
            "Number of Sentences Trained on: 30464\n",
            "Training loss epoch: 0.07943350493899068\n",
            "Training accuracy epoch: 1854.1875\n",
            "Number of batches: 1905\n",
            "Number of Sentences Trained on: 30480\n",
            "Training loss epoch: 0.0794166788852749\n",
            "Training accuracy epoch: 1855.125\n",
            "Number of batches: 1906\n",
            "Number of Sentences Trained on: 30496\n",
            "Training loss epoch: 0.07937981734892237\n",
            "Training accuracy epoch: 1856.125\n",
            "Number of batches: 1907\n",
            "Number of Sentences Trained on: 30512\n",
            "Training loss epoch: 0.07937869494030117\n",
            "Training accuracy epoch: 1857.125\n",
            "Number of batches: 1908\n",
            "Number of Sentences Trained on: 30528\n",
            "Training loss epoch: 0.07934086209597203\n",
            "Training accuracy epoch: 1858.125\n",
            "Number of batches: 1909\n",
            "Number of Sentences Trained on: 30544\n",
            "Training loss epoch: 0.07930122560220197\n",
            "Training accuracy epoch: 1859.125\n",
            "Number of batches: 1910\n",
            "Number of Sentences Trained on: 30560\n",
            "Training loss epoch: 0.07928645331216091\n",
            "Training accuracy epoch: 1860.125\n",
            "Number of batches: 1911\n",
            "Number of Sentences Trained on: 30576\n",
            "Training loss epoch: 0.07925902969332908\n",
            "Training accuracy epoch: 1861.125\n",
            "Number of batches: 1912\n",
            "Number of Sentences Trained on: 30592\n",
            "Training loss epoch: 0.0792370604319853\n",
            "Training accuracy epoch: 1862.125\n",
            "Number of batches: 1913\n",
            "Number of Sentences Trained on: 30608\n",
            "Training loss epoch: 0.07920058792729898\n",
            "Training accuracy epoch: 1863.125\n",
            "Number of batches: 1914\n",
            "Number of Sentences Trained on: 30624\n",
            "Training loss epoch: 0.0791662803376744\n",
            "Training accuracy epoch: 1864.125\n",
            "Number of batches: 1915\n",
            "Number of Sentences Trained on: 30640\n",
            "Training loss epoch: 0.0791337642218719\n",
            "Training accuracy epoch: 1865.125\n",
            "Number of batches: 1916\n",
            "Number of Sentences Trained on: 30656\n",
            "Training loss epoch: 0.07910414205509846\n",
            "Training accuracy epoch: 1866.125\n",
            "Number of batches: 1917\n",
            "Number of Sentences Trained on: 30672\n",
            "Training loss epoch: 0.07910281910226406\n",
            "Training accuracy epoch: 1867.125\n",
            "Number of batches: 1918\n",
            "Number of Sentences Trained on: 30688\n",
            "Training loss epoch: 0.0790677683785942\n",
            "Training accuracy epoch: 1868.125\n",
            "Number of batches: 1919\n",
            "Number of Sentences Trained on: 30704\n",
            "Training loss epoch: 0.07903447291511535\n",
            "Training accuracy epoch: 1869.125\n",
            "Number of batches: 1920\n",
            "Number of Sentences Trained on: 30720\n",
            "Training loss epoch: 0.07899729553914002\n",
            "Training accuracy epoch: 1870.125\n",
            "Number of batches: 1921\n",
            "Number of Sentences Trained on: 30736\n",
            "Training loss epoch: 0.07897230855073394\n",
            "Training accuracy epoch: 1871.125\n",
            "Number of batches: 1922\n",
            "Number of Sentences Trained on: 30752\n",
            "Training loss epoch: 0.07893477933038134\n",
            "Training accuracy epoch: 1872.125\n",
            "Number of batches: 1923\n",
            "Number of Sentences Trained on: 30768\n",
            "Training loss epoch: 0.07889754459088058\n",
            "Training accuracy epoch: 1873.125\n",
            "Number of batches: 1924\n",
            "Number of Sentences Trained on: 30784\n",
            "Training loss epoch: 0.07887773739970773\n",
            "Training accuracy epoch: 1874.0625\n",
            "Number of batches: 1925\n",
            "Number of Sentences Trained on: 30800\n",
            "Training loss epoch: 0.07884172268301239\n",
            "Training accuracy epoch: 1875.0625\n",
            "Number of batches: 1926\n",
            "Number of Sentences Trained on: 30816\n",
            "Training loss epoch: 0.07881536151241832\n",
            "Training accuracy epoch: 1876.0625\n",
            "Number of batches: 1927\n",
            "Number of Sentences Trained on: 30832\n",
            "Training loss epoch: 0.07877537859428355\n",
            "Training accuracy epoch: 1877.0625\n",
            "Number of batches: 1928\n",
            "Number of Sentences Trained on: 30848\n",
            "Training loss epoch: 0.07886070761675541\n",
            "Training accuracy epoch: 1878.0\n",
            "Number of batches: 1929\n",
            "Number of Sentences Trained on: 30864\n",
            "Training loss epoch: 0.07883502217442451\n",
            "Training accuracy epoch: 1879.0\n",
            "Number of batches: 1930\n",
            "Number of Sentences Trained on: 30880\n",
            "Training loss epoch: 0.07879949626590899\n",
            "Training accuracy epoch: 1880.0\n",
            "Number of batches: 1931\n",
            "Number of Sentences Trained on: 30896\n",
            "Training loss epoch: 0.07875921657179927\n",
            "Training accuracy epoch: 1881.0\n",
            "Number of batches: 1932\n",
            "Number of Sentences Trained on: 30912\n",
            "Training loss epoch: 0.0787808128212203\n",
            "Training accuracy epoch: 1881.9375\n",
            "Number of batches: 1933\n",
            "Number of Sentences Trained on: 30928\n",
            "Training loss epoch: 0.07882582635641078\n",
            "Training accuracy epoch: 1882.875\n",
            "Number of batches: 1934\n",
            "Number of Sentences Trained on: 30944\n",
            "Training loss epoch: 0.07880922130972944\n",
            "Training accuracy epoch: 1883.875\n",
            "Number of batches: 1935\n",
            "Number of Sentences Trained on: 30960\n",
            "Training loss epoch: 0.0787701003355173\n",
            "Training accuracy epoch: 1884.875\n",
            "Number of batches: 1936\n",
            "Number of Sentences Trained on: 30976\n",
            "Training loss epoch: 0.0788434592678788\n",
            "Training accuracy epoch: 1885.8125\n",
            "Number of batches: 1937\n",
            "Number of Sentences Trained on: 30992\n",
            "Training loss epoch: 0.07892431363756723\n",
            "Training accuracy epoch: 1886.75\n",
            "Number of batches: 1938\n",
            "Number of Sentences Trained on: 31008\n",
            "Training loss epoch: 0.07889519800948541\n",
            "Training accuracy epoch: 1887.75\n",
            "Number of batches: 1939\n",
            "Number of Sentences Trained on: 31024\n",
            "Training loss epoch: 0.07885502987547559\n",
            "Training accuracy epoch: 1888.75\n",
            "Number of batches: 1940\n",
            "Number of Sentences Trained on: 31040\n",
            "Training loss epoch: 0.07881793328108962\n",
            "Training accuracy epoch: 1889.75\n",
            "Number of batches: 1941\n",
            "Number of Sentences Trained on: 31056\n",
            "Training loss epoch: 0.07879103916093487\n",
            "Training accuracy epoch: 1890.75\n",
            "Number of batches: 1942\n",
            "Number of Sentences Trained on: 31072\n",
            "Training loss epoch: 0.07879868052983778\n",
            "Training accuracy epoch: 1891.6875\n",
            "Number of batches: 1943\n",
            "Number of Sentences Trained on: 31088\n",
            "Training loss epoch: 0.07880243287881256\n",
            "Training accuracy epoch: 1892.625\n",
            "Number of batches: 1944\n",
            "Number of Sentences Trained on: 31104\n",
            "Training loss epoch: 0.07877756825632985\n",
            "Training accuracy epoch: 1893.625\n",
            "Number of batches: 1945\n",
            "Number of Sentences Trained on: 31120\n",
            "Training loss epoch: 0.07874639332152385\n",
            "Training accuracy epoch: 1894.625\n",
            "Number of batches: 1946\n",
            "Number of Sentences Trained on: 31136\n",
            "Training loss epoch: 0.07871189045290315\n",
            "Training accuracy epoch: 1895.625\n",
            "Number of batches: 1947\n",
            "Number of Sentences Trained on: 31152\n",
            "Training loss epoch: 0.07889746812563851\n",
            "Training accuracy epoch: 1896.4375\n",
            "Number of batches: 1948\n",
            "Number of Sentences Trained on: 31168\n",
            "Training loss epoch: 0.07885803922899069\n",
            "Training accuracy epoch: 1897.4375\n",
            "Number of batches: 1949\n",
            "Number of Sentences Trained on: 31184\n",
            "Training loss epoch: 0.07894998198078097\n",
            "Training accuracy epoch: 1898.375\n",
            "Number of batches: 1950\n",
            "Number of Sentences Trained on: 31200\n",
            "Training loss epoch: 0.07891999422464253\n",
            "Training accuracy epoch: 1899.375\n",
            "Number of batches: 1951\n",
            "Number of Sentences Trained on: 31216\n",
            "Training loss epoch: 0.07888566408931437\n",
            "Training accuracy epoch: 1900.375\n",
            "Number of batches: 1952\n",
            "Number of Sentences Trained on: 31232\n",
            "Training loss epoch: 0.07885289644307521\n",
            "Training accuracy epoch: 1901.375\n",
            "Number of batches: 1953\n",
            "Number of Sentences Trained on: 31248\n",
            "Training loss epoch: 0.07887732719195191\n",
            "Training accuracy epoch: 1902.3125\n",
            "Number of batches: 1954\n",
            "Number of Sentences Trained on: 31264\n",
            "Training loss epoch: 0.07884434993223101\n",
            "Training accuracy epoch: 1903.3125\n",
            "Number of batches: 1955\n",
            "Number of Sentences Trained on: 31280\n",
            "Training loss epoch: 0.07894042097326381\n",
            "Training accuracy epoch: 1904.25\n",
            "Number of batches: 1956\n",
            "Number of Sentences Trained on: 31296\n",
            "Training loss epoch: 0.0789792581584537\n",
            "Training accuracy epoch: 1905.1875\n",
            "Number of batches: 1957\n",
            "Number of Sentences Trained on: 31312\n",
            "Training loss epoch: 0.0789805911442288\n",
            "Training accuracy epoch: 1906.125\n",
            "Number of batches: 1958\n",
            "Number of Sentences Trained on: 31328\n",
            "Training loss epoch: 0.0789432567507586\n",
            "Training accuracy epoch: 1907.125\n",
            "Number of batches: 1959\n",
            "Number of Sentences Trained on: 31344\n",
            "Training loss epoch: 0.07890990058719526\n",
            "Training accuracy epoch: 1908.125\n",
            "Number of batches: 1960\n",
            "Number of Sentences Trained on: 31360\n",
            "Training loss epoch: 0.07887098164888504\n",
            "Training accuracy epoch: 1909.125\n",
            "Number of batches: 1961\n",
            "Number of Sentences Trained on: 31376\n",
            "Training loss epoch: 0.07883184407252943\n",
            "Training accuracy epoch: 1910.125\n",
            "Number of batches: 1962\n",
            "Number of Sentences Trained on: 31392\n",
            "Training loss epoch: 0.07883883801943213\n",
            "Training accuracy epoch: 1911.0625\n",
            "Number of batches: 1963\n",
            "Number of Sentences Trained on: 31408\n",
            "Training loss epoch: 0.07892055441795238\n",
            "Training accuracy epoch: 1912.0\n",
            "Number of batches: 1964\n",
            "Number of Sentences Trained on: 31424\n",
            "Training loss epoch: 0.07888244279950879\n",
            "Training accuracy epoch: 1913.0\n",
            "Number of batches: 1965\n",
            "Number of Sentences Trained on: 31440\n",
            "Training loss epoch: 0.07886132055415147\n",
            "Training accuracy epoch: 1914.0\n",
            "Number of batches: 1966\n",
            "Number of Sentences Trained on: 31456\n",
            "Training loss epoch: 0.07883155356701589\n",
            "Training accuracy epoch: 1914.9375\n",
            "Number of batches: 1967\n",
            "Number of Sentences Trained on: 31472\n",
            "Training loss epoch: 0.07892363018921363\n",
            "Training accuracy epoch: 1915.8125\n",
            "Number of batches: 1968\n",
            "Number of Sentences Trained on: 31488\n",
            "Training loss epoch: 0.07888808834827728\n",
            "Training accuracy epoch: 1916.8125\n",
            "Number of batches: 1969\n",
            "Number of Sentences Trained on: 31504\n",
            "Training loss epoch: 0.0789314741835023\n",
            "Training accuracy epoch: 1917.75\n",
            "Number of batches: 1970\n",
            "Number of Sentences Trained on: 31520\n",
            "Training loss epoch: 0.07889358572678001\n",
            "Training accuracy epoch: 1918.75\n",
            "Number of batches: 1971\n",
            "Number of Sentences Trained on: 31536\n",
            "Training loss epoch: 0.07885677667827788\n",
            "Training accuracy epoch: 1919.75\n",
            "Number of batches: 1972\n",
            "Number of Sentences Trained on: 31552\n",
            "Training loss epoch: 0.07882020488518335\n",
            "Training accuracy epoch: 1920.75\n",
            "Number of batches: 1973\n",
            "Number of Sentences Trained on: 31568\n",
            "Training loss epoch: 0.07880608522225695\n",
            "Training accuracy epoch: 1921.75\n",
            "Number of batches: 1974\n",
            "Number of Sentences Trained on: 31584\n",
            "Training loss epoch: 0.07881286187912337\n",
            "Training accuracy epoch: 1922.75\n",
            "Number of batches: 1975\n",
            "Number of Sentences Trained on: 31600\n",
            "Training loss epoch: 0.07901105905970739\n",
            "Training accuracy epoch: 1923.625\n",
            "Number of batches: 1976\n",
            "Number of Sentences Trained on: 31616\n",
            "Training loss epoch: 0.07899891842592927\n",
            "Training accuracy epoch: 1924.5625\n",
            "Number of batches: 1977\n",
            "Number of Sentences Trained on: 31632\n",
            "Training loss epoch: 0.07905912945550879\n",
            "Training accuracy epoch: 1925.5625\n",
            "Number of batches: 1978\n",
            "Number of Sentences Trained on: 31648\n",
            "Training loss epoch: 0.07906715218823965\n",
            "Training accuracy epoch: 1926.5\n",
            "Number of batches: 1979\n",
            "Number of Sentences Trained on: 31664\n",
            "Training loss epoch: 0.07906128004879157\n",
            "Training accuracy epoch: 1927.4375\n",
            "Number of batches: 1980\n",
            "Number of Sentences Trained on: 31680\n",
            "Training loss epoch: 0.07904426496162889\n",
            "Training accuracy epoch: 1928.375\n",
            "Number of batches: 1981\n",
            "Number of Sentences Trained on: 31696\n",
            "Training loss epoch: 0.07903249639872141\n",
            "Training accuracy epoch: 1929.375\n",
            "Number of batches: 1982\n",
            "Number of Sentences Trained on: 31712\n",
            "Training loss epoch: 0.07906513599047718\n",
            "Training accuracy epoch: 1930.3125\n",
            "Number of batches: 1983\n",
            "Number of Sentences Trained on: 31728\n",
            "Training loss epoch: 0.07902809930321324\n",
            "Training accuracy epoch: 1931.3125\n",
            "Number of batches: 1984\n",
            "Number of Sentences Trained on: 31744\n",
            "Training loss epoch: 0.07911559252611769\n",
            "Training accuracy epoch: 1932.25\n",
            "Number of batches: 1985\n",
            "Number of Sentences Trained on: 31760\n",
            "Training loss epoch: 0.07907765481023651\n",
            "Training accuracy epoch: 1933.25\n",
            "Number of batches: 1986\n",
            "Number of Sentences Trained on: 31776\n",
            "Training loss epoch: 0.0790509158241928\n",
            "Training accuracy epoch: 1934.1875\n",
            "Number of batches: 1987\n",
            "Number of Sentences Trained on: 31792\n",
            "Training loss epoch: 0.07908088578345121\n",
            "Training accuracy epoch: 1935.125\n",
            "Number of batches: 1988\n",
            "Number of Sentences Trained on: 31808\n",
            "Training loss epoch: 0.07907818268802003\n",
            "Training accuracy epoch: 1936.125\n",
            "Number of batches: 1989\n",
            "Number of Sentences Trained on: 31824\n",
            "Training loss epoch: 0.07921850874251628\n",
            "Training accuracy epoch: 1937.0625\n",
            "Number of batches: 1990\n",
            "Number of Sentences Trained on: 31840\n",
            "Training loss epoch: 0.079303527470166\n",
            "Training accuracy epoch: 1937.9375\n",
            "Number of batches: 1991\n",
            "Number of Sentences Trained on: 31856\n",
            "Training loss epoch: 0.07926513458931798\n",
            "Training accuracy epoch: 1938.9375\n",
            "Number of batches: 1992\n",
            "Number of Sentences Trained on: 31872\n",
            "Training loss epoch: 0.07922604148868975\n",
            "Training accuracy epoch: 1939.9375\n",
            "Number of batches: 1993\n",
            "Number of Sentences Trained on: 31888\n",
            "Training loss epoch: 0.07925167360346601\n",
            "Training accuracy epoch: 1940.9375\n",
            "Number of batches: 1994\n",
            "Number of Sentences Trained on: 31904\n",
            "Training loss epoch: 0.07922676473998647\n",
            "Training accuracy epoch: 1941.9375\n",
            "Number of batches: 1995\n",
            "Number of Sentences Trained on: 31920\n",
            "Training loss epoch: 0.0794120105994355\n",
            "Training accuracy epoch: 1942.8125\n",
            "Number of batches: 1996\n",
            "Number of Sentences Trained on: 31936\n",
            "Training loss epoch: 0.07966678859542696\n",
            "Training accuracy epoch: 1943.625\n",
            "Number of batches: 1997\n",
            "Number of Sentences Trained on: 31952\n",
            "Training loss epoch: 0.07981756387180376\n",
            "Training accuracy epoch: 1944.5\n",
            "Number of batches: 1998\n",
            "Number of Sentences Trained on: 31968\n",
            "Training loss epoch: 0.07984610546919613\n",
            "Training accuracy epoch: 1945.4375\n",
            "Number of batches: 1999\n",
            "Number of Sentences Trained on: 31984\n",
            "Training loss epoch: 0.07980886207295407\n",
            "Training accuracy epoch: 1946.4375\n",
            "Number of batches: 2000\n",
            "Number of Sentences Trained on: 32000\n",
            "Training loss per 100 training steps: 0.07981015652609719\n",
            "Training loss epoch: 0.07981015652609719\n",
            "Training accuracy epoch: 1947.4375\n",
            "Number of batches: 2001\n",
            "Number of Sentences Trained on: 32016\n",
            "Training loss epoch: 0.07980334223371632\n",
            "Training accuracy epoch: 1948.4375\n",
            "Number of batches: 2002\n",
            "Number of Sentences Trained on: 32032\n",
            "Training loss epoch: 0.07976855661427866\n",
            "Training accuracy epoch: 1949.4375\n",
            "Number of batches: 2003\n",
            "Number of Sentences Trained on: 32048\n",
            "Training loss epoch: 0.07973345342618118\n",
            "Training accuracy epoch: 1950.4375\n",
            "Number of batches: 2004\n",
            "Number of Sentences Trained on: 32064\n",
            "Training loss epoch: 0.07972882913521077\n",
            "Training accuracy epoch: 1951.375\n",
            "Number of batches: 2005\n",
            "Number of Sentences Trained on: 32080\n",
            "Training loss epoch: 0.07970644540419759\n",
            "Training accuracy epoch: 1952.375\n",
            "Number of batches: 2006\n",
            "Number of Sentences Trained on: 32096\n",
            "Training loss epoch: 0.07967667853421491\n",
            "Training accuracy epoch: 1953.375\n",
            "Number of batches: 2007\n",
            "Number of Sentences Trained on: 32112\n",
            "Training loss epoch: 0.07963929534247677\n",
            "Training accuracy epoch: 1954.375\n",
            "Number of batches: 2008\n",
            "Number of Sentences Trained on: 32128\n",
            "Training loss epoch: 0.0796564539914656\n",
            "Training accuracy epoch: 1955.3125\n",
            "Number of batches: 2009\n",
            "Number of Sentences Trained on: 32144\n",
            "Training loss epoch: 0.07964852003397163\n",
            "Training accuracy epoch: 1956.3125\n",
            "Number of batches: 2010\n",
            "Number of Sentences Trained on: 32160\n",
            "Training loss epoch: 0.07960966181143211\n",
            "Training accuracy epoch: 1957.3125\n",
            "Number of batches: 2011\n",
            "Number of Sentences Trained on: 32176\n",
            "Training loss epoch: 0.07959700273273794\n",
            "Training accuracy epoch: 1958.25\n",
            "Number of batches: 2012\n",
            "Number of Sentences Trained on: 32192\n",
            "Training loss epoch: 0.07964651060510222\n",
            "Training accuracy epoch: 1959.1875\n",
            "Number of batches: 2013\n",
            "Number of Sentences Trained on: 32208\n",
            "Training loss epoch: 0.07976383352270981\n",
            "Training accuracy epoch: 1960.0625\n",
            "Number of batches: 2014\n",
            "Number of Sentences Trained on: 32224\n",
            "Training loss epoch: 0.07985802350189392\n",
            "Training accuracy epoch: 1961.0\n",
            "Number of batches: 2015\n",
            "Number of Sentences Trained on: 32240\n",
            "Training loss epoch: 0.07983257476153374\n",
            "Training accuracy epoch: 1962.0\n",
            "Number of batches: 2016\n",
            "Number of Sentences Trained on: 32256\n",
            "Training loss epoch: 0.07982209244178574\n",
            "Training accuracy epoch: 1962.9375\n",
            "Number of batches: 2017\n",
            "Number of Sentences Trained on: 32272\n",
            "Training loss epoch: 0.07989769547721576\n",
            "Training accuracy epoch: 1963.8125\n",
            "Number of batches: 2018\n",
            "Number of Sentences Trained on: 32288\n",
            "Training loss epoch: 0.0799087252438147\n",
            "Training accuracy epoch: 1964.8125\n",
            "Number of batches: 2019\n",
            "Number of Sentences Trained on: 32304\n",
            "Training loss epoch: 0.07987986610112262\n",
            "Training accuracy epoch: 1965.8125\n",
            "Number of batches: 2020\n",
            "Number of Sentences Trained on: 32320\n",
            "Training loss epoch: 0.07984200269585502\n",
            "Training accuracy epoch: 1966.8125\n",
            "Number of batches: 2021\n",
            "Number of Sentences Trained on: 32336\n",
            "Training loss epoch: 0.0798158861375484\n",
            "Training accuracy epoch: 1967.8125\n",
            "Number of batches: 2022\n",
            "Number of Sentences Trained on: 32352\n",
            "Training loss epoch: 0.07981949733454033\n",
            "Training accuracy epoch: 1968.75\n",
            "Number of batches: 2023\n",
            "Number of Sentences Trained on: 32368\n",
            "Training loss epoch: 0.07978519589970408\n",
            "Training accuracy epoch: 1969.75\n",
            "Number of batches: 2024\n",
            "Number of Sentences Trained on: 32384\n",
            "Training loss epoch: 0.07984656864002543\n",
            "Training accuracy epoch: 1970.625\n",
            "Number of batches: 2025\n",
            "Number of Sentences Trained on: 32400\n",
            "Training loss epoch: 0.07983302593542696\n",
            "Training accuracy epoch: 1971.625\n",
            "Number of batches: 2026\n",
            "Number of Sentences Trained on: 32416\n",
            "Training loss epoch: 0.07980705869515803\n",
            "Training accuracy epoch: 1972.625\n",
            "Number of batches: 2027\n",
            "Number of Sentences Trained on: 32432\n",
            "Training loss epoch: 0.07989897989492353\n",
            "Training accuracy epoch: 1973.5625\n",
            "Number of batches: 2028\n",
            "Number of Sentences Trained on: 32448\n",
            "Training loss epoch: 0.07989543729788828\n",
            "Training accuracy epoch: 1974.5625\n",
            "Number of batches: 2029\n",
            "Number of Sentences Trained on: 32464\n",
            "Training loss epoch: 0.07996227650020302\n",
            "Training accuracy epoch: 1975.5\n",
            "Number of batches: 2030\n",
            "Number of Sentences Trained on: 32480\n",
            "Training loss epoch: 0.07993565984916619\n",
            "Training accuracy epoch: 1976.5\n",
            "Number of batches: 2031\n",
            "Number of Sentences Trained on: 32496\n",
            "Training loss epoch: 0.07989770561240636\n",
            "Training accuracy epoch: 1977.5\n",
            "Number of batches: 2032\n",
            "Number of Sentences Trained on: 32512\n",
            "Training loss epoch: 0.07986830654666591\n",
            "Training accuracy epoch: 1978.5\n",
            "Number of batches: 2033\n",
            "Number of Sentences Trained on: 32528\n",
            "Training loss epoch: 0.07995170890028813\n",
            "Training accuracy epoch: 1979.4375\n",
            "Number of batches: 2034\n",
            "Number of Sentences Trained on: 32544\n",
            "Training loss epoch: 0.07991715988130835\n",
            "Training accuracy epoch: 1980.4375\n",
            "Number of batches: 2035\n",
            "Number of Sentences Trained on: 32560\n",
            "Training loss epoch: 0.07988291912207443\n",
            "Training accuracy epoch: 1981.4375\n",
            "Number of batches: 2036\n",
            "Number of Sentences Trained on: 32576\n",
            "Training loss epoch: 0.08000816020010271\n",
            "Training accuracy epoch: 1982.3125\n",
            "Number of batches: 2037\n",
            "Number of Sentences Trained on: 32592\n",
            "Training loss epoch: 0.08003105402287378\n",
            "Training accuracy epoch: 1983.1875\n",
            "Number of batches: 2038\n",
            "Number of Sentences Trained on: 32608\n",
            "Training loss epoch: 0.08000269014034167\n",
            "Training accuracy epoch: 1984.1875\n",
            "Number of batches: 2039\n",
            "Number of Sentences Trained on: 32624\n",
            "Training loss epoch: 0.07998800178159167\n",
            "Training accuracy epoch: 1985.125\n",
            "Number of batches: 2040\n",
            "Number of Sentences Trained on: 32640\n",
            "Training loss epoch: 0.07999398691998874\n",
            "Training accuracy epoch: 1986.125\n",
            "Number of batches: 2041\n",
            "Number of Sentences Trained on: 32656\n",
            "Training loss epoch: 0.07995908128263952\n",
            "Training accuracy epoch: 1987.125\n",
            "Number of batches: 2042\n",
            "Number of Sentences Trained on: 32672\n",
            "Training loss epoch: 0.07993072703612293\n",
            "Training accuracy epoch: 1988.125\n",
            "Number of batches: 2043\n",
            "Number of Sentences Trained on: 32688\n",
            "Training loss epoch: 0.07989484592634602\n",
            "Training accuracy epoch: 1989.125\n",
            "Number of batches: 2044\n",
            "Number of Sentences Trained on: 32704\n",
            "Training loss epoch: 0.07986936724984114\n",
            "Training accuracy epoch: 1990.125\n",
            "Number of batches: 2045\n",
            "Number of Sentences Trained on: 32720\n",
            "Training loss epoch: 0.0798474644164229\n",
            "Training accuracy epoch: 1991.125\n",
            "Number of batches: 2046\n",
            "Number of Sentences Trained on: 32736\n",
            "Training loss epoch: 0.07981752763776931\n",
            "Training accuracy epoch: 1992.0625\n",
            "Number of batches: 2047\n",
            "Number of Sentences Trained on: 32752\n",
            "Training loss epoch: 0.07980986689399572\n",
            "Training accuracy epoch: 1993.0625\n",
            "Number of batches: 2048\n",
            "Number of Sentences Trained on: 32768\n",
            "Training loss epoch: 0.07979519004107039\n",
            "Training accuracy epoch: 1994.0625\n",
            "Number of batches: 2049\n",
            "Number of Sentences Trained on: 32784\n",
            "Training loss epoch: 0.07981478793883026\n",
            "Training accuracy epoch: 1995.0\n",
            "Number of batches: 2050\n",
            "Number of Sentences Trained on: 32800\n",
            "Training loss epoch: 0.07977657879478033\n",
            "Training accuracy epoch: 1996.0\n",
            "Number of batches: 2051\n",
            "Number of Sentences Trained on: 32816\n",
            "Training loss epoch: 0.07979832431218929\n",
            "Training accuracy epoch: 1996.9375\n",
            "Number of batches: 2052\n",
            "Number of Sentences Trained on: 32832\n",
            "Training loss epoch: 0.07977048936119942\n",
            "Training accuracy epoch: 1997.9375\n",
            "Number of batches: 2053\n",
            "Number of Sentences Trained on: 32848\n",
            "Training loss epoch: 0.07973626237252952\n",
            "Training accuracy epoch: 1998.9375\n",
            "Number of batches: 2054\n",
            "Number of Sentences Trained on: 32864\n",
            "Training loss epoch: 0.07970835149522991\n",
            "Training accuracy epoch: 1999.9375\n",
            "Number of batches: 2055\n",
            "Number of Sentences Trained on: 32880\n",
            "Training loss epoch: 0.07967193965139721\n",
            "Training accuracy epoch: 2000.9375\n",
            "Number of batches: 2056\n",
            "Number of Sentences Trained on: 32896\n",
            "Training loss epoch: 0.07963762216587862\n",
            "Training accuracy epoch: 2001.9375\n",
            "Number of batches: 2057\n",
            "Number of Sentences Trained on: 32912\n",
            "Training loss epoch: 0.07961114949018247\n",
            "Training accuracy epoch: 2002.9375\n",
            "Number of batches: 2058\n",
            "Number of Sentences Trained on: 32928\n",
            "Training loss epoch: 0.07958001149659141\n",
            "Training accuracy epoch: 2003.9375\n",
            "Number of batches: 2059\n",
            "Number of Sentences Trained on: 32944\n",
            "Training loss epoch: 0.07955441573281259\n",
            "Training accuracy epoch: 2004.9375\n",
            "Number of batches: 2060\n",
            "Number of Sentences Trained on: 32960\n",
            "Training loss epoch: 0.07953234401886299\n",
            "Training accuracy epoch: 2005.875\n",
            "Number of batches: 2061\n",
            "Number of Sentences Trained on: 32976\n",
            "Training loss epoch: 0.07949958665053498\n",
            "Training accuracy epoch: 2006.8125\n",
            "Number of batches: 2062\n",
            "Number of Sentences Trained on: 32992\n",
            "Training loss epoch: 0.07964509482866176\n",
            "Training accuracy epoch: 2007.75\n",
            "Number of batches: 2063\n",
            "Number of Sentences Trained on: 33008\n",
            "Training loss epoch: 0.07960998621901966\n",
            "Training accuracy epoch: 2008.75\n",
            "Number of batches: 2064\n",
            "Number of Sentences Trained on: 33024\n",
            "Training loss epoch: 0.07958184517865051\n",
            "Training accuracy epoch: 2009.75\n",
            "Number of batches: 2065\n",
            "Number of Sentences Trained on: 33040\n",
            "Training loss epoch: 0.07965929937175742\n",
            "Training accuracy epoch: 2010.6875\n",
            "Number of batches: 2066\n",
            "Number of Sentences Trained on: 33056\n",
            "Training loss epoch: 0.07962412219669458\n",
            "Training accuracy epoch: 2011.6875\n",
            "Number of batches: 2067\n",
            "Number of Sentences Trained on: 33072\n",
            "Training loss epoch: 0.07962045805384704\n",
            "Training accuracy epoch: 2012.625\n",
            "Number of batches: 2068\n",
            "Number of Sentences Trained on: 33088\n",
            "Training loss epoch: 0.0795938104204209\n",
            "Training accuracy epoch: 2013.625\n",
            "Number of batches: 2069\n",
            "Number of Sentences Trained on: 33104\n",
            "Training loss epoch: 0.07956914375841243\n",
            "Training accuracy epoch: 2014.625\n",
            "Number of batches: 2070\n",
            "Number of Sentences Trained on: 33120\n",
            "Training loss epoch: 0.07957284921277523\n",
            "Training accuracy epoch: 2015.5625\n",
            "Number of batches: 2071\n",
            "Number of Sentences Trained on: 33136\n",
            "Training loss epoch: 0.07953839111523076\n",
            "Training accuracy epoch: 2016.5625\n",
            "Number of batches: 2072\n",
            "Number of Sentences Trained on: 33152\n",
            "Training loss epoch: 0.07965415377933652\n",
            "Training accuracy epoch: 2017.3125\n",
            "Number of batches: 2073\n",
            "Number of Sentences Trained on: 33168\n",
            "Training loss epoch: 0.07962317244057297\n",
            "Training accuracy epoch: 2018.3125\n",
            "Number of batches: 2074\n",
            "Number of Sentences Trained on: 33184\n",
            "Training loss epoch: 0.07958883483065236\n",
            "Training accuracy epoch: 2019.3125\n",
            "Number of batches: 2075\n",
            "Number of Sentences Trained on: 33200\n",
            "Training loss epoch: 0.07962048696931902\n",
            "Training accuracy epoch: 2020.25\n",
            "Number of batches: 2076\n",
            "Number of Sentences Trained on: 33216\n",
            "Training loss epoch: 0.07958863454546744\n",
            "Training accuracy epoch: 2021.25\n",
            "Number of batches: 2077\n",
            "Number of Sentences Trained on: 33232\n",
            "Training loss epoch: 0.07958811894699241\n",
            "Training accuracy epoch: 2022.1875\n",
            "Number of batches: 2078\n",
            "Number of Sentences Trained on: 33248\n",
            "Training loss epoch: 0.07957681873489895\n",
            "Training accuracy epoch: 2023.1875\n",
            "Number of batches: 2079\n",
            "Number of Sentences Trained on: 33264\n",
            "Training loss epoch: 0.07961340772734939\n",
            "Training accuracy epoch: 2024.125\n",
            "Number of batches: 2080\n",
            "Number of Sentences Trained on: 33280\n",
            "Training loss epoch: 0.07961928317893543\n",
            "Training accuracy epoch: 2025.0625\n",
            "Number of batches: 2081\n",
            "Number of Sentences Trained on: 33296\n",
            "Training loss epoch: 0.07958372475347725\n",
            "Training accuracy epoch: 2026.0625\n",
            "Number of batches: 2082\n",
            "Number of Sentences Trained on: 33312\n",
            "Training loss epoch: 0.07964652974016402\n",
            "Training accuracy epoch: 2027.0\n",
            "Number of batches: 2083\n",
            "Number of Sentences Trained on: 33328\n",
            "Training loss epoch: 0.07963531969539742\n",
            "Training accuracy epoch: 2028.0\n",
            "Number of batches: 2084\n",
            "Number of Sentences Trained on: 33344\n",
            "Training loss epoch: 0.07972523438951115\n",
            "Training accuracy epoch: 2028.9375\n",
            "Number of batches: 2085\n",
            "Number of Sentences Trained on: 33360\n",
            "Training loss epoch: 0.07972327616254204\n",
            "Training accuracy epoch: 2029.875\n",
            "Number of batches: 2086\n",
            "Number of Sentences Trained on: 33376\n",
            "Training loss epoch: 0.07971436617851392\n",
            "Training accuracy epoch: 2030.875\n",
            "Number of batches: 2087\n",
            "Number of Sentences Trained on: 33392\n",
            "Training loss epoch: 0.07968071123157859\n",
            "Training accuracy epoch: 2031.875\n",
            "Number of batches: 2088\n",
            "Number of Sentences Trained on: 33408\n",
            "Training loss epoch: 0.07981034970750964\n",
            "Training accuracy epoch: 2032.8125\n",
            "Number of batches: 2089\n",
            "Number of Sentences Trained on: 33424\n",
            "Training loss epoch: 0.07977879806211426\n",
            "Training accuracy epoch: 2033.8125\n",
            "Number of batches: 2090\n",
            "Number of Sentences Trained on: 33440\n",
            "Training loss epoch: 0.07974222585108652\n",
            "Training accuracy epoch: 2034.8125\n",
            "Number of batches: 2091\n",
            "Number of Sentences Trained on: 33456\n",
            "Training loss epoch: 0.07970534661845369\n",
            "Training accuracy epoch: 2035.8125\n",
            "Number of batches: 2092\n",
            "Number of Sentences Trained on: 33472\n",
            "Training loss epoch: 0.07967371167026861\n",
            "Training accuracy epoch: 2036.8125\n",
            "Number of batches: 2093\n",
            "Number of Sentences Trained on: 33488\n",
            "Training loss epoch: 0.07965112965052368\n",
            "Training accuracy epoch: 2037.8125\n",
            "Number of batches: 2094\n",
            "Number of Sentences Trained on: 33504\n",
            "Training loss epoch: 0.07962658005981342\n",
            "Training accuracy epoch: 2038.75\n",
            "Number of batches: 2095\n",
            "Number of Sentences Trained on: 33520\n",
            "Training loss epoch: 0.07963397511416206\n",
            "Training accuracy epoch: 2039.6875\n",
            "Number of batches: 2096\n",
            "Number of Sentences Trained on: 33536\n",
            "Training loss epoch: 0.07961855970227211\n",
            "Training accuracy epoch: 2040.6875\n",
            "Number of batches: 2097\n",
            "Number of Sentences Trained on: 33552\n",
            "Training loss epoch: 0.07961357668724084\n",
            "Training accuracy epoch: 2041.625\n",
            "Number of batches: 2098\n",
            "Number of Sentences Trained on: 33568\n",
            "Training loss epoch: 0.07957834062960344\n",
            "Training accuracy epoch: 2042.625\n",
            "Number of batches: 2099\n",
            "Number of Sentences Trained on: 33584\n",
            "Training loss epoch: 0.07965611271306573\n",
            "Training accuracy epoch: 2043.5\n",
            "Number of batches: 2100\n",
            "Number of Sentences Trained on: 33600\n",
            "Training loss per 100 training steps: 0.07964115363532916\n",
            "Training loss epoch: 0.07964115363532916\n",
            "Training accuracy epoch: 2044.5\n",
            "Number of batches: 2101\n",
            "Number of Sentences Trained on: 33616\n",
            "Training loss epoch: 0.0796876182536344\n",
            "Training accuracy epoch: 2045.4375\n",
            "Number of batches: 2102\n",
            "Number of Sentences Trained on: 33632\n",
            "Training loss epoch: 0.07981403977934838\n",
            "Training accuracy epoch: 2046.3125\n",
            "Number of batches: 2103\n",
            "Number of Sentences Trained on: 33648\n",
            "Training loss epoch: 0.07981361305272128\n",
            "Training accuracy epoch: 2047.25\n",
            "Number of batches: 2104\n",
            "Number of Sentences Trained on: 33664\n",
            "Training loss epoch: 0.07978655255086899\n",
            "Training accuracy epoch: 2048.25\n",
            "Number of batches: 2105\n",
            "Number of Sentences Trained on: 33680\n",
            "Training loss epoch: 0.0797537918865389\n",
            "Training accuracy epoch: 2049.25\n",
            "Number of batches: 2106\n",
            "Number of Sentences Trained on: 33696\n",
            "Training loss epoch: 0.07974831631183538\n",
            "Training accuracy epoch: 2050.25\n",
            "Number of batches: 2107\n",
            "Number of Sentences Trained on: 33712\n",
            "Training loss epoch: 0.0797182263156486\n",
            "Training accuracy epoch: 2051.25\n",
            "Number of batches: 2108\n",
            "Number of Sentences Trained on: 33728\n",
            "Training loss epoch: 0.07969319300880669\n",
            "Training accuracy epoch: 2052.25\n",
            "Number of batches: 2109\n",
            "Number of Sentences Trained on: 33744\n",
            "Training loss epoch: 0.07966417817817394\n",
            "Training accuracy epoch: 2053.25\n",
            "Number of batches: 2110\n",
            "Number of Sentences Trained on: 33760\n",
            "Training loss epoch: 0.07976266273721395\n",
            "Training accuracy epoch: 2054.1875\n",
            "Number of batches: 2111\n",
            "Number of Sentences Trained on: 33776\n",
            "Training loss epoch: 0.07985945172209319\n",
            "Training accuracy epoch: 2055.125\n",
            "Number of batches: 2112\n",
            "Number of Sentences Trained on: 33792\n",
            "Training loss epoch: 0.07988683708682112\n",
            "Training accuracy epoch: 2056.0625\n",
            "Number of batches: 2113\n",
            "Number of Sentences Trained on: 33808\n",
            "Training loss epoch: 0.07997303451807095\n",
            "Training accuracy epoch: 2057.0\n",
            "Number of batches: 2114\n",
            "Number of Sentences Trained on: 33824\n",
            "Training loss epoch: 0.08003777083744026\n",
            "Training accuracy epoch: 2057.9375\n",
            "Number of batches: 2115\n",
            "Number of Sentences Trained on: 33840\n",
            "Training loss epoch: 0.08014204829110905\n",
            "Training accuracy epoch: 2058.875\n",
            "Number of batches: 2116\n",
            "Number of Sentences Trained on: 33856\n",
            "Training loss epoch: 0.08011837867679492\n",
            "Training accuracy epoch: 2059.875\n",
            "Number of batches: 2117\n",
            "Number of Sentences Trained on: 33872\n",
            "Training loss epoch: 0.08008381880887569\n",
            "Training accuracy epoch: 2060.875\n",
            "Number of batches: 2118\n",
            "Number of Sentences Trained on: 33888\n",
            "Training loss epoch: 0.08005362564583644\n",
            "Training accuracy epoch: 2061.875\n",
            "Number of batches: 2119\n",
            "Number of Sentences Trained on: 33904\n",
            "Training loss epoch: 0.08007449078328857\n",
            "Training accuracy epoch: 2062.8125\n",
            "Number of batches: 2120\n",
            "Number of Sentences Trained on: 33920\n",
            "Training loss epoch: 0.08009288930805356\n",
            "Training accuracy epoch: 2063.75\n",
            "Number of batches: 2121\n",
            "Number of Sentences Trained on: 33936\n",
            "Training loss epoch: 0.08015179082576651\n",
            "Training accuracy epoch: 2064.75\n",
            "Number of batches: 2122\n",
            "Number of Sentences Trained on: 33952\n",
            "Training loss epoch: 0.08016734089547128\n",
            "Training accuracy epoch: 2065.6875\n",
            "Number of batches: 2123\n",
            "Number of Sentences Trained on: 33968\n",
            "Training loss epoch: 0.0801937444469583\n",
            "Training accuracy epoch: 2066.625\n",
            "Number of batches: 2124\n",
            "Number of Sentences Trained on: 33984\n",
            "Training loss epoch: 0.08016077580205713\n",
            "Training accuracy epoch: 2067.625\n",
            "Number of batches: 2125\n",
            "Number of Sentences Trained on: 34000\n",
            "Training loss epoch: 0.08014580013785086\n",
            "Training accuracy epoch: 2068.625\n",
            "Number of batches: 2126\n",
            "Number of Sentences Trained on: 34016\n",
            "Training loss epoch: 0.08025980394387075\n",
            "Training accuracy epoch: 2069.5\n",
            "Number of batches: 2127\n",
            "Number of Sentences Trained on: 34032\n",
            "Training loss epoch: 0.08024250384090222\n",
            "Training accuracy epoch: 2070.4375\n",
            "Number of batches: 2128\n",
            "Number of Sentences Trained on: 34048\n",
            "Training loss epoch: 0.08021403469718406\n",
            "Training accuracy epoch: 2071.4375\n",
            "Number of batches: 2129\n",
            "Number of Sentences Trained on: 34064\n",
            "Training loss epoch: 0.08031691191708418\n",
            "Training accuracy epoch: 2072.375\n",
            "Number of batches: 2130\n",
            "Number of Sentences Trained on: 34080\n",
            "Training loss epoch: 0.0803017505993858\n",
            "Training accuracy epoch: 2073.375\n",
            "Number of batches: 2131\n",
            "Number of Sentences Trained on: 34096\n",
            "Training loss epoch: 0.08027480983281104\n",
            "Training accuracy epoch: 2074.375\n",
            "Number of batches: 2132\n",
            "Number of Sentences Trained on: 34112\n",
            "Training loss epoch: 0.08023786416771604\n",
            "Training accuracy epoch: 2075.375\n",
            "Number of batches: 2133\n",
            "Number of Sentences Trained on: 34128\n",
            "Training loss epoch: 0.08029545962821918\n",
            "Training accuracy epoch: 2076.3125\n",
            "Number of batches: 2134\n",
            "Number of Sentences Trained on: 34144\n",
            "Training loss epoch: 0.08026702813183904\n",
            "Training accuracy epoch: 2077.3125\n",
            "Number of batches: 2135\n",
            "Number of Sentences Trained on: 34160\n",
            "Training loss epoch: 0.08023129077466112\n",
            "Training accuracy epoch: 2078.3125\n",
            "Number of batches: 2136\n",
            "Number of Sentences Trained on: 34176\n",
            "Training loss epoch: 0.0801978485854361\n",
            "Training accuracy epoch: 2079.3125\n",
            "Number of batches: 2137\n",
            "Number of Sentences Trained on: 34192\n",
            "Training loss epoch: 0.08016738190543103\n",
            "Training accuracy epoch: 2080.3125\n",
            "Number of batches: 2138\n",
            "Number of Sentences Trained on: 34208\n",
            "Training loss epoch: 0.08014897465789524\n",
            "Training accuracy epoch: 2081.3125\n",
            "Number of batches: 2139\n",
            "Number of Sentences Trained on: 34224\n",
            "Training loss epoch: 0.08030940353508362\n",
            "Training accuracy epoch: 2082.25\n",
            "Number of batches: 2140\n",
            "Number of Sentences Trained on: 34240\n",
            "Training loss epoch: 0.08027913381863702\n",
            "Training accuracy epoch: 2083.25\n",
            "Number of batches: 2141\n",
            "Number of Sentences Trained on: 34256\n",
            "Training loss epoch: 0.08024427321330846\n",
            "Training accuracy epoch: 2084.25\n",
            "Number of batches: 2142\n",
            "Number of Sentences Trained on: 34272\n",
            "Training loss epoch: 0.08025151174949259\n",
            "Training accuracy epoch: 2085.25\n",
            "Number of batches: 2143\n",
            "Number of Sentences Trained on: 34288\n",
            "Training loss epoch: 0.08021439177155659\n",
            "Training accuracy epoch: 2086.25\n",
            "Number of batches: 2144\n",
            "Number of Sentences Trained on: 34304\n",
            "Training loss epoch: 0.08023847467601158\n",
            "Training accuracy epoch: 2087.1875\n",
            "Number of batches: 2145\n",
            "Number of Sentences Trained on: 34320\n",
            "Training loss epoch: 0.08023099090190045\n",
            "Training accuracy epoch: 2088.1875\n",
            "Number of batches: 2146\n",
            "Number of Sentences Trained on: 34336\n",
            "Training loss epoch: 0.08020938649354947\n",
            "Training accuracy epoch: 2089.1875\n",
            "Number of batches: 2147\n",
            "Number of Sentences Trained on: 34352\n",
            "Training loss epoch: 0.08017499586960483\n",
            "Training accuracy epoch: 2090.1875\n",
            "Number of batches: 2148\n",
            "Number of Sentences Trained on: 34368\n",
            "Training loss epoch: 0.08018828460662765\n",
            "Training accuracy epoch: 2091.125\n",
            "Number of batches: 2149\n",
            "Number of Sentences Trained on: 34384\n",
            "Training loss epoch: 0.08018226802239443\n",
            "Training accuracy epoch: 2092.0625\n",
            "Number of batches: 2150\n",
            "Number of Sentences Trained on: 34400\n",
            "Training loss epoch: 0.08016475773320286\n",
            "Training accuracy epoch: 2093.0625\n",
            "Number of batches: 2151\n",
            "Number of Sentences Trained on: 34416\n",
            "Training loss epoch: 0.08023851218118735\n",
            "Training accuracy epoch: 2094.0\n",
            "Number of batches: 2152\n",
            "Number of Sentences Trained on: 34432\n",
            "Training loss epoch: 0.08020187864457001\n",
            "Training accuracy epoch: 2095.0\n",
            "Number of batches: 2153\n",
            "Number of Sentences Trained on: 34448\n",
            "Training loss epoch: 0.08025140954426002\n",
            "Training accuracy epoch: 2095.9375\n",
            "Number of batches: 2154\n",
            "Number of Sentences Trained on: 34464\n",
            "Training loss epoch: 0.08023522395186565\n",
            "Training accuracy epoch: 2096.9375\n",
            "Number of batches: 2155\n",
            "Number of Sentences Trained on: 34480\n",
            "Training loss epoch: 0.08026254793515605\n",
            "Training accuracy epoch: 2097.8125\n",
            "Number of batches: 2156\n",
            "Number of Sentences Trained on: 34496\n",
            "Training loss epoch: 0.08029065881745415\n",
            "Training accuracy epoch: 2098.75\n",
            "Number of batches: 2157\n",
            "Number of Sentences Trained on: 34512\n",
            "Training loss epoch: 0.08025460293275864\n",
            "Training accuracy epoch: 2099.75\n",
            "Number of batches: 2158\n",
            "Number of Sentences Trained on: 34528\n",
            "Training loss epoch: 0.0802326841115702\n",
            "Training accuracy epoch: 2100.6875\n",
            "Number of batches: 2159\n",
            "Number of Sentences Trained on: 34544\n",
            "Training loss epoch: 0.08019994512663592\n",
            "Training accuracy epoch: 2101.6875\n",
            "Number of batches: 2160\n",
            "Number of Sentences Trained on: 34560\n",
            "Training loss epoch: 0.08017757781509477\n",
            "Training accuracy epoch: 2102.6875\n",
            "Number of batches: 2161\n",
            "Number of Sentences Trained on: 34576\n",
            "Training loss epoch: 0.08015336825162726\n",
            "Training accuracy epoch: 2103.6875\n",
            "Number of batches: 2162\n",
            "Number of Sentences Trained on: 34592\n",
            "Training loss epoch: 0.08018366445836927\n",
            "Training accuracy epoch: 2104.625\n",
            "Number of batches: 2163\n",
            "Number of Sentences Trained on: 34608\n",
            "Training loss epoch: 0.08016450583171468\n",
            "Training accuracy epoch: 2105.625\n",
            "Number of batches: 2164\n",
            "Number of Sentences Trained on: 34624\n",
            "Training loss epoch: 0.08017888187378909\n",
            "Training accuracy epoch: 2106.5625\n",
            "Number of batches: 2165\n",
            "Number of Sentences Trained on: 34640\n",
            "Training loss epoch: 0.08015154103573233\n",
            "Training accuracy epoch: 2107.5625\n",
            "Number of batches: 2166\n",
            "Number of Sentences Trained on: 34656\n",
            "Training loss epoch: 0.08011605372786662\n",
            "Training accuracy epoch: 2108.5625\n",
            "Number of batches: 2167\n",
            "Number of Sentences Trained on: 34672\n",
            "Training loss epoch: 0.08009771603720794\n",
            "Training accuracy epoch: 2109.5625\n",
            "Number of batches: 2168\n",
            "Number of Sentences Trained on: 34688\n",
            "Training loss epoch: 0.0800800840717483\n",
            "Training accuracy epoch: 2110.5625\n",
            "Number of batches: 2169\n",
            "Number of Sentences Trained on: 34704\n",
            "Training loss epoch: 0.08008554937152465\n",
            "Training accuracy epoch: 2111.5\n",
            "Number of batches: 2170\n",
            "Number of Sentences Trained on: 34720\n",
            "Training loss epoch: 0.0800708821932842\n",
            "Training accuracy epoch: 2112.5\n",
            "Number of batches: 2171\n",
            "Number of Sentences Trained on: 34736\n",
            "Training loss epoch: 0.08007144237305225\n",
            "Training accuracy epoch: 2113.5\n",
            "Number of batches: 2172\n",
            "Number of Sentences Trained on: 34752\n",
            "Training loss epoch: 0.08007570048761738\n",
            "Training accuracy epoch: 2114.4375\n",
            "Number of batches: 2173\n",
            "Number of Sentences Trained on: 34768\n",
            "Training loss epoch: 0.08017850101262444\n",
            "Training accuracy epoch: 2115.3125\n",
            "Number of batches: 2174\n",
            "Number of Sentences Trained on: 34784\n",
            "Training loss epoch: 0.08014513503303805\n",
            "Training accuracy epoch: 2116.3125\n",
            "Number of batches: 2175\n",
            "Number of Sentences Trained on: 34800\n",
            "Training loss epoch: 0.08011168800822709\n",
            "Training accuracy epoch: 2117.3125\n",
            "Number of batches: 2176\n",
            "Number of Sentences Trained on: 34816\n",
            "Training loss epoch: 0.08012107620265736\n",
            "Training accuracy epoch: 2118.3125\n",
            "Number of batches: 2177\n",
            "Number of Sentences Trained on: 34832\n",
            "Training loss epoch: 0.08014432933192803\n",
            "Training accuracy epoch: 2119.25\n",
            "Number of batches: 2178\n",
            "Number of Sentences Trained on: 34848\n",
            "Training loss epoch: 0.08021021949782266\n",
            "Training accuracy epoch: 2120.1875\n",
            "Number of batches: 2179\n",
            "Number of Sentences Trained on: 34864\n",
            "Training loss epoch: 0.08021613197638001\n",
            "Training accuracy epoch: 2121.1875\n",
            "Number of batches: 2180\n",
            "Number of Sentences Trained on: 34880\n",
            "Training loss epoch: 0.08018343377290629\n",
            "Training accuracy epoch: 2122.1875\n",
            "Number of batches: 2181\n",
            "Number of Sentences Trained on: 34896\n",
            "Training loss epoch: 0.08016106466359962\n",
            "Training accuracy epoch: 2123.1875\n",
            "Number of batches: 2182\n",
            "Number of Sentences Trained on: 34912\n",
            "Training loss epoch: 0.08013189905536135\n",
            "Training accuracy epoch: 2124.1875\n",
            "Number of batches: 2183\n",
            "Number of Sentences Trained on: 34928\n",
            "Training loss epoch: 0.08013626818488181\n",
            "Training accuracy epoch: 2125.1875\n",
            "Number of batches: 2184\n",
            "Number of Sentences Trained on: 34944\n",
            "Training loss epoch: 0.08019810251445932\n",
            "Training accuracy epoch: 2126.125\n",
            "Number of batches: 2185\n",
            "Number of Sentences Trained on: 34960\n",
            "Training loss epoch: 0.08020549710967106\n",
            "Training accuracy epoch: 2127.0625\n",
            "Number of batches: 2186\n",
            "Number of Sentences Trained on: 34976\n",
            "Training loss epoch: 0.08019464261159732\n",
            "Training accuracy epoch: 2128.0\n",
            "Number of batches: 2187\n",
            "Number of Sentences Trained on: 34992\n",
            "Training loss epoch: 0.08017828111369255\n",
            "Training accuracy epoch: 2129.0\n",
            "Number of batches: 2188\n",
            "Number of Sentences Trained on: 35008\n",
            "Training loss epoch: 0.08014610910810462\n",
            "Training accuracy epoch: 2130.0\n",
            "Number of batches: 2189\n",
            "Number of Sentences Trained on: 35024\n",
            "Training loss epoch: 0.08011044095993014\n",
            "Training accuracy epoch: 2131.0\n",
            "Number of batches: 2190\n",
            "Number of Sentences Trained on: 35040\n",
            "Training loss epoch: 0.08007730300138838\n",
            "Training accuracy epoch: 2132.0\n",
            "Number of batches: 2191\n",
            "Number of Sentences Trained on: 35056\n",
            "Training loss epoch: 0.08013652746372482\n",
            "Training accuracy epoch: 2132.9375\n",
            "Number of batches: 2192\n",
            "Number of Sentences Trained on: 35072\n",
            "Training loss epoch: 0.08012040161242467\n",
            "Training accuracy epoch: 2133.9375\n",
            "Number of batches: 2193\n",
            "Number of Sentences Trained on: 35088\n",
            "Training loss epoch: 0.08010859953187507\n",
            "Training accuracy epoch: 2134.9375\n",
            "Number of batches: 2194\n",
            "Number of Sentences Trained on: 35104\n",
            "Training loss epoch: 0.08008295744406918\n",
            "Training accuracy epoch: 2135.9375\n",
            "Number of batches: 2195\n",
            "Number of Sentences Trained on: 35120\n",
            "Training loss epoch: 0.08005902663603302\n",
            "Training accuracy epoch: 2136.9375\n",
            "Number of batches: 2196\n",
            "Number of Sentences Trained on: 35136\n",
            "Training loss epoch: 0.0800668033782212\n",
            "Training accuracy epoch: 2137.9375\n",
            "Number of batches: 2197\n",
            "Number of Sentences Trained on: 35152\n",
            "Training loss epoch: 0.08003633380409665\n",
            "Training accuracy epoch: 2138.9375\n",
            "Number of batches: 2198\n",
            "Number of Sentences Trained on: 35168\n",
            "Training loss epoch: 0.08001021662686075\n",
            "Training accuracy epoch: 2139.9375\n",
            "Number of batches: 2199\n",
            "Number of Sentences Trained on: 35184\n",
            "Training loss epoch: 0.0799955586932827\n",
            "Training accuracy epoch: 2140.9375\n",
            "Number of batches: 2200\n",
            "Number of Sentences Trained on: 35200\n",
            "Training loss per 100 training steps: 0.07996631854821556\n",
            "Training loss epoch: 0.07996631854821556\n",
            "Training accuracy epoch: 2141.9375\n",
            "Number of batches: 2201\n",
            "Number of Sentences Trained on: 35216\n",
            "Training loss epoch: 0.08000616294181016\n",
            "Training accuracy epoch: 2142.875\n",
            "Number of batches: 2202\n",
            "Number of Sentences Trained on: 35232\n",
            "Training loss epoch: 0.07997833408843724\n",
            "Training accuracy epoch: 2143.875\n",
            "Number of batches: 2203\n",
            "Number of Sentences Trained on: 35248\n",
            "Training loss epoch: 0.07998560012758238\n",
            "Training accuracy epoch: 2144.8125\n",
            "Number of batches: 2204\n",
            "Number of Sentences Trained on: 35264\n",
            "Training loss epoch: 0.0799630349198258\n",
            "Training accuracy epoch: 2145.8125\n",
            "Number of batches: 2205\n",
            "Number of Sentences Trained on: 35280\n",
            "Training loss epoch: 0.0799304303592796\n",
            "Training accuracy epoch: 2146.8125\n",
            "Number of batches: 2206\n",
            "Number of Sentences Trained on: 35296\n",
            "Training loss epoch: 0.08001454043706956\n",
            "Training accuracy epoch: 2147.75\n",
            "Number of batches: 2207\n",
            "Number of Sentences Trained on: 35312\n",
            "Training loss epoch: 0.07997980561148048\n",
            "Training accuracy epoch: 2148.75\n",
            "Number of batches: 2208\n",
            "Number of Sentences Trained on: 35328\n",
            "Training loss epoch: 0.0799528427173485\n",
            "Training accuracy epoch: 2149.75\n",
            "Number of batches: 2209\n",
            "Number of Sentences Trained on: 35344\n",
            "Training loss epoch: 0.0799416097318643\n",
            "Training accuracy epoch: 2150.75\n",
            "Number of batches: 2210\n",
            "Number of Sentences Trained on: 35360\n",
            "Training loss epoch: 0.07993442462502127\n",
            "Training accuracy epoch: 2151.75\n",
            "Number of batches: 2211\n",
            "Number of Sentences Trained on: 35376\n",
            "Training loss epoch: 0.07990009280354629\n",
            "Training accuracy epoch: 2152.75\n",
            "Number of batches: 2212\n",
            "Number of Sentences Trained on: 35392\n",
            "Training loss epoch: 0.07986912661563414\n",
            "Training accuracy epoch: 2153.75\n",
            "Number of batches: 2213\n",
            "Number of Sentences Trained on: 35408\n",
            "Training loss epoch: 0.07983791587830703\n",
            "Training accuracy epoch: 2154.75\n",
            "Number of batches: 2214\n",
            "Number of Sentences Trained on: 35424\n",
            "Training loss epoch: 0.07980529223530633\n",
            "Training accuracy epoch: 2155.75\n",
            "Number of batches: 2215\n",
            "Number of Sentences Trained on: 35440\n",
            "Training loss epoch: 0.07978839445297868\n",
            "Training accuracy epoch: 2156.6875\n",
            "Number of batches: 2216\n",
            "Number of Sentences Trained on: 35456\n",
            "Training loss epoch: 0.07976582455141884\n",
            "Training accuracy epoch: 2157.6875\n",
            "Number of batches: 2217\n",
            "Number of Sentences Trained on: 35472\n",
            "Training loss epoch: 0.07985340714165921\n",
            "Training accuracy epoch: 2158.625\n",
            "Number of batches: 2218\n",
            "Number of Sentences Trained on: 35488\n",
            "Training loss epoch: 0.07982197905866878\n",
            "Training accuracy epoch: 2159.625\n",
            "Number of batches: 2219\n",
            "Number of Sentences Trained on: 35504\n",
            "Training loss epoch: 0.07983687578081436\n",
            "Training accuracy epoch: 2160.5625\n",
            "Number of batches: 2220\n",
            "Number of Sentences Trained on: 35520\n",
            "Training loss epoch: 0.07988933059189564\n",
            "Training accuracy epoch: 2161.4375\n",
            "Number of batches: 2221\n",
            "Number of Sentences Trained on: 35536\n",
            "Training loss epoch: 0.07993930751580355\n",
            "Training accuracy epoch: 2162.375\n",
            "Number of batches: 2222\n",
            "Number of Sentences Trained on: 35552\n",
            "Training loss epoch: 0.07990612994304844\n",
            "Training accuracy epoch: 2163.375\n",
            "Number of batches: 2223\n",
            "Number of Sentences Trained on: 35568\n",
            "Training loss epoch: 0.07988728139385497\n",
            "Training accuracy epoch: 2164.3125\n",
            "Number of batches: 2224\n",
            "Number of Sentences Trained on: 35584\n",
            "Training loss epoch: 0.07985392351806153\n",
            "Training accuracy epoch: 2165.3125\n",
            "Number of batches: 2225\n",
            "Number of Sentences Trained on: 35600\n",
            "Training loss epoch: 0.07982878018852718\n",
            "Training accuracy epoch: 2166.3125\n",
            "Number of batches: 2226\n",
            "Number of Sentences Trained on: 35616\n",
            "Training loss epoch: 0.07979933530956389\n",
            "Training accuracy epoch: 2167.3125\n",
            "Number of batches: 2227\n",
            "Number of Sentences Trained on: 35632\n",
            "Training loss epoch: 0.0797859326263912\n",
            "Training accuracy epoch: 2168.25\n",
            "Number of batches: 2228\n",
            "Number of Sentences Trained on: 35648\n",
            "Training loss epoch: 0.07978293641466769\n",
            "Training accuracy epoch: 2169.25\n",
            "Number of batches: 2229\n",
            "Number of Sentences Trained on: 35664\n",
            "Training loss epoch: 0.07975626969599059\n",
            "Training accuracy epoch: 2170.25\n",
            "Number of batches: 2230\n",
            "Number of Sentences Trained on: 35680\n",
            "Training loss epoch: 0.07976416121232545\n",
            "Training accuracy epoch: 2171.25\n",
            "Number of batches: 2231\n",
            "Number of Sentences Trained on: 35696\n",
            "Training loss epoch: 0.079732783539047\n",
            "Training accuracy epoch: 2172.25\n",
            "Number of batches: 2232\n",
            "Number of Sentences Trained on: 35712\n",
            "Training loss epoch: 0.07971916412110683\n",
            "Training accuracy epoch: 2173.25\n",
            "Number of batches: 2233\n",
            "Number of Sentences Trained on: 35728\n",
            "Training loss epoch: 0.0796875028393848\n",
            "Training accuracy epoch: 2174.25\n",
            "Number of batches: 2234\n",
            "Number of Sentences Trained on: 35744\n",
            "Training loss epoch: 0.07965457412122302\n",
            "Training accuracy epoch: 2175.25\n",
            "Number of batches: 2235\n",
            "Number of Sentences Trained on: 35760\n",
            "Training loss epoch: 0.0796493260936464\n",
            "Training accuracy epoch: 2176.1875\n",
            "Number of batches: 2236\n",
            "Number of Sentences Trained on: 35776\n",
            "Training loss epoch: 0.07961737980021284\n",
            "Training accuracy epoch: 2177.1875\n",
            "Number of batches: 2237\n",
            "Number of Sentences Trained on: 35792\n",
            "Training loss epoch: 0.0797752297322104\n",
            "Training accuracy epoch: 2178.0625\n",
            "Number of batches: 2238\n",
            "Number of Sentences Trained on: 35808\n",
            "Training loss epoch: 0.07976819939535498\n",
            "Training accuracy epoch: 2179.0625\n",
            "Number of batches: 2239\n",
            "Number of Sentences Trained on: 35824\n",
            "Training loss epoch: 0.07973632470565073\n",
            "Training accuracy epoch: 2180.0625\n",
            "Number of batches: 2240\n",
            "Number of Sentences Trained on: 35840\n",
            "Training loss epoch: 0.07977027665120687\n",
            "Training accuracy epoch: 2181.0\n",
            "Number of batches: 2241\n",
            "Number of Sentences Trained on: 35856\n",
            "Training loss epoch: 0.07973737295824329\n",
            "Training accuracy epoch: 2182.0\n",
            "Number of batches: 2242\n",
            "Number of Sentences Trained on: 35872\n",
            "Training loss epoch: 0.07981619584574003\n",
            "Training accuracy epoch: 2182.875\n",
            "Number of batches: 2243\n",
            "Number of Sentences Trained on: 35888\n",
            "Training loss epoch: 0.07978185578060298\n",
            "Training accuracy epoch: 2183.875\n",
            "Number of batches: 2244\n",
            "Number of Sentences Trained on: 35904\n",
            "Training loss epoch: 0.07975152275313205\n",
            "Training accuracy epoch: 2184.875\n",
            "Number of batches: 2245\n",
            "Number of Sentences Trained on: 35920\n",
            "Training loss epoch: 0.07971794518303092\n",
            "Training accuracy epoch: 2185.875\n",
            "Number of batches: 2246\n",
            "Number of Sentences Trained on: 35936\n",
            "Training loss epoch: 0.07968440264457778\n",
            "Training accuracy epoch: 2186.875\n",
            "Number of batches: 2247\n",
            "Number of Sentences Trained on: 35952\n",
            "Training loss epoch: 0.07967291317158617\n",
            "Training accuracy epoch: 2187.8125\n",
            "Number of batches: 2248\n",
            "Number of Sentences Trained on: 35968\n",
            "Training loss epoch: 0.07964373178000889\n",
            "Training accuracy epoch: 2188.8125\n",
            "Number of batches: 2249\n",
            "Number of Sentences Trained on: 35984\n",
            "Training loss epoch: 0.07966131726376867\n",
            "Training accuracy epoch: 2189.75\n",
            "Number of batches: 2250\n",
            "Number of Sentences Trained on: 36000\n",
            "Training loss epoch: 0.07968389494909085\n",
            "Training accuracy epoch: 2190.6875\n",
            "Number of batches: 2251\n",
            "Number of Sentences Trained on: 36016\n",
            "Training loss epoch: 0.07964952974182958\n",
            "Training accuracy epoch: 2191.6875\n",
            "Number of batches: 2252\n",
            "Number of Sentences Trained on: 36032\n",
            "Training loss epoch: 0.07961472957477433\n",
            "Training accuracy epoch: 2192.6875\n",
            "Number of batches: 2253\n",
            "Number of Sentences Trained on: 36048\n",
            "Training loss epoch: 0.07960860188172156\n",
            "Training accuracy epoch: 2193.6875\n",
            "Number of batches: 2254\n",
            "Number of Sentences Trained on: 36064\n",
            "Training loss epoch: 0.07959975824517636\n",
            "Training accuracy epoch: 2194.625\n",
            "Number of batches: 2255\n",
            "Number of Sentences Trained on: 36080\n",
            "Training loss epoch: 0.07957692506839677\n",
            "Training accuracy epoch: 2195.625\n",
            "Number of batches: 2256\n",
            "Number of Sentences Trained on: 36096\n",
            "Training loss epoch: 0.07954908558996661\n",
            "Training accuracy epoch: 2196.625\n",
            "Number of batches: 2257\n",
            "Number of Sentences Trained on: 36112\n",
            "Training loss epoch: 0.0795151959483109\n",
            "Training accuracy epoch: 2197.625\n",
            "Number of batches: 2258\n",
            "Number of Sentences Trained on: 36128\n",
            "Training loss epoch: 0.0795602518521329\n",
            "Training accuracy epoch: 2198.5625\n",
            "Number of batches: 2259\n",
            "Number of Sentences Trained on: 36144\n",
            "Training loss epoch: 0.07952698236932737\n",
            "Training accuracy epoch: 2199.5625\n",
            "Number of batches: 2260\n",
            "Number of Sentences Trained on: 36160\n",
            "Training loss epoch: 0.07959783293927726\n",
            "Training accuracy epoch: 2200.5\n",
            "Number of batches: 2261\n",
            "Number of Sentences Trained on: 36176\n",
            "Training loss epoch: 0.07956616763563802\n",
            "Training accuracy epoch: 2201.5\n",
            "Number of batches: 2262\n",
            "Number of Sentences Trained on: 36192\n",
            "Training loss epoch: 0.0795394124550422\n",
            "Training accuracy epoch: 2202.5\n",
            "Number of batches: 2263\n",
            "Number of Sentences Trained on: 36208\n",
            "Training loss epoch: 0.07953723595673191\n",
            "Training accuracy epoch: 2203.5\n",
            "Number of batches: 2264\n",
            "Number of Sentences Trained on: 36224\n",
            "Training loss epoch: 0.07950345835802412\n",
            "Training accuracy epoch: 2204.5\n",
            "Number of batches: 2265\n",
            "Number of Sentences Trained on: 36240\n",
            "Training loss epoch: 0.07952306271121076\n",
            "Training accuracy epoch: 2205.4375\n",
            "Number of batches: 2266\n",
            "Number of Sentences Trained on: 36256\n",
            "Training loss epoch: 0.07948992017251243\n",
            "Training accuracy epoch: 2206.4375\n",
            "Number of batches: 2267\n",
            "Number of Sentences Trained on: 36272\n",
            "Training loss epoch: 0.07948019786953597\n",
            "Training accuracy epoch: 2207.4375\n",
            "Number of batches: 2268\n",
            "Number of Sentences Trained on: 36288\n",
            "Training loss epoch: 0.07947458650012736\n",
            "Training accuracy epoch: 2208.4375\n",
            "Number of batches: 2269\n",
            "Number of Sentences Trained on: 36304\n",
            "Training loss epoch: 0.07944974746017375\n",
            "Training accuracy epoch: 2209.4375\n",
            "Number of batches: 2270\n",
            "Number of Sentences Trained on: 36320\n",
            "Training loss epoch: 0.07941532133014403\n",
            "Training accuracy epoch: 2210.4375\n",
            "Number of batches: 2271\n",
            "Number of Sentences Trained on: 36336\n",
            "Training loss epoch: 0.07939401235975131\n",
            "Training accuracy epoch: 2211.4375\n",
            "Number of batches: 2272\n",
            "Number of Sentences Trained on: 36352\n",
            "Training loss epoch: 0.0793938552388536\n",
            "Training accuracy epoch: 2212.375\n",
            "Number of batches: 2273\n",
            "Number of Sentences Trained on: 36368\n",
            "Training loss epoch: 0.07936240058724177\n",
            "Training accuracy epoch: 2213.375\n",
            "Number of batches: 2274\n",
            "Number of Sentences Trained on: 36384\n",
            "Training loss epoch: 0.07933738115001827\n",
            "Training accuracy epoch: 2214.3125\n",
            "Number of batches: 2275\n",
            "Number of Sentences Trained on: 36400\n",
            "Training loss epoch: 0.07935644165661196\n",
            "Training accuracy epoch: 2215.25\n",
            "Number of batches: 2276\n",
            "Number of Sentences Trained on: 36416\n",
            "Training loss epoch: 0.07932202695379065\n",
            "Training accuracy epoch: 2216.25\n",
            "Number of batches: 2277\n",
            "Number of Sentences Trained on: 36432\n",
            "Training loss epoch: 0.07931296875592984\n",
            "Training accuracy epoch: 2217.25\n",
            "Number of batches: 2278\n",
            "Number of Sentences Trained on: 36448\n",
            "Training loss epoch: 0.0793714477043915\n",
            "Training accuracy epoch: 2218.1875\n",
            "Number of batches: 2279\n",
            "Number of Sentences Trained on: 36464\n",
            "Training loss epoch: 0.07941752271771405\n",
            "Training accuracy epoch: 2219.0625\n",
            "Number of batches: 2280\n",
            "Number of Sentences Trained on: 36480\n",
            "Training loss epoch: 0.07938409745427559\n",
            "Training accuracy epoch: 2220.0625\n",
            "Number of batches: 2281\n",
            "Number of Sentences Trained on: 36496\n",
            "Training loss epoch: 0.07943918060484159\n",
            "Training accuracy epoch: 2221.0\n",
            "Number of batches: 2282\n",
            "Number of Sentences Trained on: 36512\n",
            "Training loss epoch: 0.07941212898806997\n",
            "Training accuracy epoch: 2222.0\n",
            "Number of batches: 2283\n",
            "Number of Sentences Trained on: 36528\n",
            "Training loss epoch: 0.07949059830873056\n",
            "Training accuracy epoch: 2222.9375\n",
            "Number of batches: 2284\n",
            "Number of Sentences Trained on: 36544\n",
            "Training loss epoch: 0.07956424005113336\n",
            "Training accuracy epoch: 2223.875\n",
            "Number of batches: 2285\n",
            "Number of Sentences Trained on: 36560\n",
            "Training loss epoch: 0.07962898911149431\n",
            "Training accuracy epoch: 2224.8125\n",
            "Number of batches: 2286\n",
            "Number of Sentences Trained on: 36576\n",
            "Training loss epoch: 0.07965359111757271\n",
            "Training accuracy epoch: 2225.6875\n",
            "Number of batches: 2287\n",
            "Number of Sentences Trained on: 36592\n",
            "Training loss epoch: 0.0796407736081953\n",
            "Training accuracy epoch: 2226.6875\n",
            "Number of batches: 2288\n",
            "Number of Sentences Trained on: 36608\n",
            "Training loss epoch: 0.07962790977168052\n",
            "Training accuracy epoch: 2227.6875\n",
            "Number of batches: 2289\n",
            "Number of Sentences Trained on: 36624\n",
            "Training loss epoch: 0.07961778190096497\n",
            "Training accuracy epoch: 2228.6875\n",
            "Number of batches: 2290\n",
            "Number of Sentences Trained on: 36640\n",
            "Training loss epoch: 0.07958362128340195\n",
            "Training accuracy epoch: 2229.6875\n",
            "Number of batches: 2291\n",
            "Number of Sentences Trained on: 36656\n",
            "Training loss epoch: 0.07960472531678603\n",
            "Training accuracy epoch: 2230.625\n",
            "Number of batches: 2292\n",
            "Number of Sentences Trained on: 36672\n",
            "Training loss epoch: 0.07957587222941291\n",
            "Training accuracy epoch: 2231.625\n",
            "Number of batches: 2293\n",
            "Number of Sentences Trained on: 36688\n",
            "Training loss epoch: 0.07954155496847518\n",
            "Training accuracy epoch: 2232.625\n",
            "Number of batches: 2294\n",
            "Number of Sentences Trained on: 36704\n",
            "Training loss epoch: 0.07950785846315266\n",
            "Training accuracy epoch: 2233.625\n",
            "Number of batches: 2295\n",
            "Number of Sentences Trained on: 36720\n",
            "Training loss epoch: 0.07948597983608194\n",
            "Training accuracy epoch: 2234.625\n",
            "Number of batches: 2296\n",
            "Number of Sentences Trained on: 36736\n",
            "Training loss epoch: 0.07955915865497828\n",
            "Training accuracy epoch: 2235.5625\n",
            "Number of batches: 2297\n",
            "Number of Sentences Trained on: 36752\n",
            "Training loss epoch: 0.07953321045255027\n",
            "Training accuracy epoch: 2236.5625\n",
            "Number of batches: 2298\n",
            "Number of Sentences Trained on: 36768\n",
            "Training loss epoch: 0.07952083012576214\n",
            "Training accuracy epoch: 2237.5625\n",
            "Number of batches: 2299\n",
            "Number of Sentences Trained on: 36784\n",
            "Training loss epoch: 0.07950486973934028\n",
            "Training accuracy epoch: 2238.5625\n",
            "Number of batches: 2300\n",
            "Number of Sentences Trained on: 36800\n",
            "Training loss per 100 training steps: 0.07947086403281724\n",
            "Training loss epoch: 0.07947086403281724\n",
            "Training accuracy epoch: 2239.5625\n",
            "Number of batches: 2301\n",
            "Number of Sentences Trained on: 36816\n",
            "Training loss epoch: 0.07943725490351267\n",
            "Training accuracy epoch: 2240.5625\n",
            "Number of batches: 2302\n",
            "Number of Sentences Trained on: 36832\n",
            "Training loss epoch: 0.07951072098024234\n",
            "Training accuracy epoch: 2241.5\n",
            "Number of batches: 2303\n",
            "Number of Sentences Trained on: 36848\n",
            "Training loss epoch: 0.0794764629164067\n",
            "Training accuracy epoch: 2242.5\n",
            "Number of batches: 2304\n",
            "Number of Sentences Trained on: 36864\n",
            "Training loss epoch: 0.07944743700986859\n",
            "Training accuracy epoch: 2243.4375\n",
            "Number of batches: 2305\n",
            "Number of Sentences Trained on: 36880\n",
            "Training loss epoch: 0.07949498193523032\n",
            "Training accuracy epoch: 2244.375\n",
            "Number of batches: 2306\n",
            "Number of Sentences Trained on: 36896\n",
            "Training loss epoch: 0.07951013764706771\n",
            "Training accuracy epoch: 2245.3125\n",
            "Number of batches: 2307\n",
            "Number of Sentences Trained on: 36912\n",
            "Training loss epoch: 0.07956106643928842\n",
            "Training accuracy epoch: 2246.3125\n",
            "Number of batches: 2308\n",
            "Number of Sentences Trained on: 36928\n",
            "Training loss epoch: 0.07952772630494313\n",
            "Training accuracy epoch: 2247.3125\n",
            "Number of batches: 2309\n",
            "Number of Sentences Trained on: 36944\n",
            "Training loss epoch: 0.07958600469238732\n",
            "Training accuracy epoch: 2248.25\n",
            "Number of batches: 2310\n",
            "Number of Sentences Trained on: 36960\n",
            "Training loss epoch: 0.0795540664802703\n",
            "Training accuracy epoch: 2249.25\n",
            "Number of batches: 2311\n",
            "Number of Sentences Trained on: 36976\n",
            "Training loss epoch: 0.0795642425036931\n",
            "Training accuracy epoch: 2250.25\n",
            "Number of batches: 2312\n",
            "Number of Sentences Trained on: 36992\n",
            "Training loss epoch: 0.07953218927515489\n",
            "Training accuracy epoch: 2251.25\n",
            "Number of batches: 2313\n",
            "Number of Sentences Trained on: 37008\n",
            "Training loss epoch: 0.07954356662163907\n",
            "Training accuracy epoch: 2252.1875\n",
            "Number of batches: 2314\n",
            "Number of Sentences Trained on: 37024\n",
            "Training loss epoch: 0.0795103480377497\n",
            "Training accuracy epoch: 2253.1875\n",
            "Number of batches: 2315\n",
            "Number of Sentences Trained on: 37040\n",
            "Training loss epoch: 0.07949123240138285\n",
            "Training accuracy epoch: 2254.1875\n",
            "Number of batches: 2316\n",
            "Number of Sentences Trained on: 37056\n",
            "Training loss epoch: 0.07946717452532355\n",
            "Training accuracy epoch: 2255.1875\n",
            "Number of batches: 2317\n",
            "Number of Sentences Trained on: 37072\n",
            "Training loss epoch: 0.07943542300146891\n",
            "Training accuracy epoch: 2256.1875\n",
            "Number of batches: 2318\n",
            "Number of Sentences Trained on: 37088\n",
            "Training loss epoch: 0.079747402187707\n",
            "Training accuracy epoch: 2257.0\n",
            "Number of batches: 2319\n",
            "Number of Sentences Trained on: 37104\n",
            "Training loss epoch: 0.07971621599199666\n",
            "Training accuracy epoch: 2258.0\n",
            "Number of batches: 2320\n",
            "Number of Sentences Trained on: 37120\n",
            "Training loss epoch: 0.07968396132667\n",
            "Training accuracy epoch: 2259.0\n",
            "Number of batches: 2321\n",
            "Number of Sentences Trained on: 37136\n",
            "Training loss epoch: 0.07965343522225937\n",
            "Training accuracy epoch: 2260.0\n",
            "Number of batches: 2322\n",
            "Number of Sentences Trained on: 37152\n",
            "Training loss epoch: 0.07962277911287882\n",
            "Training accuracy epoch: 2261.0\n",
            "Number of batches: 2323\n",
            "Number of Sentences Trained on: 37168\n",
            "Training loss epoch: 0.07959248331051971\n",
            "Training accuracy epoch: 2262.0\n",
            "Number of batches: 2324\n",
            "Number of Sentences Trained on: 37184\n",
            "Training loss epoch: 0.07966266928499775\n",
            "Training accuracy epoch: 2262.875\n",
            "Number of batches: 2325\n",
            "Number of Sentences Trained on: 37200\n",
            "Training loss epoch: 0.07966254442057707\n",
            "Training accuracy epoch: 2263.8125\n",
            "Number of batches: 2326\n",
            "Number of Sentences Trained on: 37216\n",
            "Training loss epoch: 0.07964076910472453\n",
            "Training accuracy epoch: 2264.8125\n",
            "Number of batches: 2327\n",
            "Number of Sentences Trained on: 37232\n",
            "Training loss epoch: 0.0796710778582921\n",
            "Training accuracy epoch: 2265.6875\n",
            "Number of batches: 2328\n",
            "Number of Sentences Trained on: 37248\n",
            "Training loss epoch: 0.07964816778503507\n",
            "Training accuracy epoch: 2266.6875\n",
            "Number of batches: 2329\n",
            "Number of Sentences Trained on: 37264\n",
            "Training loss epoch: 0.07974188787693531\n",
            "Training accuracy epoch: 2267.625\n",
            "Number of batches: 2330\n",
            "Number of Sentences Trained on: 37280\n",
            "Training loss epoch: 0.07971864135983397\n",
            "Training accuracy epoch: 2268.625\n",
            "Number of batches: 2331\n",
            "Number of Sentences Trained on: 37296\n",
            "Training loss epoch: 0.07970688269371724\n",
            "Training accuracy epoch: 2269.625\n",
            "Number of batches: 2332\n",
            "Number of Sentences Trained on: 37312\n",
            "Training loss epoch: 0.07968683242860475\n",
            "Training accuracy epoch: 2270.625\n",
            "Number of batches: 2333\n",
            "Number of Sentences Trained on: 37328\n",
            "Training loss epoch: 0.07965381227657085\n",
            "Training accuracy epoch: 2271.625\n",
            "Number of batches: 2334\n",
            "Number of Sentences Trained on: 37344\n",
            "Training loss epoch: 0.07962985115525377\n",
            "Training accuracy epoch: 2272.625\n",
            "Number of batches: 2335\n",
            "Number of Sentences Trained on: 37360\n",
            "Training loss epoch: 0.07960100661296578\n",
            "Training accuracy epoch: 2273.625\n",
            "Number of batches: 2336\n",
            "Number of Sentences Trained on: 37376\n",
            "Training loss epoch: 0.07961862815343906\n",
            "Training accuracy epoch: 2274.5\n",
            "Number of batches: 2337\n",
            "Number of Sentences Trained on: 37392\n",
            "Training loss epoch: 0.07963991196579079\n",
            "Training accuracy epoch: 2275.4375\n",
            "Number of batches: 2338\n",
            "Number of Sentences Trained on: 37408\n",
            "Training loss epoch: 0.07961475123951099\n",
            "Training accuracy epoch: 2276.4375\n",
            "Number of batches: 2339\n",
            "Number of Sentences Trained on: 37424\n",
            "Training loss epoch: 0.07958915237620678\n",
            "Training accuracy epoch: 2277.4375\n",
            "Number of batches: 2340\n",
            "Number of Sentences Trained on: 37440\n",
            "Training loss epoch: 0.0796645611276012\n",
            "Training accuracy epoch: 2278.375\n",
            "Number of batches: 2341\n",
            "Number of Sentences Trained on: 37456\n",
            "Training loss epoch: 0.07964079688712714\n",
            "Training accuracy epoch: 2279.375\n",
            "Number of batches: 2342\n",
            "Number of Sentences Trained on: 37472\n",
            "Training loss epoch: 0.07963556437644576\n",
            "Training accuracy epoch: 2280.375\n",
            "Number of batches: 2343\n",
            "Number of Sentences Trained on: 37488\n",
            "Training loss epoch: 0.07964025076774278\n",
            "Training accuracy epoch: 2281.3125\n",
            "Number of batches: 2344\n",
            "Number of Sentences Trained on: 37504\n",
            "Training loss epoch: 0.07961225351168544\n",
            "Training accuracy epoch: 2282.3125\n",
            "Number of batches: 2345\n",
            "Number of Sentences Trained on: 37520\n",
            "Training loss epoch: 0.07958480233723488\n",
            "Training accuracy epoch: 2283.3125\n",
            "Number of batches: 2346\n",
            "Number of Sentences Trained on: 37536\n",
            "Training loss epoch: 0.07955287618140319\n",
            "Training accuracy epoch: 2284.3125\n",
            "Number of batches: 2347\n",
            "Number of Sentences Trained on: 37552\n",
            "Training loss epoch: 0.0796580278554298\n",
            "Training accuracy epoch: 2285.25\n",
            "Number of batches: 2348\n",
            "Number of Sentences Trained on: 37568\n",
            "Training loss epoch: 0.07962593545300599\n",
            "Training accuracy epoch: 2286.25\n",
            "Number of batches: 2349\n",
            "Number of Sentences Trained on: 37584\n",
            "Training loss epoch: 0.07960343666561691\n",
            "Training accuracy epoch: 2287.25\n",
            "Number of batches: 2350\n",
            "Number of Sentences Trained on: 37600\n",
            "Training loss epoch: 0.07964113381965751\n",
            "Training accuracy epoch: 2288.1875\n",
            "Number of batches: 2351\n",
            "Number of Sentences Trained on: 37616\n",
            "Training loss epoch: 0.07966145665292915\n",
            "Training accuracy epoch: 2289.125\n",
            "Number of batches: 2352\n",
            "Number of Sentences Trained on: 37632\n",
            "Training loss epoch: 0.0796698296435182\n",
            "Training accuracy epoch: 2290.0625\n",
            "Number of batches: 2353\n",
            "Number of Sentences Trained on: 37648\n",
            "Training loss epoch: 0.07977527645251133\n",
            "Training accuracy epoch: 2290.9375\n",
            "Number of batches: 2354\n",
            "Number of Sentences Trained on: 37664\n",
            "Training loss epoch: 0.07974896893060208\n",
            "Training accuracy epoch: 2291.9375\n",
            "Number of batches: 2355\n",
            "Number of Sentences Trained on: 37680\n",
            "Training loss epoch: 0.07972103515660407\n",
            "Training accuracy epoch: 2292.9375\n",
            "Number of batches: 2356\n",
            "Number of Sentences Trained on: 37696\n",
            "Training loss epoch: 0.07987222619387867\n",
            "Training accuracy epoch: 2293.875\n",
            "Number of batches: 2357\n",
            "Number of Sentences Trained on: 37712\n",
            "Training loss epoch: 0.0799704613700106\n",
            "Training accuracy epoch: 2294.75\n",
            "Number of batches: 2358\n",
            "Number of Sentences Trained on: 37728\n",
            "Training loss epoch: 0.07993777209576589\n",
            "Training accuracy epoch: 2295.75\n",
            "Number of batches: 2359\n",
            "Number of Sentences Trained on: 37744\n",
            "Training loss epoch: 0.07994325222146365\n",
            "Training accuracy epoch: 2296.75\n",
            "Number of batches: 2360\n",
            "Number of Sentences Trained on: 37760\n",
            "Training loss epoch: 0.07994861730003554\n",
            "Training accuracy epoch: 2297.6875\n",
            "Number of batches: 2361\n",
            "Number of Sentences Trained on: 37776\n",
            "Training loss epoch: 0.07996174210411582\n",
            "Training accuracy epoch: 2298.6875\n",
            "Number of batches: 2362\n",
            "Number of Sentences Trained on: 37792\n",
            "Training loss epoch: 0.07995802189852447\n",
            "Training accuracy epoch: 2299.625\n",
            "Number of batches: 2363\n",
            "Number of Sentences Trained on: 37808\n",
            "Training loss epoch: 0.07992956304823767\n",
            "Training accuracy epoch: 2300.625\n",
            "Number of batches: 2364\n",
            "Number of Sentences Trained on: 37824\n",
            "Training loss epoch: 0.07992566496951665\n",
            "Training accuracy epoch: 2301.625\n",
            "Number of batches: 2365\n",
            "Number of Sentences Trained on: 37840\n",
            "Training loss epoch: 0.0798982631588989\n",
            "Training accuracy epoch: 2302.625\n",
            "Number of batches: 2366\n",
            "Number of Sentences Trained on: 37856\n",
            "Training loss epoch: 0.07986764688760443\n",
            "Training accuracy epoch: 2303.625\n",
            "Number of batches: 2367\n",
            "Number of Sentences Trained on: 37872\n",
            "Training loss epoch: 0.07983946492103731\n",
            "Training accuracy epoch: 2304.625\n",
            "Number of batches: 2368\n",
            "Number of Sentences Trained on: 37888\n",
            "Training loss epoch: 0.07983311751200432\n",
            "Training accuracy epoch: 2305.625\n",
            "Number of batches: 2369\n",
            "Number of Sentences Trained on: 37904\n",
            "Training loss epoch: 0.07994208179534863\n",
            "Training accuracy epoch: 2306.5625\n",
            "Number of batches: 2370\n",
            "Number of Sentences Trained on: 37920\n",
            "Training loss epoch: 0.07999123547021494\n",
            "Training accuracy epoch: 2307.5\n",
            "Number of batches: 2371\n",
            "Number of Sentences Trained on: 37936\n",
            "Training loss epoch: 0.08000669069174694\n",
            "Training accuracy epoch: 2308.4375\n",
            "Number of batches: 2372\n",
            "Number of Sentences Trained on: 37952\n",
            "Training loss epoch: 0.08000381008865481\n",
            "Training accuracy epoch: 2309.375\n",
            "Number of batches: 2373\n",
            "Number of Sentences Trained on: 37968\n",
            "Training loss epoch: 0.07998950685500318\n",
            "Training accuracy epoch: 2310.375\n",
            "Number of batches: 2374\n",
            "Number of Sentences Trained on: 37984\n",
            "Training loss epoch: 0.08000981508645132\n",
            "Training accuracy epoch: 2311.25\n",
            "Number of batches: 2375\n",
            "Number of Sentences Trained on: 38000\n",
            "Training loss epoch: 0.07998491503961057\n",
            "Training accuracy epoch: 2312.1875\n",
            "Number of batches: 2376\n",
            "Number of Sentences Trained on: 38016\n",
            "Training loss epoch: 0.07995389953123566\n",
            "Training accuracy epoch: 2313.1875\n",
            "Number of batches: 2377\n",
            "Number of Sentences Trained on: 38032\n",
            "Training loss epoch: 0.0799253642121466\n",
            "Training accuracy epoch: 2314.125\n",
            "Number of batches: 2378\n",
            "Number of Sentences Trained on: 38048\n",
            "Training loss epoch: 0.07990182241926351\n",
            "Training accuracy epoch: 2315.125\n",
            "Number of batches: 2379\n",
            "Number of Sentences Trained on: 38056\n"
          ]
        }
      ],
      "source": [
        "#instantiate Bert for NER model and send to gpu \n",
        "model2 = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))\n",
        "model2.to(device)\n",
        "\n",
        "#define the optimizer \n",
        "optimizer = torch.optim.Adam(params=model2.parameters(), lr=LEARNING_RATE)\n",
        "EPOCHS = 4\n",
        "\n",
        "train_loss, train_acc = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    batch_loss,epoch_loss, batch_acc, epoch_acc = train(model2, epoch)\n",
        "\n",
        "    train_loss.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    import pdb\n",
        "    #pdb.set_trace()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM6riwZeFcNh",
        "outputId": "45060ef4-47ad-446f-bf04-db9b451ca791"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.973150483396385]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BLmiWcoVbFR",
        "outputId": "620b9465-1c41-4f66-d962-bd93ee054f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation epoch: 1\n",
            "Validation loss per 100 evaluation steps: 0.0060434709303081036\n",
            "Validation loss per 100 evaluation steps: 0.1869116087229738\n",
            "Validation loss per 100 evaluation steps: 0.19028415911094243\n",
            "Validation loss per 100 evaluation steps: 0.1878119059244675\n",
            "Validation loss per 100 evaluation steps: 0.18445996161996783\n",
            "Validation loss per 100 evaluation steps: 0.1900999593762718\n",
            "Validation Loss: 0.18632638693626719\n",
            "Validation Accuracy: 0.9453304048892284\n",
            "Validation epoch: 2\n",
            "Validation loss per 100 evaluation steps: 0.5281784534454346\n",
            "Validation loss per 100 evaluation steps: 0.18556884961320308\n",
            "Validation loss per 100 evaluation steps: 0.1909793709738954\n",
            "Validation loss per 100 evaluation steps: 0.18035187978089395\n",
            "Validation loss per 100 evaluation steps: 0.18221794982746178\n",
            "Validation loss per 100 evaluation steps: 0.18372959348962395\n",
            "Validation Loss: 0.18642267470655008\n",
            "Validation Accuracy: 0.9453304048892284\n",
            "Validation epoch: 3\n",
            "Validation loss per 100 evaluation steps: 0.002397118369117379\n",
            "Validation loss per 100 evaluation steps: 0.1762504208571333\n",
            "Validation loss per 100 evaluation steps: 0.18001753257399897\n",
            "Validation loss per 100 evaluation steps: 0.17988106387027653\n",
            "Validation loss per 100 evaluation steps: 0.1862076233492983\n",
            "Validation loss per 100 evaluation steps: 0.18630582978902477\n",
            "Validation Loss: 0.1863052920206497\n",
            "Validation Accuracy: 0.9453781512605042\n",
            "Validation epoch: 4\n",
            "Validation loss per 100 evaluation steps: 0.1977207511663437\n",
            "Validation loss per 100 evaluation steps: 0.19407092068646342\n",
            "Validation loss per 100 evaluation steps: 0.17509254766367516\n",
            "Validation loss per 100 evaluation steps: 0.17308488989266663\n",
            "Validation loss per 100 evaluation steps: 0.1782444219431257\n",
            "Validation loss per 100 evaluation steps: 0.1879830674622431\n",
            "Validation Loss: 0.1865197436849806\n",
            "Validation Accuracy: 0.9452826585179527\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_acc = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f\"Validation epoch: {epoch + 1}\")\n",
        "  labels, predictions, val_epoch_loss, val_epoch_accuracy = valid(model2, testing_loader)\n",
        "\n",
        "  val_loss.append(val_epoch_loss)\n",
        "  val_acc.append(val_epoch_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCepe8u_OxvX"
      },
      "outputs": [],
      "source": [
        "#plot metrics per epoch\n",
        "def plot_metric_per_epoch(metric, val_metric, n_epochs, title):\n",
        "  \n",
        "    x0 = list(range(1, n_epochs + 1))\n",
        "    plt.figure(figsize =(10, 5))\n",
        "    plt.plot(x0, metric)\n",
        "    plt.plot(x0, val_metric)\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNISSss1whkb",
        "outputId": "d7200a82-c933-433f-84b4-b235b084d47a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_metric_per_epoch(train_loss, val_loss, EPOCHS, \"Loss per epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYFJGDJTHgrX",
        "outputId": "bab50942-0365-45f6-ee90-a9a7224cdd21"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAHDCAYAAADm78EeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABaDElEQVR4nO3deXhU5eH+/3sm2yRkXyEQyMKSoLKTiDsSS4VatS5oQSAsViutyq/1A5Wqtd8W7UKhSkFlU8DihtbWioUI7iSsChLWsAbIShayZ+b8/giODglIgHAymffrunK1Oec5k/vMMJibc+Z5LIZhGAIAAAAAD2M1OwAAAAAAmIEyBAAAAMAjUYYAAAAAeCTKEAAAAACPRBkCAAAA4JEoQwAAAAA8EmUIAAAAgEeiDAEAAADwSJQhAAAAAB6JMgQAgId56qmnZLFYVFRUZHYUADAVZQgA2qh//OMfslgsSktLMzsKAADtEmUIANqo5cuXKz4+XtnZ2dq7d6/ZcQAAaHcoQwDQBu3fv1+ff/65Zs2apaioKC1fvtzsSGdUWVlpdgRTeOp5A0B7QhkCgDZo+fLlCgsL08iRI3XnnXeesQyVlpbq0UcfVXx8vPz8/NSlSxeNHTvW5bMgNTU1euqpp9SzZ0/ZbDZ16tRJP/nJT7Rv3z5J0rp162SxWLRu3TqXxz5w4IAsFouWLFni3DZ+/HgFBgZq3759GjFihIKCgjR69GhJ0ieffKK77rpLXbt2lZ+fn+Li4vToo4+qurq6Se6dO3fq7rvvVlRUlPz9/dWrVy89/vjjkqS1a9fKYrHo7bffbnLcq6++KovFoi+++OKMz92SJUtksVj08ccf62c/+5kiIiIUHByssWPH6sSJE03Gv//++7r22mvVoUMHBQUFaeTIkfr6669dxpztvM8kLy9PEyZMUExMjPz8/HTZZZdp0aJFLmO+ee5fe+01/eY3v1HHjh3VoUMH/fjHP9bhw4ebPOYbb7yhgQMHyt/fX5GRkRozZozy8vKajDvb8/tdpaWlGj9+vEJDQxUSEqKMjAxVVVWd9bwAoD3xNjsAAKCp5cuX6yc/+Yl8fX117733at68edqwYYMGDx7sHHPy5Elde+21ysnJ0YQJEzRgwAAVFRXp3Xff1ZEjRxQZGSm73a4f/ehHyszM1D333KOHH35YFRUVWr16tbZv366kpKQWZ2toaNDw4cN1zTXX6C9/+YsCAgIkNf6iXlVVpQcffFARERHKzs7Wc889pyNHjuiNN95wHv/VV1/p2muvlY+Pj+6//37Fx8dr3759+ve//60//OEPuuGGGxQXF6fly5fr9ttvb/K8JCUlaciQId+bc8qUKQoNDdVTTz2lXbt2ad68eTp48KCzgEjS0qVLNW7cOA0fPlzPPvusqqqqNG/ePF1zzTXasmWL4uPjv/e8m5Ofn68rr7xSFotFU6ZMUVRUlN5//31NnDhR5eXleuSRR1zG/+EPf5DFYtH//d//qaCgQLNnz1Z6erq2bt0qf39/SY0lLyMjQ4MHD9bMmTOVn5+vOXPm6LPPPtOWLVsUGhp6Ts/vd919991KSEjQzJkztXnzZi1YsEDR0dF69tlnv/f5BYB2wQAAtCkbN240JBmrV682DMMwHA6H0aVLF+Phhx92GffEE08YkoyVK1c2eQyHw2EYhmEsWrTIkGTMmjXrjGPWrl1rSDLWrl3rsn///v2GJGPx4sXObePGjTMkGdOmTWvyeFVVVU22zZw507BYLMbBgwed26677jojKCjIZdt38xiGYUyfPt3w8/MzSktLndsKCgoMb29v48knn2zyc75r8eLFhiRj4MCBRl1dnXP7n/70J0OS8a9//cswDMOoqKgwQkNDjcmTJ7scf/z4cSMkJMRl+9nOuzkTJ040OnXqZBQVFblsv+eee4yQkBDnc/XNc9+5c2ejvLzcOe711183JBlz5swxDMMw6urqjOjoaOPyyy83qqurneP+85//GJKMJ554wrntXJ7fJ5980pBkTJgwwWXM7bffbkRERJzTOQJAe8BtcgDQxixfvlwxMTEaOnSoJMlisWjUqFFasWKF7Ha7c9xbb72lvn37Nrl68s0x34yJjIzUL37xizOOOR8PPvhgk23fXMGQGj9PU1RUpKuuukqGYWjLli2SpMLCQn388ceaMGGCunbtesY8Y8eOVW1trd58803nttdee00NDQ0aM2bMOWW8//775ePj45LZ29tb//3vfyVJq1evVmlpqe69914VFRU5v7y8vJSWlqa1a9ee03mfzjAMvfXWW7rllltkGIbLYw8fPlxlZWXavHmzyzFjx45VUFCQ8/s777xTnTp1cmbduHGjCgoK9POf/1w2m805buTIkUpOTtZ7770n6dyf32888MADLt9fe+21Ki4uVnl5+feeJwC0B5QhAGhD7Ha7VqxYoaFDh2r//v3au3ev9u7dq7S0NOXn5yszM9M5dt++fbr88svP+nj79u1Tr1695O198e6K9vb2VpcuXZpsP3TokMaPH6/w8HAFBgYqKipK119/vSSprKxMkpSbmytJ35s7OTlZgwcPdvms1PLly3XllVeqe/fu55SzR48eLt8HBgaqU6dOOnDggCRpz549kqQbb7xRUVFRLl//+9//VFBQcE7nfbrCwkKVlpbqxRdfbPK4GRkZktTksU/ParFY1L17d2fWgwcPSpJ69erV5OclJyc795/r8/uN0wtTWFiYJDX72SoAaI/4zBAAtCEffvihjh07phUrVmjFihVN9i9fvlw/+MEPLurPPNMVou9ehfouPz8/Wa3WJmNvuukmlZSU6P/+7/+UnJysDh06KC8vT+PHj5fD4WhxrrFjx+rhhx/WkSNHVFtbq/Xr1+v5559v8eOcyTeZli5dqo4dOzbZf3qBbO68z/a4Y8aM0bhx45od06dPn5bGbRVeXl7NbjcM4xInAQBzUIYAoA1Zvny5oqOjNXfu3Cb7Vq5cqbffflvz58+Xv7+/kpKStH379rM+XlJSkrKyslRfX+9yy9h3fXM1oLS01GX7N1cbzsW2bdu0e/duvfzyyxo7dqxz++rVq13GJSYmStL35pake+65R1OnTtU///lPVVdXy8fHR6NGjTrnTHv27HHeaig1Tjhx7NgxjRgxQpKck0dER0crPT39nB/3+0RFRSkoKEh2u/2cH/ebq1TfMAxDe/fudZambt26SZJ27dqlG2+80WXsrl27nPtb8vwCALhNDgDajOrqaq1cuVI/+tGPdOeddzb5mjJliioqKvTuu+9Kku644w59+eWXzU5B/c2/7N9xxx0qKipq9orKN2O6desmLy8vffzxxy77//GPf5xz9m+uMHz3ioJhGJozZ47LuKioKF133XVatGiRDh061Gyeb0RGRurmm2/WsmXLtHz5cv3whz9UZGTkOWd68cUXVV9f7/x+3rx5amho0M033yxJGj58uIKDg/XHP/7RZdw3CgsLz/lnfZeXl5fuuOMOvfXWW82WkuYe95VXXlFFRYXz+zfffFPHjh1zZh00aJCio6M1f/581dbWOse9//77ysnJ0ciRIyW17PkFAHBlCADajHfffVcVFRX68Y9/3Oz+K6+80rkA66hRo/TrX/9ab775pu666y5NmDBBAwcOVElJid59913Nnz9fffv21dixY/XKK69o6tSpys7O1rXXXqvKykqtWbNGP//5z3XrrbcqJCREd911l5577jlZLBYlJSXpP//5T5PPtZxNcnKykpKS9Ktf/Up5eXkKDg7WW2+91exnT/7+97/rmmuu0YABA3T//fcrISFBBw4c0HvvvaetW7e6jB07dqzuvPNOSdLvf//7c38yJdXV1WnYsGG6++67tWvXLv3jH//QNddc43x+g4ODNW/ePN13330aMGCA7rnnHkVFRenQoUN67733dPXVV5/3bXnPPPOM1q5dq7S0NE2ePFm9e/dWSUmJNm/erDVr1qikpMRlfHh4uK655hplZGQoPz9fs2fPVvfu3TV58mRJko+Pj5599lllZGTo+uuv17333uucWjs+Pl6PPvqo87Fa8vwCgMczaRY7AMBpbrnlFsNmsxmVlZVnHDN+/HjDx8fHOWVzcXGxMWXKFKNz586Gr6+v0aVLF2PcuHEuUzpXVVUZjz/+uJGQkGD4+PgYHTt2NO68805j3759zjGFhYXGHXfcYQQEBBhhYWHGz372M2P79u3NTq3doUOHZrPt2LHDSE9PNwIDA43IyEhj8uTJxpdfftnkMQzDMLZv327cfvvtRmhoqGGz2YxevXoZv/3tb5s8Zm1trREWFmaEhIS4TCl9Nt9Mrf3RRx8Z999/vxEWFmYEBgYao0ePNoqLi5uMX7t2rTF8+HAjJCTEsNlsRlJSkjF+/Hhj48aN53TeZ5Kfn2889NBDRlxcnPN5HzZsmPHiiy+6/GxJxj//+U9j+vTpRnR0tOHv72+MHDmyydTYhmEYr732mtG/f3/Dz8/PCA8PN0aPHm0cOXKkybjve36/mVq7sLCw2edu//79LTpXAHBXFsPgujkAoG1qaGhQbGysbrnlFi1cuPCcjvlmcdINGzZo0KBBrZzwwqxbt05Dhw7VG2+84bwCBgC4dPjMEACgzXrnnXdUWFjoMikDAAAXC58ZAgC0OVlZWfrqq6/0+9//Xv3793euVwQAwMXElSEAQJszb948Pfjgg4qOjtYrr7xidhwAQDvFZ4YAAAAAeCSuDAEAAADwSJQhAAAAAB6p3Uyg4HA4dPToUQUFBclisZgdBwAAAIBJDMNQRUWFYmNjZbWe+fpPuylDR48eVVxcnNkxAAAAALQRhw8fVpcuXc64v92UoaCgIEmNJxwcHGxyGgAAAABmKS8vV1xcnLMjnEm7KUPf3BoXHBxMGQIAAADwvR+fYQIFAAAAAB6JMgQAAADAI1GGAAAAAHgkyhAAAAAAj0QZAgAAAOCRKEMAAAAAPBJlCAAAAIBHogwBAAAA8EiUIQAAAAAeiTIEAAAAwCNRhgAAAAB4JMoQAAAAAI9EGQIAAADgkShDAAAAAC5IYUWt/rvtmAoqasyO0iLeZgcAAAAA4F6OlVUrK7dEWftLlL2/WPsKKyVJf76zj+4aFGdyunNHGQIAAABwRoZh6FBJlbL2lygrt0TZB4p1uKTaZYzFIvWKCZKPl3vdeEYZAgAAAOBkGIb2Fpw8ddWn8et4uevtb15Wiy6PDVZqQrhSEyI0OD5MoQG+JiU+f5QhAAAAwIPZHYZ2Hi9vvOqzv0TZB0pUUlnnMsbHy6K+XUKVlthYfgZ2C1Ogn/tXCfc/AwAAAADnrN7u0Pa8MudVn+wDJaqoaXAZY/OxakDXMKUmhCstIUL9u4bK5uNlUuLWQxkCAAAA2rGaeru+OlKmrNxiZR8o0aaDJ1RVZ3cZE+jnrYHdwpSWGK60hHBd0TlUvt7u9fmf80EZAgAAANqRqroGbT5Yquz9xVq/v0RbD5eqrsHhMiY0wEeD4xuLT1pChFI6BcnbzSY/uBgoQwAAAIAbK6+p16YDJ7R+f7Gy95do25EyNTgMlzGRgX7Oqz6pCeHqGR0kq9ViUuK2gzIEAAAAuJGSyjrn532y9hcr51i5Tus+ig2xKS0x4tRnfsKVENlBFgvl53SUIQAAAKANyy+vcS5umr2/RLvzTzYZEx8RoLSEiFNTXYcrLjzAhKTuhzIEAAAAtCGHS6qcV32y95foQHFVkzE9YwKdM72lJoQrJthmQlL3RxkCAAAATGIYhvYXVboscJpXWu0yxmKRencKdrnyE97B/RY4bYsoQwAAAMAl4nAY2l1Q0XjlJ7dEWftLVHSy1mWMt9WiK7qEKDUhXFcmRGhgfJiCbT4mJW7fKEMAAABAK2mwO7TjWPmp295KtOFAiUqr6l3G+Hpb1T8u9NRMbxEa0C1UAb78mn4p8CwDAAAAF0ldg0Pb8kq1PrfxlrdNB0/oZG2Dy5gAX6/GBU5PlZ8+XUJk8/EyKbFnowwBAAAA56mm3q7Nh044P++z+dAJ1dS7LnAaZPNWanzjZ33SEiN0WWywfDxwgdO2iDIEAAAAnKOTtQ3adPCEsnIbZ3r78kip6u2ui/yEd/BVany40hIbC1Byx2B5scBpm0QZAgAAAM6gtKpOGw6cUPb+YmXtL9HXR8tlP22F05hgP+dMb1cmhispKpAFTt0EZQgAAAA4pbCiVhsOlCgrt7H87MqvkOHafRQX7u8sP2kJ4eoaHkD5cVOUIQAAAHiso6XVzpnesvYXK7ewssmYpKgOSk2I0JWJ4RocH67YUH8TkqI1UIYAAADgEQzD0KGSKuf6PtkHinW4pOkCp8kdg0/N9NZYfqKC/ExKjNZ2XmVo7ty5+vOf/6zjx4+rb9++eu6555Samtrs2Pr6es2cOVMvv/yy8vLy1KtXLz377LP64Q9/6BwTHx+vgwcPNjn25z//uebOnXs+EQEAAODhDMPQ3oKTp676lCh7f7Hyy10XOPWyWnR5bLDSEiOUGh+uQfFhCg3wNSkxLrUWl6HXXntNU6dO1fz585WWlqbZs2dr+PDh2rVrl6Kjo5uMnzFjhpYtW6aXXnpJycnJ+uCDD3T77bfr888/V//+/SVJGzZskN1udx6zfft23XTTTbrrrrsu4NQAAADgSewOQzuPlyvr1Bo/2QdKVFJZ5zLG18uqvnEhSj21xs/AbmEK9ONmKU9lMYzTPxJ2dmlpaRo8eLCef/55SZLD4VBcXJx+8YtfaNq0aU3Gx8bG6vHHH9dDDz3k3HbHHXfI399fy5Yta/ZnPPLII/rPf/6jPXv2nPOH0crLyxUSEqKysjIFBwe35JQAAADghurtDm3PK3N+5mfDgRJV1LgucGrzsWpA1zDnhAf9u4aywKkHONdu0KIaXFdXp02bNmn69OnObVarVenp6friiy+aPaa2tlY2m81lm7+/vz799NMz/oxly5Zp6tSpZy1CtbW1qq399jJneXl5S04FAAAAbqam3q4vD5c6r/psOnhCVXV2lzGBft4aFB92aqa3CF3ROUS+3ixwiua1qAwVFRXJbrcrJibGZXtMTIx27tzZ7DHDhw/XrFmzdN111ykpKUmZmZlauXKly21x3/XOO++otLRU48ePP2uWmTNn6ne/+11L4gMAAMCNVNU1aPPBUmWdWuNn6+FS1TU4XMaEBvhocHzjFNdpCRFK6RQkby/KD85Nq98gOWfOHE2ePFnJycmyWCxKSkpSRkaGFi1a1Oz4hQsX6uabb1ZsbOxZH3f69OmaOnWq8/vy8nLFxcVd1OwAAAC4dMpr6rXxwDeTHZRo25EyNZy2wGlkoJ/SEr8tPz2iA2W1ssYPzk+LylBkZKS8vLyUn5/vsj0/P18dO3Zs9pioqCi98847qqmpUXFxsWJjYzVt2jQlJiY2GXvw4EGtWbNGK1eu/N4sfn5+8vNjmkMAAAB3VVJZd+rzPsXK3l+iHcfKmyxw2jnU3znNdWpCuBIiO7DAKS6aFpUhX19fDRw4UJmZmbrtttskNU6gkJmZqSlTppz1WJvNps6dO6u+vl5vvfWW7r777iZjFi9erOjoaI0cObIlsQAAAOAG8strnFNcZ+WWaE/BySZjEiI7KDU+XGmJjeWnS1iACUnhKVp8m9zUqVM1btw4DRo0SKmpqZo9e7YqKyuVkZEhSRo7dqw6d+6smTNnSpKysrKUl5enfv36KS8vT0899ZQcDocee+wxl8d1OBxavHixxo0bJ29vpjcEAABwZ4Zh6MiJapcrPweKq5qM6xkT6JzpLTUhXDHBtmYeDWgdLW4do0aNUmFhoZ544gkdP35c/fr106pVq5yTKhw6dEhW67cfWqupqdGMGTOUm5urwMBAjRgxQkuXLlVoaKjL465Zs0aHDh3ShAkTLuyMAAAAcMkZhqHcosrGmd72lygrt1hHy2pcxlgtUu/YYKXGRygtMVyD48MV3oEFTmGeFq8z1FaxzhAAAMCl43AY2l1Q4VzgNGt/iYpO1rqM8bZa1KdLiFITIpSWEK6B8WEKtvmYlBiepFXWGQIAAIBnarA7tONYubL3l2h9buMCp2XV9S5jfL2t6h8X2jjTW2KE+ncNVYAvv26i7eJPJwAAAJqoa3BoW16p1p+68rPp4AmdrG1wGRPg66WB3cJOzfYWob5xIfLz9jIpMdBylCEAAACous6uLYdPnPq8T4m2HD6hmnrXBU6DbN7fmektQpfFBsuHBU7hxihDAAAAHuhkbYM2HihxTnjw5ZFS1dtdP0oe0cHXOctbWkKEenUMkhcLnKIdoQwBAAB4gNKqOm04cKJxjZ/9JdqeVybHadNoxQT7KS2hcaa3tIRwJUUFssAp2jXKEAAAQDtUWFF76qpPY/nZlV+h0+cQ7hoe4Lzyc2VChOLC/Sk/8CiUIQAAgHbgaOm3C5xm7S9RbmFlkzFJUR2Ultg4zfXg+HDFhvqbkBRoOyhDAAAAbsYwDB0srnKu75O1v1hHTlS7jLFYpOSOwY3TXCeEa3BCuCID/UxKDLRNlCEAAIA2zjAM7S04ear4NN76ll/uusCpl9Wiy2ODlZYYodT4xis/IQEscAqcDWUIAACgjbE7DOWcWuA0e3+Jsg+UqKSyzmWMr5dVfeNCnDO9DegWpkA/frUDWoJ3DAAAgMnq7Q5tzys7ddWnRBsOlKiixnWBU5uPVQO7hSk1PkKpCeHq3zVUNh8WOAUuBGUIAADgEqupt+vLw6XOz/xsOnhC1fV2lzGBft4aFB/mvPJzRecQ+XqzwClwMVGGAAAAWllVXYM2Hyx1zvS29XCp6hocLmNCA3yUGv/tAqcpnYLk7UX5AVoTZQgAAOAiK6uu16aDp2Z6y21c4LThtBVOo4L8Tq3vE67UhAj1iA6U1coaP8ClRBkCAAC4QCWVdc7FTbP3l2jHsfImC5x2DvVX2qkFTtMSIxQfEcACp4DJKEMAAAAtlF9e45ziOiu3RHsKTjYZkxDZwVl+UhPC1SUswISkAM6GMgQAAHAWhmHoyIlqZ/nJ3l+iA8VVTcb1igk6ddUnXKnx4YoOtpmQFkBLUIYAAAC+wzAM5RZVNs70lttYfo6W1biMsVqk3rHBSktonOY6NT5cYR18TUoM4HxRhgAAgEdzOAztLqhQVm6Jc6rropO1LmO8rRb16RKi1IQIpSWGa2C3MAXbfExKDOBioQwBAACP0mB3aMexcmXvL9H63MYFTsuq613G+Hlb1b9raGP5ObXAaYAvvzYB7Q3vagAA0K7VNTj01ZFS50xvmw6e0MnaBpcxAb5eGtgtTFcmNt721qdLiPy8vUxKDOBSoQwBAIB2pbrOri2HTzhve9t86IRqT1vgNNjm7ZzlLTUhQpfHBrPAKeCBKEMAAMCtnaxt0MYD337e56sjpaq3uy7yE9HBt3Gmt1Plp1fHIHmxwCng8ShDAADArZRW1WnDgRONM70dKNH2vDI5TlvgtGOwrXGK64RwpSVEKCmqAwucAmiCMgQAANq0wopaZX+zwOn+Eu08XtFkTNfwAOcCp2kJEYoL96f8APhelCEAANCmGIah7XnlWpOTr8yd+dqeV95kTPfowO/c9hauTiH+JiQF4O4oQwAAwHQ19XZ9vq9Ia3IKlJmTr/zyb9f5sVik5I7BSjtVfgYnhCsy0M/EtADaC8oQAAAwRUFFjT7MKdCanAJ9trdI1fV2574AXy9d1yNKw1KiNTQ5mvIDoFVQhgAAwCVhGIZ2Hq/Qmh35WrOzQF8eLnXZHxti07CUGA1LidaViRGy+bDOD4DWRRkCAACtprbBrvW5JcrMyVdmToHySqtd9vftEqJhKTFKT4lRSqcgJj0AcElRhgAAwEVVfLJWa3cVKjMnXx/vLlRl3be3v9l8rLqme6TSU2J0Y3K0ooNtJiYF4OkoQwAA4IIYhqG9BSe1JqdAa3LytfnQCRnfWfcnOsjv1NWfaF3dPZLb3wC0GZQhAADQYvV2hzbsL9HqU7e/HSqpctl/WWywswBdHhsiq5Xb3wC0PZQhAABwTkqr6rRuV6HW5OTro92FqqhpcO7z9bbqqqSIxgkQkqMVG8q6PwDaPsoQAAA4o9zCk8o8dfvbxoMnZHd8e/9bZKCvhvaKVnrvGF3TPVId/Pi1AoB74W8tAADg1GB3aNPBE8rc2ViAcgsrXfb3iglSeu9oDUuJUb8uodz+BsCtUYYAAPBw5TX1+nh3odbsyNe63YUqrap37vPxsujKxAgNS24sQHHhASYmBYCLizIEAIAHOlRcpTU5+crcma+s3BI1fOf2t7AAHw3t1Vh+rusZqSCbj4lJAaD1UIYAAPAAdoehrYdPaE1OgTJz8rU7/6TL/qSoDkpPidGwlBgN6Boqby+rSUkB4NKhDAEA0E5V1jbokz2FWpNToLU7C1RcWefc52W1aHB8mNJTYpSeEqP4yA4mJgUAc1CGAABoR/JKq/VhTr5W5xRo/b5i1dkdzn1BNu9Tt79F64ae0QoJ4PY3AJ6NMgQAgBtzOAxtyyvTmpx8rckpUM6xcpf93SICTt3+Fq3B8eHy4fY3AHCiDAEA4Gaq6+z6dG+RMnPylbmzQIUVtc59Vos0sFuYhqXEKD0lWklRgbJYmP4aAJpDGQIAwA3kl9co89TkB5/uLVJtw7e3vwX6eeu6npFKT4nRDb2iFd7B18SkAOA+KEMAALRBhmHo66PlysxpXPx0W16Zy/7Oof66qXfj7W9pCRHy9eb2NwBoKcoQAABtRE29XV/kFmvNjnx9uLNAx8pqnPssFqlfXKjz8z+9YoK4/Q0ALhBlCAAAExVW1GrtzsarP5/uLVJVnd25z9/HS9f2aLz9bWhytKKC/ExMCgDtD2UIAIBLyDAM7cqvcN7+tvVwqQzj2/0dg20alhKt9JQYDUmKkM3Hy7ywANDOUYYAAGhldQ0OZe0vdhagIyeqXfZf0TnEWYAuiw3m9jcAuEQoQwAAtIITlXVau6ux/Hy8u0gnaxuc+/y8rbqme6SGnfr8T0ywzcSkAOC5KEMAAFwEhmFoX2Gl1uTkKzMnX5sOnpDjO7e/RQX5aVhytIalxOia7pHy9+X2NwAwG2UIAIDzVG93aMOBEuf6PweKq1z2p3QKVnpKYwHq0zlEViu3vwFAW0IZAgCgBcqq6rVud4Eycwq0bleBymu+vf3N18uqK5MilJ4SrRuTo9UlLMDEpACA70MZAgDgexwo+ub2twJlHyiR/Tv3v4V38NXQXtG6qXe0rukRpUA//tMKAO6Cv7EBADiN3WFo86ETWpOTrzU78rWvsNJlf8+YQA1LiVF6SrT6xYXJi9vfAMAtUYYAAJBUUVOvT/YUac2OfK3dVaATVfXOfd5Wi9ISwzUsOUbpKTHqGsHtbwDQHlCGAAAe63BJlTJz8pW5s0Drc4tVb//29rcQfx8N7RWlYSkxur5XlIJtPiYmBQC0BsoQAMBjOByGth4pbSxAOQXaebzCZX9iZAfn4qcDu4XJ28tqUlIAwKVAGQIAtGtVdQ36ZE+RMnPy9eHOAhWdrHPus1qkQfHhuunU4qeJUYEmJgUAXGqUIQBAu3OsrFqZOQVak5Ovz/cVq67B4dwX5Oet63tFKT0lRjf0ilJogK+JSQEAZqIMAQDcnsNhaPvRMq05tfjp10fLXfbHhfsrPaVx8oPB8eHy9eb2NwAAZQgA4KZq6u36bG+R1uQU6MOd+covr3Xus1ikAV3DnJ//6REdKIuF6a8BAK4oQwAAt1FQUaMPcwq0JqdAn+4tVE39t7e/Bfh66boeURqWEq2hydGKDPQzMSkAwB1QhgAAbZZhGMo5VqHMnHytycnXl0fKXPbHhtgaFz/tHaMrE8Pl5+1lUlIAgDuiDAEA2pTaBrvW55ZozY7G2d/ySqtd9vftEqL0lBgNS4lRSqcgbn8DAJw3yhAAwHTFJ2v14c4CZeYU6JM9haqsszv32XysuqZ7lNJTonVjcrSig20mJgUAtCfnNZ3O3LlzFR8fL5vNprS0NGVnZ59xbH19vZ5++mklJSXJZrOpb9++WrVqVZNxeXl5GjNmjCIiIuTv768rrrhCGzduPJ94AIA2zjAM7c6v0D/W7dUd8z7XoD+s0a/f/Eqrvj6uyjq7YoL9dG9qVy0cN0hbn/iBFowbpHtSu1KEAAAXVYuvDL322muaOnWq5s+fr7S0NM2ePVvDhw/Xrl27FB0d3WT8jBkztGzZMr300ktKTk7WBx98oNtvv12ff/65+vfvL0k6ceKErr76ag0dOlTvv/++oqKitGfPHoWFhV34GQIA2oR6u0PZ+0u05tTnfw6XuN7+dllssIalxOimlBhdFhssq5Xb3wAArctiGIbRkgPS0tI0ePBgPf/885Ikh8OhuLg4/eIXv9C0adOajI+NjdXjjz+uhx56yLntjjvukL+/v5YtWyZJmjZtmj777DN98skn530i5eXlCgkJUVlZmYKDg8/7cQAAF09pVZ3W7SrU6px8fbyrUBW1Dc59vt5WXZUUcerzP9HqFOJvYlIAQHtyrt2gRVeG6urqtGnTJk2fPt25zWq1Kj09XV988UWzx9TW1spmc72twd/fX59++qnz+3fffVfDhw/XXXfdpY8++kidO3fWz3/+c02ePLkl8QAAbUBu4clTV38KtOngCdkd3/6bW2Sgr25MjtawlBhd0z1SHfz46CoAwDwt+q9QUVGR7Ha7YmJiXLbHxMRo586dzR4zfPhwzZo1S9ddd52SkpKUmZmplStXym7/9sOxubm5mjdvnqZOnarf/OY32rBhg375y1/K19dX48aNa/Zxa2trVVv77QJ75eXlzY4DALSuBrtDGw+eUGZOvjJzCpRbVOmyP7ljkIalNBagfl1Cuf0NANBmtPo/yc2ZM0eTJ09WcnKyLBaLkpKSlJGRoUWLFjnHOBwODRo0SH/84x8lSf3799f27ds1f/78M5ahmTNn6ne/+11rxwcANKO8pl4f7SpUZk6+1u4qVFl1vXOfj5dFVyZGaNipK0Bx4QEmJgUA4MxaVIYiIyPl5eWl/Px8l+35+fnq2LFjs8dERUXpnXfeUU1NjYqLixUbG6tp06YpMTHROaZTp07q3bu3y3EpKSl66623zphl+vTpmjp1qvP78vJyxcXFteR0AAAtcKi4Smty8pW5M19ZuSVq+M7tb2EBPhraK1rpvWN0bY9IBdl8TEwKAMC5aVEZ8vX11cCBA5WZmanbbrtNUuNVnczMTE2ZMuWsx9psNnXu3Fn19fV66623dPfddzv3XX311dq1a5fL+N27d6tbt25nfDw/Pz/5+fm1JD4AoAXsDkNbD5/QmpwCrdmRrz0FJ132J0V1UHrvGKWnxGhA1zB5cfsbAMDNtPg2ualTp2rcuHEaNGiQUlNTNXv2bFVWViojI0OSNHbsWHXu3FkzZ86UJGVlZSkvL0/9+vVTXl6ennrqKTkcDj322GPOx3z00Ud11VVX6Y9//KPuvvtuZWdn68UXX9SLL754kU4TAHAuTtY26JPdhVqTU6C1uwpUUlnn3OdltSg1PlzDUqKVnhKj+MgOJiYFAODCtbgMjRo1SoWFhXriiSd0/Phx9evXT6tWrXJOqnDo0CFZrd+u5VpTU6MZM2YoNzdXgYGBGjFihJYuXarQ0FDnmMGDB+vtt9/W9OnT9fTTTyshIUGzZ8/W6NGjL/wMAQBnlVdarcxTs7+t31esOrvDuS/Y5q0bekVrWEq0bugZrZAAbn8DALQfLV5nqK1inSEAODcOh6Gv8sqcBSjnmOtsnPERARqW0nj726D4MPl4Wc/wSAAAtE2tss4QAMA9VdfZ9eneosbpr3cWqLDi26UJrBZpYLcwZwFKiuogi4XP/wAA2j/KEAC0U8fLapS5s3Htn8/2Fqm24dvb3wL9vHV9z6jG2996RSu8g6+JSQEAMAdlCADaCcMw9PXR8sbpr3MKtC2vzGV/lzB/pafEaFhKtNISIuTrze1vAADPRhkCADdWU2/XF/uKtSYnXx/uLNCxshrnPotF6hcX6ixAvWKCuP0NAIDvoAwBgJsprKjV2p0FWpOTr0/2FKm63u7c5+/jpWt7RCo9JUZDk6MVFcR6bAAAnAllCADaOMMwtCu/Qpk5BVq9I19fHinVd+cB7Rhsa1z7p3eMhiRGyObjZV5YAADcCGUIANqgugaHsvYXa82Oxumv80qrXfb36RKiYcmNt79dFhvM7W8AAJwHyhAAtBEllXVau7NAmTvz9fHuIp2sbXDu8/O26prukRp26vM/McE2E5MCANA+UIYAwCSGYWhf4UmtySlQZk6+Nh08Icd3bn+LCvLTsORoDUuJ0TXdI+Xvy+1vAABcTJQhALiE6u0ObThQosycxgkQDhZXuexP6RSs9JRopafE6IrOIbJauf0NAIDWQhkCgFZWVlWvdbsLtCanQB/tKlB5zbe3v/l6WXVlUoRuSonWjSkx6hzqb2JSAAA8C2UIAFrB/qJKZebka01OvjYcOCH7d+5/C+/gqxuTo5WeEq1rekQp0I+/igEAMAP/BQaAi6DB7tDmQ6XOArSvsNJlf8+YQA1LiVF6SrT6xYXJi9vfAAAwHWUIAM5TRU29Pt5dpMycfK3dVaATVfXOfd5Wi9ISwzUsOUbpKTHqGhFgYlIAANAcyhAAtMDhkqpTV38KlLW/WPX2b29/C/H30dBeUUrvHaPrekYp2OZjYlIAAPB9KEMAcBYOh6GtR0q1Zke+MnMKtCu/wmV/YmQHpfeO0bDkaA3sFiZvL6tJSQEAQEtRhgDgNJW1Dfpkz7e3vxWdrHPu87JaNKhbmNJPLX6aGBVoYlIAAHAhKEMAIOlYWbVz8dPP9xWrrsHh3Bfk563re0UpPSVGN/SKUmiAr4lJAQDAxUIZAuCxdhwt16qvjyszJ19fHy132dc1PEDDTi1+Ojg+XL7e3P4GAEB7QxkC4HEMw9DfM/fqb2t2O7dZLNKAro23v6WnRKt7dKAsFqa/BgCgPaMMAfAohmHoj//N0Uuf7JckpafE6IeXd9TQXlGKCPQzOR0AALiUKEMAPIbdYWjGO9v0z+zDkqQnftRbE65JMDkVAAAwC2UIgEeotzs09fUv9e8vj8pqkZ75SR/dPTjO7FgAAMBElCEA7V5NvV1TXt2sNTkF8rZaNPuefvpRn1izYwEAAJNRhgC0a5W1DZr8ykZ9vq9Yft5WzR8zUEOTo82OBQAA2gDKEIB2q6yqXuOXZGvLoVJ18PXSwvGDdWVihNmxAABAG0EZAtAuFVbU6r6FWdp5vEIh/j56eUKq+sWFmh0LAAC0IZQhAO3O0dJqjVmQpdyiSkUG+mnZpFQldww2OxYAAGhjKEMA2pUDRZUavSBLeaXV6hzqr2WT0pQQ2cHsWAAAoA2iDAFoN3Ydr9CYhVkqrKhVQmQHLZuUps6h/mbHAgAAbRRlCEC78OXhUo1bnK3SqnoldwzS0olpigryMzsWAABowyhDANze+txiTVyyQZV1dvWLC9XLGakKCfAxOxYAAGjjKEMA3NranQV6YNkm1TY4NCQxQi+NG6RAP/5qAwAA34/fGAC4rfe+OqaHV2xRg8PQsORozR09QDYfL7NjAQAAN0EZAuCWXt9wWNNWfiWHId3SN1az7u4rHy+r2bEAAIAboQwBcDuLP9uv3/17hyTpnsFx+sPtV8jLajE5FQAAcDeUIQBuwzAMzV27V3/5325J0sRrEjRjZIosFooQAABoOcoQALdgGIaeeX+nXvg4V5L0SHoPPTysB0UIAACcN8oQgDbP4TD0239t1/KsQ5KkGSNTNOnaRJNTAQAAd0cZAtCm1dsd+vUbX+qdrUdlsUh/vP0K3Zva1exYAACgHaAMAWizaurt+sU/t2j1jnx5Wy2aNaqfftw31uxYAACgnaAMAWiTquoadP8rm/Tp3iL5els1b/QADUuJMTsWAABoRyhDANqcsup6TViyQZsOnlCAr5cWjBukq5IizY4FAADaGcoQgDal6GStxi7M1o5j5Qq2eWvJhFQN6BpmdiwAANAOUYYAtBnHyqo1ZkGW9hVWKjLQV0snpimlU7DZsQAAQDtFGQLQJhwsrtToBVk6cqJanUJsWjYpTUlRgWbHAgAA7RhlCIDpdudXaMyCLBVU1Co+IkDLJqWpS1iA2bEAAEA7RxkCYKptR8o0dlGWTlTVq1dMkJZOSlV0kM3sWAAAwANQhgCYJnt/iSYs2aCTtQ3q2yVEL09IVWiAr9mxAACAh6AMATDFul0FemDZJtXUO5SWEK6F4wcr0I+/kgAAwKXDbx4ALrn3tx3TL1dsUb3d0NBeUZo3ZqBsPl5mxwIAAB6GMgTgknpz0xE99uaXchjSyCs66W+j+snX22p2LAAA4IEoQwAumZc/P6An3/1aknT3oC6a+ZM+8rJaTE4FAAA8FWUIwCUxd+1e/fmDXZKkjKvj9duRvWWlCAEAABNRhgC0KsMw9KcPdmneun2SpF/e2F2P3tRTFgtFCAAAmIsyBKDVOByGnnz3ay1df1CSNP3mZP3s+iSTUwEAADSiDAFoFQ12hx578yut3JIni0X6f7ddrtFp3cyOBQAA4EQZAnDR1TbY9ct/btEHX+fLy2rRrLv76tZ+nc2OBQAA4IIyBOCiqqpr0M+WbtIne4rk62XV8z/trx9c1tHsWAAAAE1QhgBcNOU19ZqweIM2Hjwhfx8vvTR2kK7pEWl2LAAAgGZRhgBcFMUnazVucba255UryOatJRmDNbBbuNmxAAAAzogyBOCCHS+r0ZiFWdpbcFIRHXz1ysRUXRYbYnYsAACAs6IMAbggh0uq9NMF63W4pFodg21aNilN3aMDzY4FAADwvShDAM7b3oIKjV6QpfzyWnUND9DySWmKCw8wOxYAAMA5oQwBOC/b88o0dlG2Sirr1CM6UMsmpSkm2GZ2LAAAgHNGGQLQYhsPlChj8QZV1Dbois4henlCqsI7+JodCwAAoEUoQwBa5JM9hbr/lU2qrrcrNT5cC8cPUpDNx+xYAAAALWY9n4Pmzp2r+Ph42Ww2paWlKTs7+4xj6+vr9fTTTyspKUk2m019+/bVqlWrXMY89dRTslgsLl/JycnnEw1AK1q1/bgmLtmo6nq7ru8ZpZcnpFKEAACA22pxGXrttdc0depUPfnkk9q8ebP69u2r4cOHq6CgoNnxM2bM0AsvvKDnnntOO3bs0AMPPKDbb79dW7ZscRl32WWX6dixY86vTz/99PzOCECreHvLET306mbV2R26+fKOemnsIPn7epkdCwAA4Ly1uAzNmjVLkydPVkZGhnr37q358+crICBAixYtanb80qVL9Zvf/EYjRoxQYmKiHnzwQY0YMUJ//etfXcZ5e3urY8eOzq/ISFatB9qKpesP6tHXvpTdYejOgV303L395et9XheWAQAA2owW/TZTV1enTZs2KT09/dsHsFqVnp6uL774otljamtrZbO5zjDl7+/f5MrPnj17FBsbq8TERI0ePVqHDh06a5ba2lqVl5e7fAG4+Oat26ffvrNdkjT+qnj96Y4+8vaiCAEAAPfXot9oioqKZLfbFRMT47I9JiZGx48fb/aY4cOHa9asWdqzZ48cDodWr16tlStX6tixY84xaWlpWrJkiVatWqV58+Zp//79uvbaa1VRUXHGLDNnzlRISIjzKy4uriWnAuB7GIahP3+wU8+u2ilJemhokp68pbesVovJyQAAAC6OVv/n3Tlz5qhHjx5KTk6Wr6+vpkyZooyMDFmt3/7om2++WXfddZf69Omj4cOH67///a9KS0v1+uuvn/Fxp0+frrKyMufX4cOHW/tUAI/hcBj63b93aO7afZKk//thsn49PFkWC0UIAAC0Hy0qQ5GRkfLy8lJ+fr7L9vz8fHXs2LHZY6KiovTOO++osrJSBw8e1M6dOxUYGKjExMQz/pzQ0FD17NlTe/fuPeMYPz8/BQcHu3wBuHANdocee+srLfn8gCTp97depgdvSDI3FAAAQCtoURny9fXVwIEDlZmZ6dzmcDiUmZmpIUOGnPVYm82mzp07q6GhQW+99ZZuvfXWM449efKk9u3bp06dOrUkHoALVNfg0C9XbNGbm47Iy2rRrLv76r4h8WbHAgAAaBUtXnR16tSpGjdunAYNGqTU1FTNnj1blZWVysjIkCSNHTtWnTt31syZMyVJWVlZysvLU79+/ZSXl6ennnpKDodDjz32mPMxf/WrX+mWW25Rt27ddPToUT355JPy8vLSvffee5FOE8D3qa6z64Flm/TR7kL5eln193v764eXN3/FFwAAoD1ocRkaNWqUCgsL9cQTT+j48ePq16+fVq1a5ZxU4dChQy6fB6qpqdGMGTOUm5urwMBAjRgxQkuXLlVoaKhzzJEjR3TvvfequLhYUVFRuuaaa7R+/XpFRUVd+BkC+F4VNfWauGSjsg+UyOZj1Yv3DdJ1PXn/AQCA9s1iGIZhdoiLoby8XCEhISorK+PzQ0ALnKis07jF2frqSJmC/Ly1KGOwBseHmx0LAADgvJ1rN2jxlSEA7UdBeY3GLMzS7vyTCu/gq1cmpOryziFmxwIAALgkKEOAhzpcUqUxC7N0sLhKMcF+Wj4pTd2jg8yOBQAAcMlQhgAPtLfgpMYsyNLx8hrFhftr+cQr1TUiwOxYAAAAlxRlCPAwXx8t09iF2SqurFP36EAtm5imjiE2s2MBAABccpQhwINsOnhC4xdnq6KmQZfFBuuVCamKCPQzOxYAAIApKEOAh/hsb5Emv7JRVXV2DeoWpkUZgxVs8zE7FgAAgGkoQ4AHWL0jXw8t36w6u0PX9ojUC/cNVIAvb38AAODZ+G0IaOf+tTVPU1//UnaHoeGXxejv9/aXn7eX2bEAAABMRxkC2rFXsw7p8Xe2yTCkn/TvrD/d2UfeXlazYwEAALQJlCGgnXrx43364393SpLuu7Kbfvfjy2S1WkxOBQAA0HZQhoB2xjAM/W31bv39w72SpAdvSNJjw3vJYqEIAQAAfBdlCGhHDMPQ0//ZocWfHZAk/Xp4Lz00tLu5oQAAANooyhDQTtgdhn6zcpte23hYkvS7H1+mcVfFmxsKAACgDaMMAe1AXYNDj76+Ve99dUxWi/SnO/vqzoFdzI4FAADQplGGADdXU2/Xg8s2ae2uQvl4WfT3e/rr5is6mR0LAACgzaMMAW7sZG2DJi7ZoKz9JbL5WDV/zEDd0Cva7FgAAABugTIEuKnSqjqNW7xBXx4uVaCftxaNH6zUhHCzYwEAALgNyhDghgoqanTfgmztyq9QWICPXp6Qqj5dQs2OBQAA4FYoQ4CbOXKiSmMWZOlAcZWig/y0bFKaesYEmR0LAADA7VCGADeSW3hSYxZk6WhZjbqE+Wv5pDR1i+hgdiwAAAC3RBkC3ETOsXLdtzBLRSfrlBTVQcsmpalTiL/ZsQAAANwWZQhwA1sOndC4Rdkqr2lQ707BemViqiID/cyOBQAA4NYoQ0Ab9/m+Ik16eaOq6uwa0DVUizNSFeLvY3YsAAAAt0cZAtqwzJx8Pbh8s+oaHLq6e4RevG+QOvjxtgUAALgY+K0KaKP+/eVRPfraVjU4DN3UO0bP3dtfNh8vs2MBAAC0G5QhoA1akX1I09/eJsOQbu0Xq7/c1Vc+XlazYwEAALQrlCGgjVnwSa7+33s5kqSfpnXV/7v1clmtFpNTAQAAtD+UIaCNMAxDczL3aPaaPZKkn12XqGk3J8tioQgBaCWG4fq/jd+0vW1Op/4+tFga/7/L/6rl2/j7FfB4lKGLrXC3tPq3ZqeAmzFkaE/+SV1eUqUFPlJSVKDiSwNk+Sf/oW5Vl/wXO7N+bku36RzHkc8t8+EMLkK5kuVUXztDaTuvbRfh8VxyqpltF3reauGx35dd5zjuIj2Pl+y8z2XbWXKe9zZd2OOdNXszP6tTXyk8Ue6CMnSx1ZRJu1eZnQJuxiKpp6Se38yPUHLqCwBwiRjfuVJmbhLArY38K2XIo4XFSz9+zuwUcBMNDkOvbTisL4+UyirpjgFdNDgh3OxYHub0f906j20ut9o0t+3041pybGtvszTZdVGek4u9rdlzUDPb2tLzebY/A239OT59zJnGXcJtLle7jNP+Vy3YdvpjtOTYs237zuM2l/O8tp3lvM91W7PZ1cy2i/FcnMtzez7n04rPbav8GbiQx2uFPwNNtuncjz2f8w7sKHdCGbrYAqOkAWPNTgE3UFNv15RXN2vNwUh5Wy2afU8/De4Ta3YsAAAAj0EZAkxQWdugya9s1Of7iuXnbdX8MQM1NDna7FgAAAAehTIEXGJlVfUavyRbWw6VqoOvlxaOH6wrEyPMjgUAAOBxKEPAJVRYUav7FmZp5/EKhfj76OUJqeoXF2p2LAAAAI9EGQIukbzSat23IEu5RZWKDPTTskmpSu4YbHYsAAAAj0UZAi6B/UWVGrMgS3ml1eoc6q9lk9KUENnB7FgAAAAejTIEtLKdx8s1ZkG2ik7WKjGyg5ZNSlNsqL/ZsQAAADweZQhoRVsPl2rcomyVVdcrpVOwXpmQqqggP7NjAQAAQJQhoNWszy3WxCUbVFlnV7+4UL2ckaqQAB+zYwEAAOAUyhDQCtbuLNADyzaptsGhIYkRemncIAX68XYDAABoS/jtDLjI3vvqmB5esUUNDkPDkqM1d/QA2Xy8zI4FAACA01CGgIvo9Q2HNW3lV3IY0i19YzXr7r7y8bKaHQsAAADNoAwBF8miT/fr6f/skCTdMzhOf7j9CnlZLSanAgAAwJlQhoALZBiGnv9wr/66erckadI1CXp8ZIosFooQAABAW0YZAi6AYRia+f5OvfhxriTp0fSe+uWw7hQhAAAAN0AZAs6T3WHot//arlezDkmSZoxM0aRrE01OBQAAgHNFGQLOQ73doV+98aX+tfWoLBZp5u1X6J7UrmbHAgAAQAtQhoAWqqm36xf/3KLVO/LlbbXob6P66Za+sWbHAgAAQAtRhoAWqKpr0P2vbNKne4vk623VvNEDNCwlxuxYAAAAOA+UIeAclVXXa8KSDdp08IQCfL20YNwgXZUUaXYsAAAAnCfKEHAOik7WauzCbO04Vq5gm7eWTEjVgK5hZscCAADABaAMAd/jWFm1Ri/IUm5hpSIDfbV0YppSOgWbHQsAAAAXiDIEnMXB4kr99KUs5ZVWKzbEpmWT0pQYFWh2LAAAAFwElCHgDHbnV2jMgiwVVNQqPiJAyydfqc6h/mbHAgAAwEVCGQKa8dWRUo1dlK3SqnoldwzSKxNTFR1kMzsWAAAALiLKEHCa7P0lmrBkg07WNqhvXKhezhis0ABfs2MBAADgIqMMAd+xbleBHli2STX1Dl2ZGK4F4wYr0I+3CQAAQHvEb3nAKe9vO6Zfrtiieruhob2iNG/MQNl8vMyOBQAAgFZCGQIkvbnpiB5780s5DGnkFZ30t1H95OttNTsWAAAAWhFlCB7v5c8P6Ml3v5Yk3T2oi2b+pI+8rBaTUwEAAKC1UYbg0eau3as/f7BLkpRxdbx+O7K3rBQhAAAAj0AZgkcyDEPPrtql+R/tkyT9clgPPZreQxYLRQgAAMBTUIbgcRwOQ0+8u13L1h+SJP1mRLLuvy7J5FQAAAC41ChD8CgNdocee/MrrdySJ4tF+sNtV+inaV3NjgUAAAATUIbgMWob7PrlP7fog6/z5WW1aNbdfXVrv85mxwIAAIBJzmvu4Llz5yo+Pl42m01paWnKzs4+49j6+no9/fTTSkpKks1mU9++fbVq1aozjn/mmWdksVj0yCOPnE80oFlVdQ2a9PJGffB1vny9rJo/ZiBFCAAAwMO1uAy99tprmjp1qp588klt3rxZffv21fDhw1VQUNDs+BkzZuiFF17Qc889px07duiBBx7Q7bffri1btjQZu2HDBr3wwgvq06dPy88EOIPymnqNXZitT/YUyd/HS4vGD9ZNvWPMjgUAAACTtbgMzZo1S5MnT1ZGRoZ69+6t+fPnKyAgQIsWLWp2/NKlS/Wb3/xGI0aMUGJioh588EGNGDFCf/3rX13GnTx5UqNHj9ZLL72ksLCw8zsb4DTFJ2v105fWa+PBEwqyeWvZpFRd0yPS7FgAAABoA1pUhurq6rRp0yalp6d/+wBWq9LT0/XFF180e0xtba1sNpvLNn9/f3366acu2x566CGNHDnS5bGBC3G8rEajXlyv7XnliujgqxX3X6mB3cLNjgUAAIA2okUTKBQVFclutysmxvUWo5iYGO3cubPZY4YPH65Zs2bpuuuuU1JSkjIzM7Vy5UrZ7XbnmBUrVmjz5s3asGHDOWepra1VbW2t8/vy8vKWnArauUPFVRq9cL0Ol1SrU4hNSyemqXt0oNmxAAAA0Iac1wQKLTFnzhz16NFDycnJ8vX11ZQpU5SRkSGrtfFHHz58WA8//LCWL1/e5ArS2cycOVMhISHOr7i4uNY6BbiZPfkVuuuFz3W4pFrdIgL0+s+GUIQAAADQRIvKUGRkpLy8vJSfn++yPT8/Xx07dmz2mKioKL3zzjuqrKzUwYMHtXPnTgUGBioxMVGStGnTJhUUFGjAgAHy9vaWt7e3PvroI/3973+Xt7e3yxWk75o+fbrKysqcX4cPH27JqaCd2p5XplEvrld+ea16xgTqjZ8NUVx4gNmxAAAA0Aa1qAz5+vpq4MCByszMdG5zOBzKzMzUkCFDznqszWZT586d1dDQoLfeeku33nqrJGnYsGHatm2btm7d6vwaNGiQRo8era1bt8rLy6vZx/Pz81NwcLDLFzzbxgMluvfF9SqprFOfLiF67f4hig4+96uNAAAA8CwtXnR16tSpGjdunAYNGqTU1FTNnj1blZWVysjIkCSNHTtWnTt31syZMyVJWVlZysvLU79+/ZSXl6ennnpKDodDjz32mCQpKChIl19+ucvP6NChgyIiIppsB87kkz2Fuv+VTaqutys1IVwLxw1SkM3H7FgAAABow1pchkaNGqXCwkI98cQTOn78uPr166dVq1Y5J1U4dOiQ8/NAklRTU6MZM2YoNzdXgYGBGjFihJYuXarQ0NCLdhLwbKu2H9cv/7lFdXaHru8ZpfljBsrft/krigAAAMA3LIZhGGaHuBjKy8sVEhKisrIybpnzIG9vOaJfvfGV7A5DN1/eUXPu6S9f71afFwQAAABt2Ll2gxZfGQLaiqXrD+q372yXJN05sIue+ckV8vaiCAEAAODcUIbgluat26dnVzWubTX+qng98aPeslotJqcCAACAO6EMwa0YhqG//G+X5q7dJ0maMrS7/r8f9JTFQhECAABAy1CG4DYcDkO/+/fXevmLg5KkaTcn64Hrk0xOBQAAAHdFGYJbaLA79H9vbdNbm4/IYpGevvVy3XdlN7NjAQAAwI1RhtDm1TU49PCKLXp/+3F5WS36y119dHv/LmbHAgAAgJujDKFNq66z64Flm/TR7kL5eln13E/7a/hlHc2OBQAAgHaAMoQ2q6KmXhOXbFT2gRL5+3jpxbEDdW2PKLNjAQAAoJ2gDKFNOlFZp3GLs/XVkTIF+XlrccZgDYoPNzsWAAAA2hHKENqcgvIajVmYpd35JxXewVevTEjV5Z1DzI4FAACAdoYyhDblcEmVxizM0sHiKsUE+2n5pDR1jw4yOxYAAADaIcoQ2oy9BSc1ZkGWjpfXKC7cX69OulJx4QFmxwIAAEA7RRlCm7A9r0zjFmWruLJO3aMDtWximjqG2MyOBQAAgHaMMgTTbTpYovGLN6iipkGXdw7WKxPSFN7B1+xYAAAAaOcoQzDVp3uKNPmVjaqut2twfJgWjh+sYJuP2bEAAADgAShDMM3qHfl6aPlm1dkdurZHpF68b5D8fb3MjgUAAAAPQRmCKf61NU9TX/9SdoehH17WUXPu7Sc/b4oQAAAALh3KEC65V7MO6fF3tskwpJ/076w/3dlH3l5Ws2MBAADAw1CGcEm9+PE+/fG/OyVJ913ZTb/78WWyWi0mpwIAAIAnogzhkjAMQ39bvVt//3CvJOnBG5L02PBeslgoQgAAADAHZQitzuEw9Pv3dmjxZwckSb8e3ksPDe1ubigAAAB4PMoQWpXdYWj6yq/0+sYjkqSnb71MY4fEmxsKAAAAEGUIraiuwaFHX9uq97Ydk9Ui/enOvrpzYBezYwEAAACSKENoJTX1dj24bJPW7iqUj5dFf7+nv26+opPZsQAAAAAnyhAuupO1DZq4ZIOy9pfI5mPVC/cN0vU9o8yOBQAAALigDOGiKq2q07jFG/Tl4VIF+Xlr4fjBSk0INzsWAAAA0ARlCBdNQUWN7luQrV35FQoL8NErE9J0RZcQs2MBAAAAzaIM4aI4cqJKYxZk6UBxlaKD/LRsUpp6xgSZHQsAAAA4I8oQLlhu4UmNWZClo2U16hLmr+WT0tQtooPZsQAAAICzogzhguw4Wq6xi7JUdLJOSVEdtGxSmjqF+JsdCwAAAPhelCGct82HTmj8omyV1zSod6dgvTIxVZGBfmbHAgAAAM4JZQjn5fO9RZr0ykZV1dk1sFuYFo0frBB/H7NjAQAAAOeMMoQWy8zJ14PLN6uuwaFrukfqxbEDFeDLHyUAAAC4F36DRYv8+8ujevS1rWpwGLqpd4yeu7e/bD5eZscCAAAAWowyhHO2IvuQpr+9TYYh3dYvVn++q698vKxmxwIAAADOC2UI52TBJ7n6f+/lSJJGp3XV72+9XFarxeRUAAAAwPmjDOGsDMPQnMw9mr1mjyTpZ9clatrNybJYKEIAAABwb5QhnJFhGPrDezla8Ol+SdKvftBTDw3tThECAABAu0AZQrPsDkOPv71NKzYcliQ9eUtvZVydYHIqAAAA4OKhDKGJertDj762Vf/56pisFumZO/ro7kFxZscCAAAALirKEFzU1Nv10PLNytxZIB8vi2aP6q+RfTqZHQsAAAC46ChDcKqsbdCklzfqi9xi+XlbNf++gRraK9rsWAAAAECroAxBklRWVa/xS7K15VCpAv28tWDcIF2ZGGF2LAAAAKDVUIagwopa3bcwSzuPVyg0wEcvZ6Sqb1yo2bEAAACAVkUZ8nB5pdW6b0GWcosqFRnop+WT0tSrY5DZsQAAAIBWRxnyYPuLKjVmQZbySqvVOdRfyyalKSGyg9mxAAAAgEuCMuShdh4v15gF2So6WavEyA5aNilNsaH+ZscCAAAALhnKkAfaerhU4xZlq6y6XimdgvXKhFRFBfmZHQsAAAC4pChDHuaLfcWa9PIGVdbZ1b9rqJaMT1VIgI/ZsQAAAIBLjjLkQdbuLNADyzaptsGhq5Ii9NLYQergxx8BAAAAeCZ+E/YQ7311TA+v2KIGh6H0lGg9/9MBsvl4mR0LAAAAMA1lyAO8vuGwpq38Sg5D+nHfWP317r7y8bKaHQsAAAAwFWWonVv06X49/Z8dkqR7U+P0/267Ql5Wi8mpAAAAAPNRhtopwzD0/Id79dfVuyVJk69N0G9GpMhioQgBAAAAEmWoXTIMQzPf36kXP86VJD2a3lO/HNadIgQAAAB8B2WonbE7DP32X9v1atYhSdKMkSmadG2iyakAAACAtocy1I7U2x361Rtf6l9bj8pikWbefoXuSe1qdiwAAACgTaIMtRM19XZNeXWL1uTky9tq0d9G9dMtfWPNjgUAAAC0WZShdqCytkH3L92oz/YWy9fbqvljBujG5BizYwEAAABtGmXIzZVV1ytjcbY2HypVB18vvTRukK5KijQ7FgAAANDmUYbcWNHJWo1dmK0dx8oV4u+jJRmD1b9rmNmxAAAAALdAGXJTx8qqNXpBlnILKxUZ6KelE1OV0inY7FgAAACA26AMuaGDxZX66UtZyiutVmyITcsmpSkxKtDsWAAAAIBboQy5md35FRqzIEsFFbWKjwjQ8slXqnOov9mxAAAAALdDGXIjXx0p1dhF2SqtqldyxyC9MjFV0UE2s2MBAAAAboky5Caycos18eWNOlnboL5xoXo5Y7BCA3zNjgUAAAC4LcqQG1i3q0A/W7pJtQ0OXZkYrgXjBivQj5cOAAAAuBDW8zlo7ty5io+Pl81mU1pamrKzs884tr6+Xk8//bSSkpJks9nUt29frVq1ymXMvHnz1KdPHwUHBys4OFhDhgzR+++/fz7R2p3/bjumya9sVG2DQzcmR2tJRipFCAAAALgIWlyGXnvtNU2dOlVPPvmkNm/erL59+2r48OEqKChodvyMGTP0wgsv6LnnntOOHTv0wAMP6Pbbb9eWLVucY7p06aJnnnlGmzZt0saNG3XjjTfq1ltv1ddff33+Z9YOvLnpiKa8uln1dkMj+3TS/DEDZfPxMjsWAAAA0C5YDMMwWnJAWlqaBg8erOeff16S5HA4FBcXp1/84heaNm1ak/GxsbF6/PHH9dBDDzm33XHHHfL399eyZcvO+HPCw8P15z//WRMnTjynXOXl5QoJCVFZWZmCg91/vZ2XPz+gJ99tLIOjBsXpjz+5Ql5Wi8mpAAAAgLbvXLtBi64M1dXVadOmTUpPT//2AaxWpaen64svvmj2mNraWtlsrjOe+fv769NPP212vN1u14oVK1RZWakhQ4acMUttba3Ky8tdvtqLuWv3OovQhKsT9MwdFCEAAADgYmtRGSoqKpLdbldMTIzL9piYGB0/frzZY4YPH65Zs2Zpz549cjgcWr16tVauXKljx465jNu2bZsCAwPl5+enBx54QG+//bZ69+59xiwzZ85USEiI8ysuLq4lp9ImGYahZ97fqT9/sEuS9PCwHvrtj1JksVCEAAAAgIvtvCZQaIk5c+aoR48eSk5Olq+vr6ZMmaKMjAxZra4/ulevXtq6dauysrL04IMPaty4cdqxY8cZH3f69OkqKytzfh0+fLi1T6VVORyGfvuv7Zr/0T5J0uMjUvToTT0pQgAAAEAraVEZioyMlJeXl/Lz81225+fnq2PHjs0eExUVpXfeeUeVlZU6ePCgdu7cqcDAQCUmJrqM8/X1Vffu3TVw4EDNnDlTffv21Zw5c86Yxc/Pzzn73Ddf7qrB7tCv3vhSy9YfksUi/fH2KzT5usTvPxAAAADAeWtRGfL19dXAgQOVmZnp3OZwOJSZmXnWz/dIks1mU+fOndXQ0KC33npLt95661nHOxwO1dbWtiSeW6ptsOuhVzdr5ZY8eVktmj2qn36a1tXsWAAAAEC71+IFa6ZOnapx48Zp0KBBSk1N1ezZs1VZWamMjAxJ0tixY9W5c2fNnDlTkpSVlaW8vDz169dPeXl5euqpp+RwOPTYY485H3P69Om6+eab1bVrV1VUVOjVV1/VunXr9MEHH1yk02ybquoa9LOlm/TJniL5els196cDdFPvmO8/EAAAAMAFa3EZGjVqlAoLC/XEE0/o+PHj6tevn1atWuWcVOHQoUMunweqqanRjBkzlJubq8DAQI0YMUJLly5VaGioc0xBQYHGjh2rY8eOKSQkRH369NEHH3ygm2666cLPsI0qq67XxCUbtPHgCQX4eumlsYN0dfdIs2MBAAAAHqPF6wy1Ve60zlDxyVqNXZStr4+WK9jmrcUZqRrYLczsWAAAAEC7cK7doMVXhnBhjpfVaMzCLO0tOKmIDr56ZWKqLosNMTsWAAAA4HEoQ5fQoeIqjV64XodLqtUpxKZlk9KUFBVodiwAAADAI1GGLpE9+RUaszBL+eW16hYRoOWT0tQlLMDsWAAAAIDHogxdAtvzyjR2UbZKKuvUMyZQyyamKTrYZnYsAAAAwKNRhlrZhgMlmrB4gypqG9SnS4hezkhVWAdfs2MBAAAAHo8y1Io+3l2o+5duVE29Q6kJ4Vo4bpCCbD5mxwIAAAAgylCrWbX9uH75zy2qszt0fc8ozR8zUP6+XmbHAgAAAHAKZagVrNx8RL9+8yvZHYZGXNFRs0f1l6+39fsPBAAAAHDJUIYusn9/eVRTX/9SknTnwC565idXyNuLIgQAAAC0NZShi2xIUoQSozrouh5ReuJHvWW1WsyOBAAAAKAZlKGLLDLQT2///GoF27xlsVCEAAAAgLaKMtQKQvyZMQ4AAABo6/gwCwAAAACPRBkCAAAA4JEoQwAAAAA8EmUIAAAAgEeiDAEAAADwSJQhAAAAAB6JMgQAAADAI1GGAAAAAHgkyhAAAAAAj0QZAgAAAOCRKEMAAAAAPBJlCAAAAIBHogwBAAAA8EjeZge4WAzDkCSVl5ebnAQAAACAmb7pBN90hDNpN2WooqJCkhQXF2dyEgAAAABtQUVFhUJCQs6432J8X11yEw6HQ0ePHlVQUJAsFoupWcrLyxUXF6fDhw8rODjY1Cy4OHhN2yde1/aH17R94nVtf3hN25+29poahqGKigrFxsbKaj3zJ4PazZUhq9WqLl26mB3DRXBwcJv4w4CLh9e0feJ1bX94TdsnXtf2h9e0/WlLr+nZrgh9gwkUAAAAAHgkyhAAAAAAj0QZagV+fn568skn5efnZ3YUXCS8pu0Tr2v7w2vaPvG6tj+8pu2Pu76m7WYCBQAAAABoCa4MAQAAAPBIlCEAAAAAHokyBAAAAMAjUYYAAAAAeCTKUAt9/PHHuuWWWxQbGyuLxaJ33nnne49Zt26dBgwYID8/P3Xv3l1Llixp9ZxomZa+ruvWrZPFYmnydfz48UsTGN9r5syZGjx4sIKCghQdHa3bbrtNu3bt+t7j3njjDSUnJ8tms+mKK67Qf//730uQFufifF7TJUuWNHmf2my2S5QY52LevHnq06ePc6HGIUOG6P333z/rMbxP27aWvqa8T93PM888I4vFokceeeSs49zhvUoZaqHKykr17dtXc+fOPafx+/fv18iRIzV06FBt3bpVjzzyiCZNmqQPPviglZOiJVr6un5j165dOnbsmPMrOjq6lRKipT766CM99NBDWr9+vVavXq36+nr94Ac/UGVl5RmP+fzzz3Xvvfdq4sSJ2rJli2677Tbddttt2r59+yVMjjM5n9dUalwN/bvv04MHD16ixDgXXbp00TPPPKNNmzZp48aNuvHGG3Xrrbfq66+/bnY879O2r6WvqcT71J1s2LBBL7zwgvr06XPWcW7zXjVw3iQZb7/99lnHPPbYY8Zll13msm3UqFHG8OHDWzEZLsS5vK5r1641JBknTpy4JJlw4QoKCgxJxkcffXTGMXfffbcxcuRIl21paWnGz372s9aOh/NwLq/p4sWLjZCQkEsXChdFWFiYsWDBgmb38T51T2d7TXmfuo+KigqjR48exurVq43rr7/eePjhh8841l3eq1wZamVffPGF0tPTXbYNHz5cX3zxhUmJcDH169dPnTp10k033aTPPvvM7Dg4i7KyMklSeHj4GcfwfnUv5/KaStLJkyfVrVs3xcXFfe+/TsNcdrtdK1asUGVlpYYMGdLsGN6n7uVcXlOJ96m7eOihhzRy5Mgm78HmuMt71dvsAO3d8ePHFRMT47ItJiZG5eXlqq6ulr+/v0nJcCE6deqk+fPna9CgQaqtrdWCBQt0ww03KCsrSwMGDDA7Hk7jcDj0yCOP6Oqrr9bll19+xnFner/yWbC251xf0169emnRokXq06ePysrK9Je//EVXXXWVvv76a3Xp0uUSJsbZbNu2TUOGDFFNTY0CAwP19ttvq3fv3s2O5X3qHlrymvI+dQ8rVqzQ5s2btWHDhnMa7y7vVcoQcB569eqlXr16Ob+/6qqrtG/fPv3tb3/T0qVLTUyG5jz00EPavn27Pv30U7Oj4CI519d0yJAhLv8afdVVVyklJUUvvPCCfv/737d2TJyjXr16aevWrSorK9Obb76pcePG6aOPPjrjL89o+1rymvI+bfsOHz6shx9+WKtXr253k1tQhlpZx44dlZ+f77ItPz9fwcHBXBVqZ1JTU/lluw2aMmWK/vOf/+jjjz/+3n9hPNP7tWPHjq0ZES3Uktf0dD4+Purfv7/27t3bSulwPnx9fdW9e3dJ0sCBA7VhwwbNmTNHL7zwQpOxvE/dQ0te09PxPm17Nm3apIKCApe7X+x2uz7++GM9//zzqq2tlZeXl8sx7vJe5TNDrWzIkCHKzMx02bZ69eqz3jcL97R161Z16tTJ7Bg4xTAMTZkyRW+//bY+/PBDJSQkfO8xvF/btvN5TU9nt9u1bds23qttnMPhUG1tbbP7eJ+6p7O9pqfjfdr2DBs2TNu2bdPWrVudX4MGDdLo0aO1devWJkVIcqP3qtkzOLibiooKY8uWLcaWLVsMScasWbOMLVu2GAcPHjQMwzCmTZtm3Hfffc7xubm5RkBAgPHrX//ayMnJMebOnWt4eXkZq1atMusU0IyWvq5/+9vfjHfeecfYs2ePsW3bNuPhhx82rFarsWbNGrNOAad58MEHjZCQEGPdunXGsWPHnF9VVVXOMffdd58xbdo05/efffaZ4e3tbfzlL38xcnJyjCeffNLw8fExtm3bZsYp4DTn85r+7ne/Mz744ANj3759xqZNm4x77rnHsNlsxtdff23GKaAZ06ZNMz766CNj//79xldffWVMmzbNsFgsxv/+9z/DMHifuqOWvqa8T93T6bPJuet7lTLUQt9MqXz617hx4wzDMIxx48YZ119/fZNj+vXrZ/j6+hqJiYnG4sWLL3lunF1LX9dnn33WSEpKMmw2mxEeHm7ccMMNxocffmhOeDSruddTksv77/rrr3e+xt94/fXXjZ49exq+vr7GZZddZrz33nuXNjjO6Hxe00ceecTo2rWr4evra8TExBgjRowwNm/efOnD44wmTJhgdOvWzfD19TWioqKMYcOGOX9pNgzep+6opa8p71P3dHoZctf3qsUwDOPSXYcCAAAAgLaBzwwBAAAA8EiUIQAAAAAeiTIEAAAAwCNRhgAAAAB4JMoQAAAAAI9EGQIAAADgkShDAAAAADwSZQgAAACAR6IMAQAAAPBIlCEAAAAAHokyBAAAAMAjUYYAAAAAeKT/H1tS6C44bAQ8AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_metric_per_epoch(train_acc, val_acc, EPOCHS, \"Accuracy per epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDrkJdygZH7V",
        "outputId": "45b2ee60-5fb5-434e-aadf-914d1cdd8e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-geo       0.73      0.89      0.80       650\n",
            "       B-gpe       0.97      0.88      0.92       603\n",
            "       B-org       0.68      0.67      0.68       534\n",
            "       B-per       0.94      0.91      0.92       796\n",
            "       B-tim       0.85      0.87      0.86        99\n",
            "           O       0.99      0.98      0.99      6833\n",
            "\n",
            "    accuracy                           0.95      9515\n",
            "   macro avg       0.86      0.87      0.86      9515\n",
            "weighted avg       0.95      0.95      0.95      9515\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.classification_report(labels, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haQIsYpMZMez",
        "outputId": "ddbba9dd-af97-49f2-d910-06442c0ea71f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['John', 'Smith', 'is', 'a', 'software', 'engineer', 'at', 'Microsoft', 'Corporation.', 'He', 'lives', 'in', 'Seattle,', 'Washington', 'and', 'enjoys', 'hiking', 'in', 'the', 'nearby', 'mountains.', 'He', 'has', 'a', 'Bachelor’s', 'degree', 'in', 'Computer', 'Science', 'from', 'the', 'University', 'of', 'Washington.']\n",
            "['B-per', 'B-per', 'O', 'O', 'O', 'O', 'O', 'B-org', 'B-org', 'O', 'O', 'O', 'B-geo', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'B-org', 'O', 'B-geo']\n"
          ]
        }
      ],
      "source": [
        "perform_NER(model2, sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0xNY_nWNn50"
      },
      "outputs": [],
      "source": [
        "save_model(model2, \"Bert NER_4_epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFhf1htSRPu8"
      },
      "source": [
        "# 7. Import Annotated Medical Data for Use in Model\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "kytkInDNZzy6",
        "outputId": "ddfac38d-1343-49c8-cbcf-abdcdedb2f86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bioc\n",
            "  Downloading bioc-2.0.post5-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bioc) (4.65.0)\n",
            "Collecting intervaltree\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.10/dist-packages (from bioc) (4.9.2)\n",
            "Collecting jsonlines>=1.2.0\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines>=1.2.0->bioc) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree->bioc) (2.4.0)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26114 sha256=47d6023e94d25d283dececc65db146a865719c1af667555807e5867a2be64583\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: jsonlines, intervaltree, bioc\n",
            "Successfully installed bioc-2.0.post5 intervaltree-3.1.0 jsonlines-3.1.0\n"
          ]
        },
        {
          "ename": "XMLSyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-1-f2a8a8350206>\"\u001b[0m, line \u001b[1;32m8\u001b[0m, in \u001b[1;35m<cell line: 7>\u001b[0m\n    collection = biocxml.load(fp)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/bioc/biocxml/__init__.py\"\u001b[0m, line \u001b[1;32m27\u001b[0m, in \u001b[1;35mload\u001b[0m\n    return load1(fp)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/bioc/biocxml/decoder.py\"\u001b[0m, line \u001b[1;32m294\u001b[0m, in \u001b[1;35mload\u001b[0m\n    return BioCXMLDecoder().decode(fp)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/bioc/biocxml/decoder.py\"\u001b[0m, line \u001b[1;32m40\u001b[0m, in \u001b[1;35mdecode\u001b[0m\n    tree = etree.parse(fp)\n",
            "  File \u001b[1;32m\"src/lxml/etree.pyx\"\u001b[0m, line \u001b[1;32m3541\u001b[0m, in \u001b[1;35mlxml.etree.parse\u001b[0m\n",
            "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1900\u001b[0m, in \u001b[1;35mlxml.etree._parseDocument\u001b[0m\n",
            "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1920\u001b[0m, in \u001b[1;35mlxml.etree._parseFilelikeDocument\u001b[0m\n",
            "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1814\u001b[0m, in \u001b[1;35mlxml.etree._parseDocFromFilelike\u001b[0m\n",
            "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1204\u001b[0m, in \u001b[1;35mlxml.etree._BaseParser._parseDocFromFilelike\u001b[0m\n",
            "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m618\u001b[0m, in \u001b[1;35mlxml.etree._ParserContext._handleParseResultDoc\u001b[0m\n",
            "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m728\u001b[0m, in \u001b[1;35mlxml.etree._handleParseResult\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"src/lxml/parser.pxi\"\u001b[0;36m, line \u001b[0;32m657\u001b[0;36m, in \u001b[0;35mlxml.etree._raiseParseError\u001b[0;36m\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/content/drive/MyDrive/Colab Notebooks/Capstone/CaseReportCorpus.pickle\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31mXMLSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Start tag expected, '<' not found, line 1, column 1\n"
          ]
        }
      ],
      "source": [
        "medical_file = \"/content/drive/MyDrive/Colab Notebooks/Capstone/CaseReportCorpus.pickle\"\n",
        "\n",
        "%pip install bioc\n",
        "\n",
        "from bioc import biocxml\n",
        "# Deserialize ``fp`` to a BioC collection object.\n",
        "with open(medical_file, 'rb') as fp:\n",
        "    collection = biocxml.load(fp)\n",
        "    corpus = pd.DataFarme(collection)\n",
        "'''\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "scores = {} # scores is an empty dict already\n",
        "\n",
        "with open(medical_file, \"rb\") as f:\n",
        "  med_data = pkl.load(f)\n",
        "    \n",
        "  corpus = pd.DataFrame(med_data)\n",
        "  corpus.to_csv(medical_file)\n",
        "  scores\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjtqqLmFRit9"
      },
      "source": [
        "# 8. Train and Validate Model on Medical Data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09c406674a854d8a9d7a1a2f887fde6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "186616aa25144a8e956069276e574dfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ae0981bc9d043a18833cf4b28860702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cf23fb16a5841898b7100ca06b013b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21912a216a3b4cb1af7ef0a24e3b89db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e364d07b38b74359acf255a5416a757c",
              "IPY_MODEL_526d6ce9c0cd439b98624b8a4b1c28cd",
              "IPY_MODEL_c5d110d610d6402792e7197e73c691f7"
            ],
            "layout": "IPY_MODEL_3cedbe20d2804134a47e0c85a12b379e"
          }
        },
        "28e7b8d5b8f6425bb892fdace08699e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3c1af83a6a248f88a727cd186554b12",
            "placeholder": "​",
            "style": "IPY_MODEL_e489c299466d4bbf8a0ce7d7927c9b15",
            "value": " 466k/466k [00:00&lt;00:00, 20.3MB/s]"
          }
        },
        "2e5ef85d47d84141ab50612b6f930e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30f44504f0d24629897801a44caacf99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "365aea2ce9d94af58a4a55edf343ccd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "386fc05de954494aaeb770ea38f5b830": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aac03bfbc7bc4d0dbcf611e01ac890d1",
            "placeholder": "​",
            "style": "IPY_MODEL_938cada5b49b4b3e8a4476d56e103831",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "39b0cf1c971e4fa68ec10958a761b88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f32044a1a8ce4a49a48f155632a30a31",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ae0981bc9d043a18833cf4b28860702",
            "value": 440473133
          }
        },
        "3cedbe20d2804134a47e0c85a12b379e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44cc7cd1f25e44d59ba3dea998c66684": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_574ce02c814547b98c485352eddad1e2",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_810f47705a9c46528c1ccae3c3544232",
            "value": 466062
          }
        },
        "478847241d02467badd38b2be2ffc36d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af5a8ab788be45ccb06007b9647d3efa",
            "placeholder": "​",
            "style": "IPY_MODEL_30f44504f0d24629897801a44caacf99",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "4fd31b77d4db49a194e7babc01a929e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506c83cfbf3548e5a7105f217d7b1499": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d988d1d2b2604932a6ed0a6ffa5f2863",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ebb64a055984eeb897c7ea8975b93b7",
            "value": 570
          }
        },
        "526d6ce9c0cd439b98624b8a4b1c28cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c21ef92cfd78456c83c838857dd845e2",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b47a72c47395411fbc267c8e56d761a1",
            "value": 231508
          }
        },
        "564ea5ee81c049c2aaf519391db5b3b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea7290e35bcc4f1ba681a2c056aaf687",
              "IPY_MODEL_506c83cfbf3548e5a7105f217d7b1499",
              "IPY_MODEL_8e6db33a39a5466db49a8e6e56b31046"
            ],
            "layout": "IPY_MODEL_e2cf7febc6ab400ca3a33715809b37b6"
          }
        },
        "574ce02c814547b98c485352eddad1e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a08cf79c3684773997908dc608b72d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96fa318db3394eebb43351af24c8e9b8",
            "placeholder": "​",
            "style": "IPY_MODEL_365aea2ce9d94af58a4a55edf343ccd6",
            "value": " 28.0/28.0 [00:00&lt;00:00, 2.55kB/s]"
          }
        },
        "5ae2108ee6c74636bdeb19c5975eaccc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b123f1fc2754a82963f338496fa974e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ebb64a055984eeb897c7ea8975b93b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fc2dae41f0d49c78e447355d6b8caab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3fe36c9bc0240bf956998af6f3419c8",
              "IPY_MODEL_39b0cf1c971e4fa68ec10958a761b88b",
              "IPY_MODEL_758be8b7c78344f1b61669490cf21f54"
            ],
            "layout": "IPY_MODEL_a4ea4171c78b46898f7b52f80026b64d"
          }
        },
        "72e7c41c42a54f8ca902021e883d8e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_478847241d02467badd38b2be2ffc36d",
              "IPY_MODEL_9969ab2ce15b49e28e32bca46b9d0eeb",
              "IPY_MODEL_5a08cf79c3684773997908dc608b72d8"
            ],
            "layout": "IPY_MODEL_186616aa25144a8e956069276e574dfa"
          }
        },
        "758be8b7c78344f1b61669490cf21f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ae2108ee6c74636bdeb19c5975eaccc",
            "placeholder": "​",
            "style": "IPY_MODEL_a1b363c69dfb438a8510cd78ea11b8ee",
            "value": " 440M/440M [00:01&lt;00:00, 488MB/s]"
          }
        },
        "767fe7a0a45b4293ab07c800a23ff92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "810f47705a9c46528c1ccae3c3544232": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e6db33a39a5466db49a8e6e56b31046": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae8cdfd371ed4c0888d5562b0deb3af8",
            "placeholder": "​",
            "style": "IPY_MODEL_2e5ef85d47d84141ab50612b6f930e6f",
            "value": " 570/570 [00:00&lt;00:00, 54.8kB/s]"
          }
        },
        "938cada5b49b4b3e8a4476d56e103831": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96fa318db3394eebb43351af24c8e9b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9969ab2ce15b49e28e32bca46b9d0eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf74450f9444628bc6b7527ba3d23dd",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09c406674a854d8a9d7a1a2f887fde6f",
            "value": 28
          }
        },
        "9fb0b5d4e4af45999f3df633cf6b6d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0b922631d844583bac144d127264e34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1b363c69dfb438a8510cd78ea11b8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4ea4171c78b46898f7b52f80026b64d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aac03bfbc7bc4d0dbcf611e01ac890d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae8cdfd371ed4c0888d5562b0deb3af8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af5a8ab788be45ccb06007b9647d3efa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3fe36c9bc0240bf956998af6f3419c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fd31b77d4db49a194e7babc01a929e7",
            "placeholder": "​",
            "style": "IPY_MODEL_d9dbfe6326d94af581cd7de0c6c716d4",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "b47a72c47395411fbc267c8e56d761a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c21ef92cfd78456c83c838857dd845e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d110d610d6402792e7197e73c691f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8c8bc41098046babcf9f944bb3385fe",
            "placeholder": "​",
            "style": "IPY_MODEL_ce068d8f5ed2431bad0017d00062d1a8",
            "value": " 232k/232k [00:00&lt;00:00, 548kB/s]"
          }
        },
        "c8c8bc41098046babcf9f944bb3385fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce068d8f5ed2431bad0017d00062d1a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d988d1d2b2604932a6ed0a6ffa5f2863": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9dbfe6326d94af581cd7de0c6c716d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2cf7febc6ab400ca3a33715809b37b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e364d07b38b74359acf255a5416a757c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0b922631d844583bac144d127264e34",
            "placeholder": "​",
            "style": "IPY_MODEL_767fe7a0a45b4293ab07c800a23ff92d",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "e489c299466d4bbf8a0ce7d7927c9b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea7290e35bcc4f1ba681a2c056aaf687": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b123f1fc2754a82963f338496fa974e",
            "placeholder": "​",
            "style": "IPY_MODEL_9fb0b5d4e4af45999f3df633cf6b6d62",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "f32044a1a8ce4a49a48f155632a30a31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c1af83a6a248f88a727cd186554b12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f87b87c8a4cf4efb816bb20310f89fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_386fc05de954494aaeb770ea38f5b830",
              "IPY_MODEL_44cc7cd1f25e44d59ba3dea998c66684",
              "IPY_MODEL_28e7b8d5b8f6425bb892fdace08699e5"
            ],
            "layout": "IPY_MODEL_1cf23fb16a5841898b7100ca06b013b7"
          }
        },
        "fdf74450f9444628bc6b7527ba3d23dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
